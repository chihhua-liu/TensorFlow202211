{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "m = tf.keras.metrics.Accuracy() \n",
    "_ = m.update_state([1, 2, 3, 4], [0, 2, 3, 4]) \n",
    "\n",
    "print(m.result().numpy()) \n",
    "\n",
    "#m.reset_states() \n",
    "\n",
    "_ = m.update_state([1, 2, 3, 4], [0, 2, 3, 4], \n",
    "                    sample_weight=[1, 1, 0, 0]) \n",
    "print(m.result().numpy()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "m = tf.keras.metrics.CategoricalAccuracy() \n",
    "_ = m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8], \n",
    "                    [0.05, 0.95, 0]]) \n",
    "print(m.result().numpy()) \n",
    "\n",
    "#m.reset_states() \n",
    "\n",
    "_ = m.update_state([[0, 1,0], [0, 1, 0]], [[0.1, 0.9, 0.8], \n",
    "                    [0.05, 0.95, 0]]) \n",
    "print(m.result().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "m = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "_ = m.update_state([[2], [1]], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]) \n",
    "\n",
    "print(m.result().numpy())\n",
    "\n",
    "#m.reset_states() \n",
    "\n",
    "_ = m.update_state([[0], [0]], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]) \n",
    "print(m.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1769392\n",
      "0.65707886\n"
     ]
    }
   ],
   "source": [
    "m = tf.keras.metrics.CategoricalCrossentropy()\n",
    "m.update_state([[0, 1, 0], [0, 0, 1]],\n",
    "               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "'''\n",
    "# EPSILON = 1e-7, y = y_true, y` = y_pred\n",
    "# y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n",
    "# y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n",
    "\n",
    "# xent = -sum(y * log(y'), axis = -1)\n",
    "#      = ((-log 0.95), (-log 0.1))\n",
    "#      = [0.051, 2.302]\n",
    "# Reduced mean = (0.051 + 2.302) / 2\n",
    "'''\n",
    "print(m.result().numpy()) \n",
    "\n",
    "\n",
    "#m.reset_states() \n",
    "\n",
    "m.update_state([[0, 1, 0], [0, 1, 0]],\n",
    "               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "\n",
    "print(m.result().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1769392\n",
      "1.913049\n"
     ]
    }
   ],
   "source": [
    "m = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "m.update_state(\n",
    "  [1, 2],\n",
    "  [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "\n",
    "print(m.result().numpy())  \n",
    "\n",
    "#m.reset_states() \n",
    "\n",
    "m.update_state(\n",
    "  [0, 0],\n",
    "  [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "\n",
    "print(m.result().numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN神經網路訓練及驗證 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.354974\n",
      "batch 1: loss 2.284669\n",
      "batch 2: loss 2.218428\n",
      "batch 3: loss 2.188677\n",
      "batch 4: loss 2.051362\n",
      "batch 5: loss 1.954491\n",
      "batch 6: loss 2.010463\n",
      "batch 7: loss 1.823143\n",
      "batch 8: loss 1.834284\n",
      "batch 9: loss 1.857236\n",
      "batch 10: loss 1.733362\n",
      "batch 11: loss 1.747833\n",
      "batch 12: loss 1.549515\n",
      "batch 13: loss 1.496554\n",
      "batch 14: loss 1.487920\n",
      "batch 15: loss 1.547255\n",
      "batch 16: loss 1.440515\n",
      "batch 17: loss 1.393875\n",
      "batch 18: loss 1.277772\n",
      "batch 19: loss 1.475433\n",
      "batch 20: loss 1.328399\n",
      "batch 21: loss 1.254127\n",
      "batch 22: loss 1.189001\n",
      "batch 23: loss 1.021514\n",
      "batch 24: loss 1.149965\n",
      "batch 25: loss 1.097938\n",
      "batch 26: loss 1.171792\n",
      "batch 27: loss 0.985283\n",
      "batch 28: loss 1.133463\n",
      "batch 29: loss 1.090808\n",
      "batch 30: loss 0.809829\n",
      "batch 31: loss 0.989750\n",
      "batch 32: loss 0.813451\n",
      "batch 33: loss 0.785609\n",
      "batch 34: loss 0.863971\n",
      "batch 35: loss 0.881977\n",
      "batch 36: loss 0.879459\n",
      "batch 37: loss 0.622087\n",
      "batch 38: loss 0.813776\n",
      "batch 39: loss 0.702438\n",
      "batch 40: loss 0.749615\n",
      "batch 41: loss 0.796997\n",
      "batch 42: loss 0.703516\n",
      "batch 43: loss 0.707992\n",
      "batch 44: loss 0.746117\n",
      "batch 45: loss 0.701371\n",
      "batch 46: loss 0.538637\n",
      "batch 47: loss 0.559088\n",
      "batch 48: loss 0.803474\n",
      "batch 49: loss 0.632005\n",
      "batch 50: loss 0.456995\n",
      "batch 51: loss 0.656536\n",
      "batch 52: loss 0.627605\n",
      "batch 53: loss 0.620824\n",
      "batch 54: loss 0.589300\n",
      "batch 55: loss 0.699004\n",
      "batch 56: loss 0.492618\n",
      "batch 57: loss 0.778736\n",
      "batch 58: loss 0.499810\n",
      "batch 59: loss 0.421707\n",
      "batch 60: loss 0.355826\n",
      "batch 61: loss 0.447860\n",
      "batch 62: loss 0.549188\n",
      "batch 63: loss 0.528192\n",
      "batch 64: loss 0.579882\n",
      "batch 65: loss 0.431626\n",
      "batch 66: loss 0.399162\n",
      "batch 67: loss 0.307997\n",
      "batch 68: loss 0.481343\n",
      "batch 69: loss 0.537094\n",
      "batch 70: loss 0.523405\n",
      "batch 71: loss 0.517978\n",
      "batch 72: loss 0.494816\n",
      "batch 73: loss 0.372668\n",
      "batch 74: loss 0.554033\n",
      "batch 75: loss 0.496205\n",
      "batch 76: loss 0.391924\n",
      "batch 77: loss 0.432681\n",
      "batch 78: loss 0.373365\n",
      "batch 79: loss 0.379743\n",
      "batch 80: loss 0.235060\n",
      "batch 81: loss 0.527422\n",
      "batch 82: loss 0.273229\n",
      "batch 83: loss 0.326217\n",
      "batch 84: loss 0.499645\n",
      "batch 85: loss 0.476667\n",
      "batch 86: loss 0.705911\n",
      "batch 87: loss 0.544736\n",
      "batch 88: loss 0.493252\n",
      "batch 89: loss 0.490163\n",
      "batch 90: loss 0.462308\n",
      "batch 91: loss 0.329629\n",
      "batch 92: loss 0.597701\n",
      "batch 93: loss 0.388193\n",
      "batch 94: loss 0.585924\n",
      "batch 95: loss 0.351912\n",
      "batch 96: loss 0.499552\n",
      "batch 97: loss 0.576810\n",
      "batch 98: loss 0.634373\n",
      "batch 99: loss 0.390119\n",
      "batch 100: loss 0.545465\n",
      "batch 101: loss 0.475492\n",
      "batch 102: loss 0.335066\n",
      "batch 103: loss 0.389145\n",
      "batch 104: loss 0.357354\n",
      "batch 105: loss 0.400875\n",
      "batch 106: loss 0.398969\n",
      "batch 107: loss 0.470116\n",
      "batch 108: loss 0.370587\n",
      "batch 109: loss 0.447828\n",
      "batch 110: loss 0.329984\n",
      "batch 111: loss 0.360660\n",
      "batch 112: loss 0.280426\n",
      "batch 113: loss 0.315389\n",
      "batch 114: loss 0.237689\n",
      "batch 115: loss 0.453492\n",
      "batch 116: loss 0.369260\n",
      "batch 117: loss 0.368900\n",
      "batch 118: loss 0.486414\n",
      "batch 119: loss 0.280006\n",
      "batch 120: loss 0.464622\n",
      "batch 121: loss 0.537309\n",
      "batch 122: loss 0.482507\n",
      "batch 123: loss 0.403529\n",
      "batch 124: loss 0.417413\n",
      "batch 125: loss 0.238798\n",
      "batch 126: loss 0.266505\n",
      "batch 127: loss 0.322692\n",
      "batch 128: loss 0.229543\n",
      "batch 129: loss 0.492272\n",
      "batch 130: loss 0.547962\n",
      "batch 131: loss 0.252721\n",
      "batch 132: loss 0.264999\n",
      "batch 133: loss 0.329799\n",
      "batch 134: loss 0.608384\n",
      "batch 135: loss 0.382179\n",
      "batch 136: loss 0.365720\n",
      "batch 137: loss 0.199371\n",
      "batch 138: loss 0.265842\n",
      "batch 139: loss 0.488758\n",
      "batch 140: loss 0.310759\n",
      "batch 141: loss 0.437132\n",
      "batch 142: loss 0.453189\n",
      "batch 143: loss 0.500185\n",
      "batch 144: loss 0.497073\n",
      "batch 145: loss 0.471711\n",
      "batch 146: loss 0.375185\n",
      "batch 147: loss 0.422025\n",
      "batch 148: loss 0.331694\n",
      "batch 149: loss 0.389594\n",
      "batch 150: loss 0.416646\n",
      "batch 151: loss 0.713666\n",
      "batch 152: loss 0.284135\n",
      "batch 153: loss 0.595892\n",
      "batch 154: loss 0.377609\n",
      "batch 155: loss 0.156512\n",
      "batch 156: loss 0.455707\n",
      "batch 157: loss 0.617767\n",
      "batch 158: loss 0.451095\n",
      "batch 159: loss 0.350887\n",
      "batch 160: loss 0.240174\n",
      "batch 161: loss 0.301559\n",
      "batch 162: loss 0.275868\n",
      "batch 163: loss 0.251725\n",
      "batch 164: loss 0.149955\n",
      "batch 165: loss 0.294940\n",
      "batch 166: loss 0.339282\n",
      "batch 167: loss 0.368444\n",
      "batch 168: loss 0.287629\n",
      "batch 169: loss 0.316446\n",
      "batch 170: loss 0.202981\n",
      "batch 171: loss 0.266098\n",
      "batch 172: loss 0.608386\n",
      "batch 173: loss 0.148958\n",
      "batch 174: loss 0.357815\n",
      "batch 175: loss 0.599836\n",
      "batch 176: loss 0.264364\n",
      "batch 177: loss 0.235345\n",
      "batch 178: loss 0.328683\n",
      "batch 179: loss 0.276861\n",
      "batch 180: loss 0.480874\n",
      "batch 181: loss 0.464458\n",
      "batch 182: loss 0.329318\n",
      "batch 183: loss 0.267238\n",
      "batch 184: loss 0.240998\n",
      "batch 185: loss 0.652373\n",
      "batch 186: loss 0.247826\n",
      "batch 187: loss 0.282102\n",
      "batch 188: loss 0.311312\n",
      "batch 189: loss 0.308304\n",
      "batch 190: loss 0.355352\n",
      "batch 191: loss 0.285915\n",
      "batch 192: loss 0.485467\n",
      "batch 193: loss 0.399413\n",
      "batch 194: loss 0.204128\n",
      "batch 195: loss 0.366307\n",
      "batch 196: loss 0.271145\n",
      "batch 197: loss 0.427157\n",
      "batch 198: loss 0.522001\n",
      "batch 199: loss 0.256095\n",
      "batch 200: loss 0.319936\n",
      "batch 201: loss 0.236554\n",
      "batch 202: loss 0.446120\n",
      "batch 203: loss 0.384313\n",
      "batch 204: loss 0.443818\n",
      "batch 205: loss 0.357550\n",
      "batch 206: loss 0.282782\n",
      "batch 207: loss 0.163011\n",
      "batch 208: loss 0.387550\n",
      "batch 209: loss 0.315301\n",
      "batch 210: loss 0.374595\n",
      "batch 211: loss 0.167123\n",
      "batch 212: loss 0.370487\n",
      "batch 213: loss 0.242225\n",
      "batch 214: loss 0.393841\n",
      "batch 215: loss 0.199367\n",
      "batch 216: loss 0.271768\n",
      "batch 217: loss 0.201506\n",
      "batch 218: loss 0.181645\n",
      "batch 219: loss 0.178410\n",
      "batch 220: loss 0.430353\n",
      "batch 221: loss 0.266207\n",
      "batch 222: loss 0.320578\n",
      "batch 223: loss 0.323203\n",
      "batch 224: loss 0.203815\n",
      "batch 225: loss 0.327580\n",
      "batch 226: loss 0.293109\n",
      "batch 227: loss 0.121158\n",
      "batch 228: loss 0.335314\n",
      "batch 229: loss 0.194957\n",
      "batch 230: loss 0.447311\n",
      "batch 231: loss 0.424986\n",
      "batch 232: loss 0.364674\n",
      "batch 233: loss 0.439626\n",
      "batch 234: loss 0.153137\n",
      "batch 235: loss 0.481896\n",
      "batch 236: loss 0.171719\n",
      "batch 237: loss 0.282885\n",
      "batch 238: loss 0.420639\n",
      "batch 239: loss 0.449563\n",
      "batch 240: loss 0.198652\n",
      "batch 241: loss 0.199379\n",
      "batch 242: loss 0.288900\n",
      "batch 243: loss 0.548815\n",
      "batch 244: loss 0.237771\n",
      "batch 245: loss 0.504981\n",
      "batch 246: loss 0.143208\n",
      "batch 247: loss 0.309083\n",
      "batch 248: loss 0.286689\n",
      "batch 249: loss 0.142792\n",
      "batch 250: loss 0.210813\n",
      "batch 251: loss 0.451542\n",
      "batch 252: loss 0.317636\n",
      "batch 253: loss 0.346131\n",
      "batch 254: loss 0.195225\n",
      "batch 255: loss 0.221546\n",
      "batch 256: loss 0.189587\n",
      "batch 257: loss 0.200191\n",
      "batch 258: loss 0.334115\n",
      "batch 259: loss 0.311804\n",
      "batch 260: loss 0.308224\n",
      "batch 261: loss 0.276448\n",
      "batch 262: loss 0.381297\n",
      "batch 263: loss 0.445393\n",
      "batch 264: loss 0.167338\n",
      "batch 265: loss 0.321796\n",
      "batch 266: loss 0.384847\n",
      "batch 267: loss 0.321524\n",
      "batch 268: loss 0.369509\n",
      "batch 269: loss 0.256402\n",
      "batch 270: loss 0.454075\n",
      "batch 271: loss 0.241399\n",
      "batch 272: loss 0.185317\n",
      "batch 273: loss 0.174061\n",
      "batch 274: loss 0.235924\n",
      "batch 275: loss 0.454608\n",
      "batch 276: loss 0.174710\n",
      "batch 277: loss 0.361443\n",
      "batch 278: loss 0.195270\n",
      "batch 279: loss 0.192837\n",
      "batch 280: loss 0.411230\n",
      "batch 281: loss 0.235293\n",
      "batch 282: loss 0.210256\n",
      "batch 283: loss 0.322755\n",
      "batch 284: loss 0.235952\n",
      "batch 285: loss 0.319813\n",
      "batch 286: loss 0.131623\n",
      "batch 287: loss 0.285720\n",
      "batch 288: loss 0.312619\n",
      "batch 289: loss 0.379616\n",
      "batch 290: loss 0.355004\n",
      "batch 291: loss 0.351178\n",
      "batch 292: loss 0.266508\n",
      "batch 293: loss 0.213548\n",
      "batch 294: loss 0.321838\n",
      "batch 295: loss 0.369329\n",
      "batch 296: loss 0.325854\n",
      "batch 297: loss 0.229989\n",
      "batch 298: loss 0.261940\n",
      "batch 299: loss 0.336626\n",
      "batch 300: loss 0.292544\n",
      "batch 301: loss 0.207953\n",
      "batch 302: loss 0.635159\n",
      "batch 303: loss 0.276092\n",
      "batch 304: loss 0.193702\n",
      "batch 305: loss 0.349467\n",
      "batch 306: loss 0.287645\n",
      "batch 307: loss 0.204166\n",
      "batch 308: loss 0.084743\n",
      "batch 309: loss 0.184618\n",
      "batch 310: loss 0.524220\n",
      "batch 311: loss 0.434204\n",
      "batch 312: loss 0.328353\n",
      "batch 313: loss 0.198847\n",
      "batch 314: loss 0.313176\n",
      "batch 315: loss 0.326614\n",
      "batch 316: loss 0.124574\n",
      "batch 317: loss 0.116042\n",
      "batch 318: loss 0.260571\n",
      "batch 319: loss 0.265018\n",
      "batch 320: loss 0.303154\n",
      "batch 321: loss 0.342993\n",
      "batch 322: loss 0.347605\n",
      "batch 323: loss 0.209628\n",
      "batch 324: loss 0.255923\n",
      "batch 325: loss 0.220546\n",
      "batch 326: loss 0.179673\n",
      "batch 327: loss 0.374236\n",
      "batch 328: loss 0.168210\n",
      "batch 329: loss 0.288749\n",
      "batch 330: loss 0.344992\n",
      "batch 331: loss 0.328197\n",
      "batch 332: loss 0.477785\n",
      "batch 333: loss 0.325043\n",
      "batch 334: loss 0.143770\n",
      "batch 335: loss 0.249279\n",
      "batch 336: loss 0.169851\n",
      "batch 337: loss 0.258610\n",
      "batch 338: loss 0.394218\n",
      "batch 339: loss 0.190185\n",
      "batch 340: loss 0.408322\n",
      "batch 341: loss 0.257129\n",
      "batch 342: loss 0.326230\n",
      "batch 343: loss 0.207395\n",
      "batch 344: loss 0.750537\n",
      "batch 345: loss 0.219071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 346: loss 0.370000\n",
      "batch 347: loss 0.312630\n",
      "batch 348: loss 0.291699\n",
      "batch 349: loss 0.207417\n",
      "batch 350: loss 0.369015\n",
      "batch 351: loss 0.221622\n",
      "batch 352: loss 0.223978\n",
      "batch 353: loss 0.421958\n",
      "batch 354: loss 0.210544\n",
      "batch 355: loss 0.412597\n",
      "batch 356: loss 0.258187\n",
      "batch 357: loss 0.292473\n",
      "batch 358: loss 0.213100\n",
      "batch 359: loss 0.157366\n",
      "batch 360: loss 0.342251\n",
      "batch 361: loss 0.294321\n",
      "batch 362: loss 0.177613\n",
      "batch 363: loss 0.157225\n",
      "batch 364: loss 0.316996\n",
      "batch 365: loss 0.342151\n",
      "batch 366: loss 0.291449\n",
      "batch 367: loss 0.516263\n",
      "batch 368: loss 0.311859\n",
      "batch 369: loss 0.442579\n",
      "batch 370: loss 0.216022\n",
      "batch 371: loss 0.425467\n",
      "batch 372: loss 0.106068\n",
      "batch 373: loss 0.347413\n",
      "batch 374: loss 0.232258\n",
      "batch 375: loss 0.260568\n",
      "batch 376: loss 0.399738\n",
      "batch 377: loss 0.377000\n",
      "batch 378: loss 0.180393\n",
      "batch 379: loss 0.260714\n",
      "batch 380: loss 0.385250\n",
      "batch 381: loss 0.336319\n",
      "batch 382: loss 0.338826\n",
      "batch 383: loss 0.280164\n",
      "batch 384: loss 0.349495\n",
      "batch 385: loss 0.149280\n",
      "batch 386: loss 0.189019\n",
      "batch 387: loss 0.115931\n",
      "batch 388: loss 0.160029\n",
      "batch 389: loss 0.550083\n",
      "batch 390: loss 0.145780\n",
      "batch 391: loss 0.437222\n",
      "batch 392: loss 0.105527\n",
      "batch 393: loss 0.261789\n",
      "batch 394: loss 0.325034\n",
      "batch 395: loss 0.239843\n",
      "batch 396: loss 0.338887\n",
      "batch 397: loss 0.546679\n",
      "batch 398: loss 0.181377\n",
      "batch 399: loss 0.316408\n",
      "batch 400: loss 0.231887\n",
      "batch 401: loss 0.545015\n",
      "batch 402: loss 0.514960\n",
      "batch 403: loss 0.278305\n",
      "batch 404: loss 0.321955\n",
      "batch 405: loss 0.294860\n",
      "batch 406: loss 0.197882\n",
      "batch 407: loss 0.146867\n",
      "batch 408: loss 0.059440\n",
      "batch 409: loss 0.244507\n",
      "batch 410: loss 0.412582\n",
      "batch 411: loss 0.178609\n",
      "batch 412: loss 0.329758\n",
      "batch 413: loss 0.145127\n",
      "batch 414: loss 0.284348\n",
      "batch 415: loss 0.222639\n",
      "batch 416: loss 0.156754\n",
      "batch 417: loss 0.231156\n",
      "batch 418: loss 0.305892\n",
      "batch 419: loss 0.238533\n",
      "batch 420: loss 0.302492\n",
      "batch 421: loss 0.223075\n",
      "batch 422: loss 0.209998\n",
      "batch 423: loss 0.324090\n",
      "batch 424: loss 0.193257\n",
      "batch 425: loss 0.297078\n",
      "batch 426: loss 0.250978\n",
      "batch 427: loss 0.285195\n",
      "batch 428: loss 0.104471\n",
      "batch 429: loss 0.287174\n",
      "batch 430: loss 0.428142\n",
      "batch 431: loss 0.516730\n",
      "batch 432: loss 0.327601\n",
      "batch 433: loss 0.231357\n",
      "batch 434: loss 0.152087\n",
      "batch 435: loss 0.349628\n",
      "batch 436: loss 0.244977\n",
      "batch 437: loss 0.305791\n",
      "batch 438: loss 0.325658\n",
      "batch 439: loss 0.370409\n",
      "batch 440: loss 0.346684\n",
      "batch 441: loss 0.278846\n",
      "batch 442: loss 0.209408\n",
      "batch 443: loss 0.310236\n",
      "batch 444: loss 0.257367\n",
      "batch 445: loss 0.268473\n",
      "batch 446: loss 0.107523\n",
      "batch 447: loss 0.287964\n",
      "batch 448: loss 0.221334\n",
      "batch 449: loss 0.117017\n",
      "batch 450: loss 0.277639\n",
      "batch 451: loss 0.302292\n",
      "batch 452: loss 0.384187\n",
      "batch 453: loss 0.231012\n",
      "batch 454: loss 0.218485\n",
      "batch 455: loss 0.453435\n",
      "batch 456: loss 0.279233\n",
      "batch 457: loss 0.191457\n",
      "batch 458: loss 0.402336\n",
      "batch 459: loss 0.247288\n",
      "batch 460: loss 0.143494\n",
      "batch 461: loss 0.227238\n",
      "batch 462: loss 0.276445\n",
      "batch 463: loss 0.146715\n",
      "batch 464: loss 0.234233\n",
      "batch 465: loss 0.229356\n",
      "batch 466: loss 0.303134\n",
      "batch 467: loss 0.301613\n",
      "batch 468: loss 0.279193\n",
      "batch 469: loss 0.175547\n",
      "batch 470: loss 0.180158\n",
      "batch 471: loss 0.336708\n",
      "batch 472: loss 0.123296\n",
      "batch 473: loss 0.186996\n",
      "batch 474: loss 0.452417\n",
      "batch 475: loss 0.244495\n",
      "batch 476: loss 0.243810\n",
      "batch 477: loss 0.366613\n",
      "batch 478: loss 0.164184\n",
      "batch 479: loss 0.286328\n",
      "batch 480: loss 0.439247\n",
      "batch 481: loss 0.146247\n",
      "batch 482: loss 0.294357\n",
      "batch 483: loss 0.335873\n",
      "batch 484: loss 0.219480\n",
      "batch 485: loss 0.322436\n",
      "batch 486: loss 0.275243\n",
      "batch 487: loss 0.448319\n",
      "batch 488: loss 0.221094\n",
      "batch 489: loss 0.328169\n",
      "batch 490: loss 0.152871\n",
      "batch 491: loss 0.299926\n",
      "batch 492: loss 0.289433\n",
      "batch 493: loss 0.161282\n",
      "batch 494: loss 0.391072\n",
      "batch 495: loss 0.478364\n",
      "batch 496: loss 0.245280\n",
      "batch 497: loss 0.212606\n",
      "batch 498: loss 0.137224\n",
      "batch 499: loss 0.180558\n",
      "batch 500: loss 0.268856\n",
      "batch 501: loss 0.324045\n",
      "batch 502: loss 0.267440\n",
      "batch 503: loss 0.318987\n",
      "batch 504: loss 0.379152\n",
      "batch 505: loss 0.164921\n",
      "batch 506: loss 0.282593\n",
      "batch 507: loss 0.316712\n",
      "batch 508: loss 0.169162\n",
      "batch 509: loss 0.418425\n",
      "batch 510: loss 0.229802\n",
      "batch 511: loss 0.287695\n",
      "batch 512: loss 0.382203\n",
      "batch 513: loss 0.079596\n",
      "batch 514: loss 0.198447\n",
      "batch 515: loss 0.311107\n",
      "batch 516: loss 0.280408\n",
      "batch 517: loss 0.189034\n",
      "batch 518: loss 0.448839\n",
      "batch 519: loss 0.283874\n",
      "batch 520: loss 0.187069\n",
      "batch 521: loss 0.191741\n",
      "batch 522: loss 0.149352\n",
      "batch 523: loss 0.271206\n",
      "batch 524: loss 0.141782\n",
      "batch 525: loss 0.218908\n",
      "batch 526: loss 0.184756\n",
      "batch 527: loss 0.151189\n",
      "batch 528: loss 0.298637\n",
      "batch 529: loss 0.140821\n",
      "batch 530: loss 0.620992\n",
      "batch 531: loss 0.088133\n",
      "batch 532: loss 0.143242\n",
      "batch 533: loss 0.327105\n",
      "batch 534: loss 0.367120\n",
      "batch 535: loss 0.197853\n",
      "batch 536: loss 0.177326\n",
      "batch 537: loss 0.611803\n",
      "batch 538: loss 0.101808\n",
      "batch 539: loss 0.119416\n",
      "batch 540: loss 0.134856\n",
      "batch 541: loss 0.349144\n",
      "batch 542: loss 0.253174\n",
      "batch 543: loss 0.136664\n",
      "batch 544: loss 0.161358\n",
      "batch 545: loss 0.187969\n",
      "batch 546: loss 0.278688\n",
      "batch 547: loss 0.207121\n",
      "batch 548: loss 0.296825\n",
      "batch 549: loss 0.372508\n",
      "batch 550: loss 0.155811\n",
      "batch 551: loss 0.364806\n",
      "batch 552: loss 0.153893\n",
      "batch 553: loss 0.395036\n",
      "batch 554: loss 0.237989\n",
      "batch 555: loss 0.220655\n",
      "batch 556: loss 0.167266\n",
      "batch 557: loss 0.166673\n",
      "batch 558: loss 0.145835\n",
      "batch 559: loss 0.148409\n",
      "batch 560: loss 0.165892\n",
      "batch 561: loss 0.252914\n",
      "batch 562: loss 0.246649\n",
      "batch 563: loss 0.182404\n",
      "batch 564: loss 0.233216\n",
      "batch 565: loss 0.116287\n",
      "batch 566: loss 0.284875\n",
      "batch 567: loss 0.109119\n",
      "batch 568: loss 0.277064\n",
      "batch 569: loss 0.190196\n",
      "batch 570: loss 0.268377\n",
      "batch 571: loss 0.312644\n",
      "batch 572: loss 0.212739\n",
      "batch 573: loss 0.300500\n",
      "batch 574: loss 0.362686\n",
      "batch 575: loss 0.089063\n",
      "batch 576: loss 0.340381\n",
      "batch 577: loss 0.354115\n",
      "batch 578: loss 0.367149\n",
      "batch 579: loss 0.474854\n",
      "batch 580: loss 0.287438\n",
      "batch 581: loss 0.178980\n",
      "batch 582: loss 0.331946\n",
      "batch 583: loss 0.303869\n",
      "batch 584: loss 0.114028\n",
      "batch 585: loss 0.164780\n",
      "batch 586: loss 0.188085\n",
      "batch 587: loss 0.213625\n",
      "batch 588: loss 0.145952\n",
      "batch 589: loss 0.195250\n",
      "batch 590: loss 0.098604\n",
      "batch 591: loss 0.311023\n",
      "batch 592: loss 0.141814\n",
      "batch 593: loss 0.224890\n",
      "batch 594: loss 0.288914\n",
      "batch 595: loss 0.176665\n",
      "batch 596: loss 0.328397\n",
      "batch 597: loss 0.318081\n",
      "batch 598: loss 0.470257\n",
      "batch 599: loss 0.231125\n",
      "batch 600: loss 0.207244\n",
      "batch 601: loss 0.279993\n",
      "batch 602: loss 0.211257\n",
      "batch 603: loss 0.285104\n",
      "batch 604: loss 0.211255\n",
      "batch 605: loss 0.226713\n",
      "batch 606: loss 0.207275\n",
      "batch 607: loss 0.308193\n",
      "batch 608: loss 0.217878\n",
      "batch 609: loss 0.370282\n",
      "batch 610: loss 0.083602\n",
      "batch 611: loss 0.278294\n",
      "batch 612: loss 0.258224\n",
      "batch 613: loss 0.193828\n",
      "batch 614: loss 0.259145\n",
      "batch 615: loss 0.161015\n",
      "batch 616: loss 0.211214\n",
      "batch 617: loss 0.207039\n",
      "batch 618: loss 0.540766\n",
      "batch 619: loss 0.179086\n",
      "batch 620: loss 0.383944\n",
      "batch 621: loss 0.250190\n",
      "batch 622: loss 0.110899\n",
      "batch 623: loss 0.424410\n",
      "batch 624: loss 0.160643\n",
      "batch 625: loss 0.151644\n",
      "batch 626: loss 0.133972\n",
      "batch 627: loss 0.218945\n",
      "batch 628: loss 0.400137\n",
      "batch 629: loss 0.174744\n",
      "batch 630: loss 0.254956\n",
      "batch 631: loss 0.204379\n",
      "batch 632: loss 0.125264\n",
      "batch 633: loss 0.336283\n",
      "batch 634: loss 0.310358\n",
      "batch 635: loss 0.450003\n",
      "batch 636: loss 0.136770\n",
      "batch 637: loss 0.412079\n",
      "batch 638: loss 0.105625\n",
      "batch 639: loss 0.157277\n",
      "batch 640: loss 0.275172\n",
      "batch 641: loss 0.206227\n",
      "batch 642: loss 0.082755\n",
      "batch 643: loss 0.379021\n",
      "batch 644: loss 0.208653\n",
      "batch 645: loss 0.125072\n",
      "batch 646: loss 0.170093\n",
      "batch 647: loss 0.267039\n",
      "batch 648: loss 0.212018\n",
      "batch 649: loss 0.247563\n",
      "batch 650: loss 0.107317\n",
      "batch 651: loss 0.245218\n",
      "batch 652: loss 0.166902\n",
      "batch 653: loss 0.250380\n",
      "batch 654: loss 0.120494\n",
      "batch 655: loss 0.252406\n",
      "batch 656: loss 0.193292\n",
      "batch 657: loss 0.117381\n",
      "batch 658: loss 0.114548\n",
      "batch 659: loss 0.239949\n",
      "batch 660: loss 0.160487\n",
      "batch 661: loss 0.188376\n",
      "batch 662: loss 0.099513\n",
      "batch 663: loss 0.189355\n",
      "batch 664: loss 0.158883\n",
      "batch 665: loss 0.200838\n",
      "batch 666: loss 0.089843\n",
      "batch 667: loss 0.215668\n",
      "batch 668: loss 0.334442\n",
      "batch 669: loss 0.254923\n",
      "batch 670: loss 0.113813\n",
      "batch 671: loss 0.492961\n",
      "batch 672: loss 0.325127\n",
      "batch 673: loss 0.241992\n",
      "batch 674: loss 0.148080\n",
      "batch 675: loss 0.134517\n",
      "batch 676: loss 0.240016\n",
      "batch 677: loss 0.146137\n",
      "batch 678: loss 0.137461\n",
      "batch 679: loss 0.602206\n",
      "batch 680: loss 0.096820\n",
      "batch 681: loss 0.143449\n",
      "batch 682: loss 0.262029\n",
      "batch 683: loss 0.300416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 684: loss 0.185845\n",
      "batch 685: loss 0.172178\n",
      "batch 686: loss 0.180075\n",
      "batch 687: loss 0.110639\n",
      "batch 688: loss 0.214267\n",
      "batch 689: loss 0.091975\n",
      "batch 690: loss 0.122914\n",
      "batch 691: loss 0.156976\n",
      "batch 692: loss 0.421308\n",
      "batch 693: loss 0.254533\n",
      "batch 694: loss 0.181864\n",
      "batch 695: loss 0.063782\n",
      "batch 696: loss 0.073115\n",
      "batch 697: loss 0.217698\n",
      "batch 698: loss 0.162939\n",
      "batch 699: loss 0.221288\n",
      "batch 700: loss 0.123307\n",
      "batch 701: loss 0.198036\n",
      "batch 702: loss 0.062792\n",
      "batch 703: loss 0.088096\n",
      "batch 704: loss 0.260044\n",
      "batch 705: loss 0.267470\n",
      "batch 706: loss 0.161040\n",
      "batch 707: loss 0.155489\n",
      "batch 708: loss 0.075494\n",
      "batch 709: loss 0.318256\n",
      "batch 710: loss 0.181334\n",
      "batch 711: loss 0.267374\n",
      "batch 712: loss 0.141915\n",
      "batch 713: loss 0.240593\n",
      "batch 714: loss 0.210442\n",
      "batch 715: loss 0.301391\n",
      "batch 716: loss 0.243376\n",
      "batch 717: loss 0.162683\n",
      "batch 718: loss 0.223502\n",
      "batch 719: loss 0.250063\n",
      "batch 720: loss 0.196407\n",
      "batch 721: loss 0.146566\n",
      "batch 722: loss 0.202879\n",
      "batch 723: loss 0.192433\n",
      "batch 724: loss 0.104599\n",
      "batch 725: loss 0.202855\n",
      "batch 726: loss 0.082516\n",
      "batch 727: loss 0.211674\n",
      "batch 728: loss 0.179533\n",
      "batch 729: loss 0.116302\n",
      "batch 730: loss 0.136234\n",
      "batch 731: loss 0.438720\n",
      "batch 732: loss 0.269267\n",
      "batch 733: loss 0.180117\n",
      "batch 734: loss 0.072883\n",
      "batch 735: loss 0.424904\n",
      "batch 736: loss 0.420267\n",
      "batch 737: loss 0.346295\n",
      "batch 738: loss 0.082424\n",
      "batch 739: loss 0.165555\n",
      "batch 740: loss 0.170472\n",
      "batch 741: loss 0.261550\n",
      "batch 742: loss 0.368091\n",
      "batch 743: loss 0.109449\n",
      "batch 744: loss 0.315682\n",
      "batch 745: loss 0.279423\n",
      "batch 746: loss 0.150558\n",
      "batch 747: loss 0.076713\n",
      "batch 748: loss 0.128169\n",
      "batch 749: loss 0.078358\n",
      "batch 750: loss 0.138424\n",
      "batch 751: loss 0.053376\n",
      "batch 752: loss 0.158854\n",
      "batch 753: loss 0.112138\n",
      "batch 754: loss 0.196552\n",
      "batch 755: loss 0.169033\n",
      "batch 756: loss 0.201239\n",
      "batch 757: loss 0.235036\n",
      "batch 758: loss 0.132141\n",
      "batch 759: loss 0.500883\n",
      "batch 760: loss 0.298898\n",
      "batch 761: loss 0.230423\n",
      "batch 762: loss 0.322900\n",
      "batch 763: loss 0.331868\n",
      "batch 764: loss 0.181938\n",
      "batch 765: loss 0.132486\n",
      "batch 766: loss 0.323783\n",
      "batch 767: loss 0.120591\n",
      "batch 768: loss 0.090975\n",
      "batch 769: loss 0.186976\n",
      "batch 770: loss 0.143606\n",
      "batch 771: loss 0.401377\n",
      "batch 772: loss 0.134330\n",
      "batch 773: loss 0.347258\n",
      "batch 774: loss 0.233036\n",
      "batch 775: loss 0.185648\n",
      "batch 776: loss 0.145768\n",
      "batch 777: loss 0.205176\n",
      "batch 778: loss 0.065562\n",
      "batch 779: loss 0.311502\n",
      "batch 780: loss 0.300474\n",
      "batch 781: loss 0.167111\n",
      "batch 782: loss 0.174727\n",
      "batch 783: loss 0.250029\n",
      "batch 784: loss 0.092666\n",
      "batch 785: loss 0.237713\n",
      "batch 786: loss 0.222966\n",
      "batch 787: loss 0.123553\n",
      "batch 788: loss 0.182524\n",
      "batch 789: loss 0.187963\n",
      "batch 790: loss 0.227400\n",
      "batch 791: loss 0.162687\n",
      "batch 792: loss 0.300843\n",
      "batch 793: loss 0.589147\n",
      "batch 794: loss 0.236261\n",
      "batch 795: loss 0.195197\n",
      "batch 796: loss 0.225983\n",
      "batch 797: loss 0.164415\n",
      "batch 798: loss 0.160090\n",
      "batch 799: loss 0.625647\n",
      "batch 800: loss 0.194467\n",
      "batch 801: loss 0.374501\n",
      "batch 802: loss 0.405641\n",
      "batch 803: loss 0.052334\n",
      "batch 804: loss 0.384349\n",
      "batch 805: loss 0.280647\n",
      "batch 806: loss 0.327551\n",
      "batch 807: loss 0.371863\n",
      "batch 808: loss 0.211916\n",
      "batch 809: loss 0.233626\n",
      "batch 810: loss 0.198672\n",
      "batch 811: loss 0.259036\n",
      "batch 812: loss 0.215848\n",
      "batch 813: loss 0.160130\n",
      "batch 814: loss 0.134841\n",
      "batch 815: loss 0.335436\n",
      "batch 816: loss 0.129513\n",
      "batch 817: loss 0.145953\n",
      "batch 818: loss 0.085457\n",
      "batch 819: loss 0.163059\n",
      "batch 820: loss 0.166239\n",
      "batch 821: loss 0.217408\n",
      "batch 822: loss 0.157754\n",
      "batch 823: loss 0.135999\n",
      "batch 824: loss 0.225645\n",
      "batch 825: loss 0.077695\n",
      "batch 826: loss 0.069215\n",
      "batch 827: loss 0.097536\n",
      "batch 828: loss 0.404687\n",
      "batch 829: loss 0.437530\n",
      "batch 830: loss 0.111322\n",
      "batch 831: loss 0.105538\n",
      "batch 832: loss 0.112155\n",
      "batch 833: loss 0.119089\n",
      "batch 834: loss 0.314274\n",
      "batch 835: loss 0.115968\n",
      "batch 836: loss 0.142933\n",
      "batch 837: loss 0.195270\n",
      "batch 838: loss 0.214973\n",
      "batch 839: loss 0.216947\n",
      "batch 840: loss 0.186262\n",
      "batch 841: loss 0.165090\n",
      "batch 842: loss 0.139593\n",
      "batch 843: loss 0.064196\n",
      "batch 844: loss 0.130515\n",
      "batch 845: loss 0.357565\n",
      "batch 846: loss 0.149060\n",
      "batch 847: loss 0.158315\n",
      "batch 848: loss 0.103994\n",
      "batch 849: loss 0.132358\n",
      "batch 850: loss 0.132534\n",
      "batch 851: loss 0.231596\n",
      "batch 852: loss 0.137443\n",
      "batch 853: loss 0.137563\n",
      "batch 854: loss 0.353305\n",
      "batch 855: loss 0.154675\n",
      "batch 856: loss 0.245107\n",
      "batch 857: loss 0.544304\n",
      "batch 858: loss 0.131608\n",
      "batch 859: loss 0.191364\n",
      "batch 860: loss 0.124250\n",
      "batch 861: loss 0.248133\n",
      "batch 862: loss 0.294921\n",
      "batch 863: loss 0.139669\n",
      "batch 864: loss 0.091560\n",
      "batch 865: loss 0.066226\n",
      "batch 866: loss 0.259999\n",
      "batch 867: loss 0.267129\n",
      "batch 868: loss 0.219824\n",
      "batch 869: loss 0.210981\n",
      "batch 870: loss 0.118618\n",
      "batch 871: loss 0.358386\n",
      "batch 872: loss 0.286198\n",
      "batch 873: loss 0.088801\n",
      "batch 874: loss 0.145405\n",
      "batch 875: loss 0.128083\n",
      "batch 876: loss 0.083350\n",
      "batch 877: loss 0.117097\n",
      "batch 878: loss 0.139676\n",
      "batch 879: loss 0.119250\n",
      "batch 880: loss 0.304259\n",
      "batch 881: loss 0.130708\n",
      "batch 882: loss 0.121075\n",
      "batch 883: loss 0.321977\n",
      "batch 884: loss 0.069642\n",
      "batch 885: loss 0.215619\n",
      "batch 886: loss 0.351843\n",
      "batch 887: loss 0.270855\n",
      "batch 888: loss 0.138155\n",
      "batch 889: loss 0.108323\n",
      "batch 890: loss 0.358983\n",
      "batch 891: loss 0.081165\n",
      "batch 892: loss 0.200181\n",
      "batch 893: loss 0.075236\n",
      "batch 894: loss 0.096592\n",
      "batch 895: loss 0.099857\n",
      "batch 896: loss 0.139234\n",
      "batch 897: loss 0.187971\n",
      "batch 898: loss 0.224719\n",
      "batch 899: loss 0.146835\n",
      "batch 900: loss 0.085459\n",
      "batch 901: loss 0.140614\n",
      "batch 902: loss 0.407309\n",
      "batch 903: loss 0.131420\n",
      "batch 904: loss 0.238703\n",
      "batch 905: loss 0.201630\n",
      "batch 906: loss 0.087462\n",
      "batch 907: loss 0.080458\n",
      "batch 908: loss 0.279005\n",
      "batch 909: loss 0.253656\n",
      "batch 910: loss 0.145916\n",
      "batch 911: loss 0.200959\n",
      "batch 912: loss 0.210361\n",
      "batch 913: loss 0.058339\n",
      "batch 914: loss 0.127011\n",
      "batch 915: loss 0.090924\n",
      "batch 916: loss 0.089799\n",
      "batch 917: loss 0.163831\n",
      "batch 918: loss 0.127861\n",
      "batch 919: loss 0.146212\n",
      "batch 920: loss 0.237276\n",
      "batch 921: loss 0.235179\n",
      "batch 922: loss 0.122895\n",
      "batch 923: loss 0.108458\n",
      "batch 924: loss 0.115473\n",
      "batch 925: loss 0.283318\n",
      "batch 926: loss 0.247876\n",
      "batch 927: loss 0.127142\n",
      "batch 928: loss 0.311257\n",
      "batch 929: loss 0.286091\n",
      "batch 930: loss 0.309156\n",
      "batch 931: loss 0.121847\n",
      "batch 932: loss 0.108401\n",
      "batch 933: loss 0.099522\n",
      "batch 934: loss 0.260383\n",
      "batch 935: loss 0.145375\n",
      "batch 936: loss 0.176906\n",
      "batch 937: loss 0.217446\n",
      "batch 938: loss 0.339360\n",
      "batch 939: loss 0.251507\n",
      "batch 940: loss 0.175292\n",
      "batch 941: loss 0.162265\n",
      "batch 942: loss 0.368155\n",
      "batch 943: loss 0.153551\n",
      "batch 944: loss 0.091623\n",
      "batch 945: loss 0.261311\n",
      "batch 946: loss 0.055566\n",
      "batch 947: loss 0.116876\n",
      "batch 948: loss 0.109986\n",
      "batch 949: loss 0.479257\n",
      "batch 950: loss 0.090520\n",
      "batch 951: loss 0.194525\n",
      "batch 952: loss 0.114464\n",
      "batch 953: loss 0.116213\n",
      "batch 954: loss 0.213247\n",
      "batch 955: loss 0.070370\n",
      "batch 956: loss 0.304874\n",
      "batch 957: loss 0.039723\n",
      "batch 958: loss 0.235287\n",
      "batch 959: loss 0.157483\n",
      "batch 960: loss 0.121697\n",
      "batch 961: loss 0.198793\n",
      "batch 962: loss 0.332271\n",
      "batch 963: loss 0.306455\n",
      "batch 964: loss 0.163871\n",
      "batch 965: loss 0.280567\n",
      "batch 966: loss 0.139469\n",
      "batch 967: loss 0.086891\n",
      "batch 968: loss 0.111271\n",
      "batch 969: loss 0.260540\n",
      "batch 970: loss 0.143739\n",
      "batch 971: loss 0.080632\n",
      "batch 972: loss 0.103133\n",
      "batch 973: loss 0.210007\n",
      "batch 974: loss 0.103692\n",
      "batch 975: loss 0.098358\n",
      "batch 976: loss 0.253748\n",
      "batch 977: loss 0.124947\n",
      "batch 978: loss 0.111486\n",
      "batch 979: loss 0.244452\n",
      "batch 980: loss 0.051799\n",
      "batch 981: loss 0.154407\n",
      "batch 982: loss 0.113499\n",
      "batch 983: loss 0.133893\n",
      "batch 984: loss 0.067153\n",
      "batch 985: loss 0.148680\n",
      "batch 986: loss 0.285059\n",
      "batch 987: loss 0.207385\n",
      "batch 988: loss 0.106333\n",
      "batch 989: loss 0.429106\n",
      "batch 990: loss 0.071417\n",
      "batch 991: loss 0.144463\n",
      "batch 992: loss 0.185426\n",
      "batch 993: loss 0.228305\n",
      "batch 994: loss 0.153802\n",
      "batch 995: loss 0.083283\n",
      "batch 996: loss 0.048689\n",
      "batch 997: loss 0.337432\n",
      "batch 998: loss 0.269309\n",
      "batch 999: loss 0.110953\n",
      "batch 1000: loss 0.206324\n",
      "batch 1001: loss 0.102328\n",
      "batch 1002: loss 0.270266\n",
      "batch 1003: loss 0.100504\n",
      "batch 1004: loss 0.175587\n",
      "batch 1005: loss 0.231608\n",
      "batch 1006: loss 0.049072\n",
      "batch 1007: loss 0.202748\n",
      "batch 1008: loss 0.225694\n",
      "batch 1009: loss 0.489637\n",
      "batch 1010: loss 0.156646\n",
      "batch 1011: loss 0.422559\n",
      "batch 1012: loss 0.161175\n",
      "batch 1013: loss 0.078572\n",
      "batch 1014: loss 0.145544\n",
      "batch 1015: loss 0.095771\n",
      "batch 1016: loss 0.299054\n",
      "batch 1017: loss 0.117425\n",
      "batch 1018: loss 0.112610\n",
      "batch 1019: loss 0.113564\n",
      "batch 1020: loss 0.198211\n",
      "batch 1021: loss 0.136667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1022: loss 0.079797\n",
      "batch 1023: loss 0.149820\n",
      "batch 1024: loss 0.325003\n",
      "batch 1025: loss 0.138606\n",
      "batch 1026: loss 0.118375\n",
      "batch 1027: loss 0.144403\n",
      "batch 1028: loss 0.102640\n",
      "batch 1029: loss 0.236083\n",
      "batch 1030: loss 0.071915\n",
      "batch 1031: loss 0.085338\n",
      "batch 1032: loss 0.053681\n",
      "batch 1033: loss 0.065566\n",
      "batch 1034: loss 0.193237\n",
      "batch 1035: loss 0.095986\n",
      "batch 1036: loss 0.135531\n",
      "batch 1037: loss 0.101196\n",
      "batch 1038: loss 0.262739\n",
      "batch 1039: loss 0.269792\n",
      "batch 1040: loss 0.107781\n",
      "batch 1041: loss 0.281506\n",
      "batch 1042: loss 0.019244\n",
      "batch 1043: loss 0.181595\n",
      "batch 1044: loss 0.080537\n",
      "batch 1045: loss 0.057033\n",
      "batch 1046: loss 0.130381\n",
      "batch 1047: loss 0.132931\n",
      "batch 1048: loss 0.164143\n",
      "batch 1049: loss 0.157836\n",
      "batch 1050: loss 0.064536\n",
      "batch 1051: loss 0.269267\n",
      "batch 1052: loss 0.157283\n",
      "batch 1053: loss 0.122124\n",
      "batch 1054: loss 0.099034\n",
      "batch 1055: loss 0.214776\n",
      "batch 1056: loss 0.271808\n",
      "batch 1057: loss 0.151808\n",
      "batch 1058: loss 0.131906\n",
      "batch 1059: loss 0.218510\n",
      "batch 1060: loss 0.090372\n",
      "batch 1061: loss 0.210911\n",
      "batch 1062: loss 0.138528\n",
      "batch 1063: loss 0.169906\n",
      "batch 1064: loss 0.153553\n",
      "batch 1065: loss 0.126610\n",
      "batch 1066: loss 0.178730\n",
      "batch 1067: loss 0.190847\n",
      "batch 1068: loss 0.149852\n",
      "batch 1069: loss 0.083427\n",
      "batch 1070: loss 0.096469\n",
      "batch 1071: loss 0.170753\n",
      "batch 1072: loss 0.198846\n",
      "batch 1073: loss 0.123092\n",
      "batch 1074: loss 0.034203\n",
      "batch 1075: loss 0.053134\n",
      "batch 1076: loss 0.155720\n",
      "batch 1077: loss 0.122277\n",
      "batch 1078: loss 0.070204\n",
      "batch 1079: loss 0.194879\n",
      "batch 1080: loss 0.259957\n",
      "batch 1081: loss 0.194558\n",
      "batch 1082: loss 0.070794\n",
      "batch 1083: loss 0.119077\n",
      "batch 1084: loss 0.266073\n",
      "batch 1085: loss 0.311437\n",
      "batch 1086: loss 0.270233\n",
      "batch 1087: loss 0.134929\n",
      "batch 1088: loss 0.122139\n",
      "batch 1089: loss 0.345053\n",
      "batch 1090: loss 0.345772\n",
      "batch 1091: loss 0.206766\n",
      "batch 1092: loss 0.152474\n",
      "batch 1093: loss 0.103662\n",
      "batch 1094: loss 0.115309\n",
      "batch 1095: loss 0.102171\n",
      "batch 1096: loss 0.180630\n",
      "batch 1097: loss 0.134521\n",
      "batch 1098: loss 0.087690\n",
      "batch 1099: loss 0.123160\n",
      "batch 1100: loss 0.125190\n",
      "batch 1101: loss 0.174068\n",
      "batch 1102: loss 0.111935\n",
      "batch 1103: loss 0.125746\n",
      "batch 1104: loss 0.112549\n",
      "batch 1105: loss 0.024579\n",
      "batch 1106: loss 0.136650\n",
      "batch 1107: loss 0.225757\n",
      "batch 1108: loss 0.190923\n",
      "batch 1109: loss 0.124233\n",
      "batch 1110: loss 0.125742\n",
      "batch 1111: loss 0.077837\n",
      "batch 1112: loss 0.039140\n",
      "batch 1113: loss 0.164876\n",
      "batch 1114: loss 0.099392\n",
      "batch 1115: loss 0.181354\n",
      "batch 1116: loss 0.051968\n",
      "batch 1117: loss 0.156164\n",
      "batch 1118: loss 0.198184\n",
      "batch 1119: loss 0.213455\n",
      "batch 1120: loss 0.030058\n",
      "batch 1121: loss 0.039282\n",
      "batch 1122: loss 0.097483\n",
      "batch 1123: loss 0.233475\n",
      "batch 1124: loss 0.120465\n",
      "batch 1125: loss 0.313682\n",
      "batch 1126: loss 0.209681\n",
      "batch 1127: loss 0.232622\n",
      "batch 1128: loss 0.211869\n",
      "batch 1129: loss 0.264611\n",
      "batch 1130: loss 0.100996\n",
      "batch 1131: loss 0.096697\n",
      "batch 1132: loss 0.133857\n",
      "batch 1133: loss 0.095842\n",
      "batch 1134: loss 0.114887\n",
      "batch 1135: loss 0.112336\n",
      "batch 1136: loss 0.272728\n",
      "batch 1137: loss 0.310691\n",
      "batch 1138: loss 0.241248\n",
      "batch 1139: loss 0.170685\n",
      "batch 1140: loss 0.121670\n",
      "batch 1141: loss 0.084172\n",
      "batch 1142: loss 0.080894\n",
      "batch 1143: loss 0.066034\n",
      "batch 1144: loss 0.145874\n",
      "batch 1145: loss 0.097267\n",
      "batch 1146: loss 0.096190\n",
      "batch 1147: loss 0.152973\n",
      "batch 1148: loss 0.064726\n",
      "batch 1149: loss 0.188339\n",
      "batch 1150: loss 0.068830\n",
      "batch 1151: loss 0.069649\n",
      "batch 1152: loss 0.103659\n",
      "batch 1153: loss 0.121322\n",
      "batch 1154: loss 0.092353\n",
      "batch 1155: loss 0.068640\n",
      "batch 1156: loss 0.098086\n",
      "batch 1157: loss 0.203455\n",
      "batch 1158: loss 0.129826\n",
      "batch 1159: loss 0.233433\n",
      "batch 1160: loss 0.259011\n",
      "batch 1161: loss 0.217672\n",
      "batch 1162: loss 0.059142\n",
      "batch 1163: loss 0.305445\n",
      "batch 1164: loss 0.208309\n",
      "batch 1165: loss 0.063798\n",
      "batch 1166: loss 0.164127\n",
      "batch 1167: loss 0.123161\n",
      "batch 1168: loss 0.298716\n",
      "batch 1169: loss 0.291123\n",
      "batch 1170: loss 0.192507\n",
      "batch 1171: loss 0.321635\n",
      "batch 1172: loss 0.140396\n",
      "batch 1173: loss 0.044460\n",
      "batch 1174: loss 0.126469\n",
      "batch 1175: loss 0.211120\n",
      "batch 1176: loss 0.096133\n",
      "batch 1177: loss 0.294385\n",
      "batch 1178: loss 0.239526\n",
      "batch 1179: loss 0.192129\n",
      "batch 1180: loss 0.335069\n",
      "batch 1181: loss 0.132494\n",
      "batch 1182: loss 0.167991\n",
      "batch 1183: loss 0.117341\n",
      "batch 1184: loss 0.077339\n",
      "batch 1185: loss 0.138942\n",
      "batch 1186: loss 0.287983\n",
      "batch 1187: loss 0.174821\n",
      "batch 1188: loss 0.093032\n",
      "batch 1189: loss 0.082716\n",
      "batch 1190: loss 0.087148\n",
      "batch 1191: loss 0.259999\n",
      "batch 1192: loss 0.300239\n",
      "batch 1193: loss 0.055040\n",
      "batch 1194: loss 0.082084\n",
      "batch 1195: loss 0.147694\n",
      "batch 1196: loss 0.193315\n",
      "batch 1197: loss 0.173522\n",
      "batch 1198: loss 0.143329\n",
      "batch 1199: loss 0.086377\n",
      "batch 1200: loss 0.324357\n",
      "batch 1201: loss 0.154331\n",
      "batch 1202: loss 0.086405\n",
      "batch 1203: loss 0.063221\n",
      "batch 1204: loss 0.034744\n",
      "batch 1205: loss 0.131882\n",
      "batch 1206: loss 0.115097\n",
      "batch 1207: loss 0.334497\n",
      "batch 1208: loss 0.087668\n",
      "batch 1209: loss 0.049527\n",
      "batch 1210: loss 0.090592\n",
      "batch 1211: loss 0.227928\n",
      "batch 1212: loss 0.133241\n",
      "batch 1213: loss 0.391379\n",
      "batch 1214: loss 0.166191\n",
      "batch 1215: loss 0.122521\n",
      "batch 1216: loss 0.247250\n",
      "batch 1217: loss 0.246791\n",
      "batch 1218: loss 0.190152\n",
      "batch 1219: loss 0.103908\n",
      "batch 1220: loss 0.132803\n",
      "batch 1221: loss 0.195935\n",
      "batch 1222: loss 0.166159\n",
      "batch 1223: loss 0.140885\n",
      "batch 1224: loss 0.130008\n",
      "batch 1225: loss 0.132143\n",
      "batch 1226: loss 0.271507\n",
      "batch 1227: loss 0.084439\n",
      "batch 1228: loss 0.164222\n",
      "batch 1229: loss 0.124718\n",
      "batch 1230: loss 0.164321\n",
      "batch 1231: loss 0.207986\n",
      "batch 1232: loss 0.198244\n",
      "batch 1233: loss 0.114145\n",
      "batch 1234: loss 0.148562\n",
      "batch 1235: loss 0.121242\n",
      "batch 1236: loss 0.113196\n",
      "batch 1237: loss 0.119856\n",
      "batch 1238: loss 0.114224\n",
      "batch 1239: loss 0.264253\n",
      "batch 1240: loss 0.294202\n",
      "batch 1241: loss 0.088597\n",
      "batch 1242: loss 0.184128\n",
      "batch 1243: loss 0.113710\n",
      "batch 1244: loss 0.196712\n",
      "batch 1245: loss 0.295720\n",
      "batch 1246: loss 0.141144\n",
      "batch 1247: loss 0.161584\n",
      "batch 1248: loss 0.063390\n",
      "batch 1249: loss 0.246904\n",
      "batch 1250: loss 0.179429\n",
      "batch 1251: loss 0.138998\n",
      "batch 1252: loss 0.056045\n",
      "batch 1253: loss 0.087318\n",
      "batch 1254: loss 0.240732\n",
      "batch 1255: loss 0.124306\n",
      "batch 1256: loss 0.076208\n",
      "batch 1257: loss 0.232685\n",
      "batch 1258: loss 0.142298\n",
      "batch 1259: loss 0.171745\n",
      "batch 1260: loss 0.255330\n",
      "batch 1261: loss 0.030737\n",
      "batch 1262: loss 0.220916\n",
      "batch 1263: loss 0.075941\n",
      "batch 1264: loss 0.165392\n",
      "batch 1265: loss 0.135824\n",
      "batch 1266: loss 0.092424\n",
      "batch 1267: loss 0.122527\n",
      "batch 1268: loss 0.173468\n",
      "batch 1269: loss 0.182619\n",
      "batch 1270: loss 0.193741\n",
      "batch 1271: loss 0.350868\n",
      "batch 1272: loss 0.222798\n",
      "batch 1273: loss 0.140844\n",
      "batch 1274: loss 0.255583\n",
      "batch 1275: loss 0.106906\n",
      "batch 1276: loss 0.135634\n",
      "batch 1277: loss 0.137970\n",
      "batch 1278: loss 0.176489\n",
      "batch 1279: loss 0.160945\n",
      "batch 1280: loss 0.136353\n",
      "batch 1281: loss 0.211323\n",
      "batch 1282: loss 0.098204\n",
      "batch 1283: loss 0.066941\n",
      "batch 1284: loss 0.119645\n",
      "batch 1285: loss 0.183966\n",
      "batch 1286: loss 0.199367\n",
      "batch 1287: loss 0.203510\n",
      "batch 1288: loss 0.249256\n",
      "batch 1289: loss 0.097003\n",
      "batch 1290: loss 0.088692\n",
      "batch 1291: loss 0.178380\n",
      "batch 1292: loss 0.168888\n",
      "batch 1293: loss 0.135915\n",
      "batch 1294: loss 0.105525\n",
      "batch 1295: loss 0.318455\n",
      "batch 1296: loss 0.088490\n",
      "batch 1297: loss 0.113364\n",
      "batch 1298: loss 0.201859\n",
      "batch 1299: loss 0.075519\n",
      "batch 1300: loss 0.213378\n",
      "batch 1301: loss 0.090653\n",
      "batch 1302: loss 0.080374\n",
      "batch 1303: loss 0.154969\n",
      "batch 1304: loss 0.271089\n",
      "batch 1305: loss 0.138335\n",
      "batch 1306: loss 0.109617\n",
      "batch 1307: loss 0.102729\n",
      "batch 1308: loss 0.258781\n",
      "batch 1309: loss 0.236150\n",
      "batch 1310: loss 0.087773\n",
      "batch 1311: loss 0.062801\n",
      "batch 1312: loss 0.067174\n",
      "batch 1313: loss 0.084847\n",
      "batch 1314: loss 0.049690\n",
      "batch 1315: loss 0.090350\n",
      "batch 1316: loss 0.080644\n",
      "batch 1317: loss 0.167362\n",
      "batch 1318: loss 0.132369\n",
      "batch 1319: loss 0.351124\n",
      "batch 1320: loss 0.200531\n",
      "batch 1321: loss 0.244203\n",
      "batch 1322: loss 0.081450\n",
      "batch 1323: loss 0.169593\n",
      "batch 1324: loss 0.211636\n",
      "batch 1325: loss 0.277319\n",
      "batch 1326: loss 0.147538\n",
      "batch 1327: loss 0.073090\n",
      "batch 1328: loss 0.204207\n",
      "batch 1329: loss 0.116813\n",
      "batch 1330: loss 0.086171\n",
      "batch 1331: loss 0.130118\n",
      "batch 1332: loss 0.180182\n",
      "batch 1333: loss 0.126872\n",
      "batch 1334: loss 0.074529\n",
      "batch 1335: loss 0.093405\n",
      "batch 1336: loss 0.232054\n",
      "batch 1337: loss 0.296499\n",
      "batch 1338: loss 0.138702\n",
      "batch 1339: loss 0.080082\n",
      "batch 1340: loss 0.214730\n",
      "batch 1341: loss 0.090223\n",
      "batch 1342: loss 0.099797\n",
      "batch 1343: loss 0.140656\n",
      "batch 1344: loss 0.639108\n",
      "batch 1345: loss 0.060376\n",
      "batch 1346: loss 0.066905\n",
      "batch 1347: loss 0.113510\n",
      "batch 1348: loss 0.128208\n",
      "batch 1349: loss 0.138415\n",
      "batch 1350: loss 0.112885\n",
      "batch 1351: loss 0.220389\n",
      "batch 1352: loss 0.188146\n",
      "batch 1353: loss 0.105664\n",
      "batch 1354: loss 0.074656\n",
      "batch 1355: loss 0.063982\n",
      "batch 1356: loss 0.296005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1357: loss 0.097024\n",
      "batch 1358: loss 0.327961\n",
      "batch 1359: loss 0.271296\n",
      "batch 1360: loss 0.065213\n",
      "batch 1361: loss 0.109159\n",
      "batch 1362: loss 0.149694\n",
      "batch 1363: loss 0.132359\n",
      "batch 1364: loss 0.159941\n",
      "batch 1365: loss 0.338057\n",
      "batch 1366: loss 0.143690\n",
      "batch 1367: loss 0.138126\n",
      "batch 1368: loss 0.140219\n",
      "batch 1369: loss 0.144624\n",
      "batch 1370: loss 0.137077\n",
      "batch 1371: loss 0.228258\n",
      "batch 1372: loss 0.151209\n",
      "batch 1373: loss 0.116681\n",
      "batch 1374: loss 0.205639\n",
      "batch 1375: loss 0.162341\n",
      "batch 1376: loss 0.144972\n",
      "batch 1377: loss 0.233726\n",
      "batch 1378: loss 0.256713\n",
      "batch 1379: loss 0.117442\n",
      "batch 1380: loss 0.131154\n",
      "batch 1381: loss 0.202371\n",
      "batch 1382: loss 0.167288\n",
      "batch 1383: loss 0.061871\n",
      "batch 1384: loss 0.115307\n",
      "batch 1385: loss 0.305838\n",
      "batch 1386: loss 0.024907\n",
      "batch 1387: loss 0.127639\n",
      "batch 1388: loss 0.129598\n",
      "batch 1389: loss 0.083454\n",
      "batch 1390: loss 0.118064\n",
      "batch 1391: loss 0.185637\n",
      "batch 1392: loss 0.200854\n",
      "batch 1393: loss 0.192245\n",
      "batch 1394: loss 0.208101\n",
      "batch 1395: loss 0.309766\n",
      "batch 1396: loss 0.038775\n",
      "batch 1397: loss 0.065290\n",
      "batch 1398: loss 0.040753\n",
      "batch 1399: loss 0.242606\n",
      "batch 1400: loss 0.101863\n",
      "batch 1401: loss 0.077298\n",
      "batch 1402: loss 0.075385\n",
      "batch 1403: loss 0.139791\n",
      "batch 1404: loss 0.082789\n",
      "batch 1405: loss 0.160679\n",
      "batch 1406: loss 0.049528\n",
      "batch 1407: loss 0.311358\n",
      "batch 1408: loss 0.109089\n",
      "batch 1409: loss 0.081481\n",
      "batch 1410: loss 0.101417\n",
      "batch 1411: loss 0.109485\n",
      "batch 1412: loss 0.129103\n",
      "batch 1413: loss 0.225406\n",
      "batch 1414: loss 0.207553\n",
      "batch 1415: loss 0.096387\n",
      "batch 1416: loss 0.223901\n",
      "batch 1417: loss 0.103415\n",
      "batch 1418: loss 0.180679\n",
      "batch 1419: loss 0.065616\n",
      "batch 1420: loss 0.136866\n",
      "batch 1421: loss 0.056965\n",
      "batch 1422: loss 0.075741\n",
      "batch 1423: loss 0.116770\n",
      "batch 1424: loss 0.073579\n",
      "batch 1425: loss 0.042216\n",
      "batch 1426: loss 0.130865\n",
      "batch 1427: loss 0.117064\n",
      "batch 1428: loss 0.227983\n",
      "batch 1429: loss 0.131716\n",
      "batch 1430: loss 0.050212\n",
      "batch 1431: loss 0.042071\n",
      "batch 1432: loss 0.201173\n",
      "batch 1433: loss 0.059746\n",
      "batch 1434: loss 0.182733\n",
      "batch 1435: loss 0.149397\n",
      "batch 1436: loss 0.236321\n",
      "batch 1437: loss 0.391036\n",
      "batch 1438: loss 0.101760\n",
      "batch 1439: loss 0.163868\n",
      "batch 1440: loss 0.193966\n",
      "batch 1441: loss 0.075732\n",
      "batch 1442: loss 0.162008\n",
      "batch 1443: loss 0.209967\n",
      "batch 1444: loss 0.077345\n",
      "batch 1445: loss 0.287910\n",
      "batch 1446: loss 0.139866\n",
      "batch 1447: loss 0.254716\n",
      "batch 1448: loss 0.402542\n",
      "batch 1449: loss 0.149205\n",
      "batch 1450: loss 0.154841\n",
      "batch 1451: loss 0.060365\n",
      "batch 1452: loss 0.079265\n",
      "batch 1453: loss 0.154574\n",
      "batch 1454: loss 0.188274\n",
      "batch 1455: loss 0.342227\n",
      "batch 1456: loss 0.038798\n",
      "batch 1457: loss 0.082374\n",
      "batch 1458: loss 0.090366\n",
      "batch 1459: loss 0.133662\n",
      "batch 1460: loss 0.058534\n",
      "batch 1461: loss 0.142040\n",
      "batch 1462: loss 0.147955\n",
      "batch 1463: loss 0.177870\n",
      "batch 1464: loss 0.258872\n",
      "batch 1465: loss 0.079962\n",
      "batch 1466: loss 0.090892\n",
      "batch 1467: loss 0.111536\n",
      "batch 1468: loss 0.190750\n",
      "batch 1469: loss 0.139996\n",
      "batch 1470: loss 0.095688\n",
      "batch 1471: loss 0.073241\n",
      "batch 1472: loss 0.166954\n",
      "batch 1473: loss 0.234195\n",
      "batch 1474: loss 0.187050\n",
      "batch 1475: loss 0.207982\n",
      "batch 1476: loss 0.059182\n",
      "batch 1477: loss 0.121806\n",
      "batch 1478: loss 0.062671\n",
      "batch 1479: loss 0.080896\n",
      "batch 1480: loss 0.125664\n",
      "batch 1481: loss 0.153945\n",
      "batch 1482: loss 0.106807\n",
      "batch 1483: loss 0.384089\n",
      "batch 1484: loss 0.042274\n",
      "batch 1485: loss 0.209339\n",
      "batch 1486: loss 0.187609\n",
      "batch 1487: loss 0.082559\n",
      "batch 1488: loss 0.068358\n",
      "batch 1489: loss 0.107066\n",
      "batch 1490: loss 0.064463\n",
      "batch 1491: loss 0.368352\n",
      "batch 1492: loss 0.153196\n",
      "batch 1493: loss 0.163922\n",
      "batch 1494: loss 0.040265\n",
      "batch 1495: loss 0.130862\n",
      "batch 1496: loss 0.154939\n",
      "batch 1497: loss 0.228473\n",
      "batch 1498: loss 0.152319\n",
      "batch 1499: loss 0.098032\n",
      "batch 1500: loss 0.129853\n",
      "batch 1501: loss 0.091741\n",
      "batch 1502: loss 0.070891\n",
      "batch 1503: loss 0.228075\n",
      "batch 1504: loss 0.449241\n",
      "batch 1505: loss 0.132956\n",
      "batch 1506: loss 0.174774\n",
      "batch 1507: loss 0.210014\n",
      "batch 1508: loss 0.040950\n",
      "batch 1509: loss 0.106513\n",
      "batch 1510: loss 0.041210\n",
      "batch 1511: loss 0.216832\n",
      "batch 1512: loss 0.141218\n",
      "batch 1513: loss 0.157540\n",
      "batch 1514: loss 0.076310\n",
      "batch 1515: loss 0.273371\n",
      "batch 1516: loss 0.089688\n",
      "batch 1517: loss 0.112437\n",
      "batch 1518: loss 0.216163\n",
      "batch 1519: loss 0.110388\n",
      "batch 1520: loss 0.126317\n",
      "batch 1521: loss 0.104616\n",
      "batch 1522: loss 0.082805\n",
      "batch 1523: loss 0.218878\n",
      "batch 1524: loss 0.156579\n",
      "batch 1525: loss 0.125527\n",
      "batch 1526: loss 0.269472\n",
      "batch 1527: loss 0.160872\n",
      "batch 1528: loss 0.102638\n",
      "batch 1529: loss 0.082747\n",
      "batch 1530: loss 0.087521\n",
      "batch 1531: loss 0.102478\n",
      "batch 1532: loss 0.084313\n",
      "batch 1533: loss 0.140024\n",
      "batch 1534: loss 0.052918\n",
      "batch 1535: loss 0.122928\n",
      "batch 1536: loss 0.215934\n",
      "batch 1537: loss 0.196101\n",
      "batch 1538: loss 0.030773\n",
      "batch 1539: loss 0.095432\n",
      "batch 1540: loss 0.105517\n",
      "batch 1541: loss 0.055922\n",
      "batch 1542: loss 0.155118\n",
      "batch 1543: loss 0.075049\n",
      "batch 1544: loss 0.276813\n",
      "batch 1545: loss 0.138845\n",
      "batch 1546: loss 0.095401\n",
      "batch 1547: loss 0.157687\n",
      "batch 1548: loss 0.050286\n",
      "batch 1549: loss 0.079763\n",
      "batch 1550: loss 0.159874\n",
      "batch 1551: loss 0.108382\n",
      "batch 1552: loss 0.288241\n",
      "batch 1553: loss 0.048608\n",
      "batch 1554: loss 0.093280\n",
      "batch 1555: loss 0.030746\n",
      "batch 1556: loss 0.097008\n",
      "batch 1557: loss 0.042595\n",
      "batch 1558: loss 0.074822\n",
      "batch 1559: loss 0.149897\n",
      "batch 1560: loss 0.076665\n",
      "batch 1561: loss 0.190453\n",
      "batch 1562: loss 0.091396\n",
      "batch 1563: loss 0.070128\n",
      "batch 1564: loss 0.066033\n",
      "batch 1565: loss 0.088163\n",
      "batch 1566: loss 0.122506\n",
      "batch 1567: loss 0.130144\n",
      "batch 1568: loss 0.075051\n",
      "batch 1569: loss 0.047833\n",
      "batch 1570: loss 0.065689\n",
      "batch 1571: loss 0.072982\n",
      "batch 1572: loss 0.103813\n",
      "batch 1573: loss 0.167848\n",
      "batch 1574: loss 0.047507\n",
      "batch 1575: loss 0.311531\n",
      "batch 1576: loss 0.091614\n",
      "batch 1577: loss 0.205727\n",
      "batch 1578: loss 0.280947\n",
      "batch 1579: loss 0.091782\n",
      "batch 1580: loss 0.277571\n",
      "batch 1581: loss 0.157825\n",
      "batch 1582: loss 0.064948\n",
      "batch 1583: loss 0.187281\n",
      "batch 1584: loss 0.106560\n",
      "batch 1585: loss 0.094967\n",
      "batch 1586: loss 0.271178\n",
      "batch 1587: loss 0.073916\n",
      "batch 1588: loss 0.103661\n",
      "batch 1589: loss 0.376888\n",
      "batch 1590: loss 0.108013\n",
      "batch 1591: loss 0.125614\n",
      "batch 1592: loss 0.098780\n",
      "batch 1593: loss 0.064047\n",
      "batch 1594: loss 0.219360\n",
      "batch 1595: loss 0.339437\n",
      "batch 1596: loss 0.116040\n",
      "batch 1597: loss 0.104007\n",
      "batch 1598: loss 0.197415\n",
      "batch 1599: loss 0.097387\n",
      "batch 1600: loss 0.094028\n",
      "batch 1601: loss 0.072694\n",
      "batch 1602: loss 0.039828\n",
      "batch 1603: loss 0.266253\n",
      "batch 1604: loss 0.042792\n",
      "batch 1605: loss 0.045876\n",
      "batch 1606: loss 0.077266\n",
      "batch 1607: loss 0.141591\n",
      "batch 1608: loss 0.062299\n",
      "batch 1609: loss 0.386173\n",
      "batch 1610: loss 0.193668\n",
      "batch 1611: loss 0.124729\n",
      "batch 1612: loss 0.112525\n",
      "batch 1613: loss 0.243400\n",
      "batch 1614: loss 0.166005\n",
      "batch 1615: loss 0.275016\n",
      "batch 1616: loss 0.079376\n",
      "batch 1617: loss 0.173230\n",
      "batch 1618: loss 0.102954\n",
      "batch 1619: loss 0.188656\n",
      "batch 1620: loss 0.105964\n",
      "batch 1621: loss 0.231942\n",
      "batch 1622: loss 0.211512\n",
      "batch 1623: loss 0.164848\n",
      "batch 1624: loss 0.070435\n",
      "batch 1625: loss 0.164537\n",
      "batch 1626: loss 0.168069\n",
      "batch 1627: loss 0.195874\n",
      "batch 1628: loss 0.158225\n",
      "batch 1629: loss 0.062909\n",
      "batch 1630: loss 0.249387\n",
      "batch 1631: loss 0.075550\n",
      "batch 1632: loss 0.047426\n",
      "batch 1633: loss 0.025605\n",
      "batch 1634: loss 0.049968\n",
      "batch 1635: loss 0.087925\n",
      "batch 1636: loss 0.187066\n",
      "batch 1637: loss 0.082507\n",
      "batch 1638: loss 0.281637\n",
      "batch 1639: loss 0.301132\n",
      "batch 1640: loss 0.192792\n",
      "batch 1641: loss 0.094433\n",
      "batch 1642: loss 0.165083\n",
      "batch 1643: loss 0.199510\n",
      "batch 1644: loss 0.237934\n",
      "batch 1645: loss 0.076170\n",
      "batch 1646: loss 0.058819\n",
      "batch 1647: loss 0.057549\n",
      "batch 1648: loss 0.204806\n",
      "batch 1649: loss 0.091441\n",
      "batch 1650: loss 0.060790\n",
      "batch 1651: loss 0.113297\n",
      "batch 1652: loss 0.091543\n",
      "batch 1653: loss 0.115285\n",
      "batch 1654: loss 0.244765\n",
      "batch 1655: loss 0.071424\n",
      "batch 1656: loss 0.054877\n",
      "batch 1657: loss 0.217078\n",
      "batch 1658: loss 0.172734\n",
      "batch 1659: loss 0.103226\n",
      "batch 1660: loss 0.240179\n",
      "batch 1661: loss 0.072344\n",
      "batch 1662: loss 0.348453\n",
      "batch 1663: loss 0.225996\n",
      "batch 1664: loss 0.233527\n",
      "batch 1665: loss 0.153865\n",
      "batch 1666: loss 0.149754\n",
      "batch 1667: loss 0.317735\n",
      "batch 1668: loss 0.057866\n",
      "batch 1669: loss 0.327495\n",
      "batch 1670: loss 0.137870\n",
      "batch 1671: loss 0.142837\n",
      "batch 1672: loss 0.081294\n",
      "batch 1673: loss 0.078731\n",
      "batch 1674: loss 0.233005\n",
      "batch 1675: loss 0.244727\n",
      "batch 1676: loss 0.069023\n",
      "batch 1677: loss 0.209587\n",
      "batch 1678: loss 0.075408\n",
      "batch 1679: loss 0.446386\n",
      "batch 1680: loss 0.091706\n",
      "batch 1681: loss 0.125472\n",
      "batch 1682: loss 0.061740\n",
      "batch 1683: loss 0.040685\n",
      "batch 1684: loss 0.085904\n",
      "batch 1685: loss 0.121271\n",
      "batch 1686: loss 0.218353\n",
      "batch 1687: loss 0.055742\n",
      "batch 1688: loss 0.072794\n",
      "batch 1689: loss 0.153279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1690: loss 0.241091\n",
      "batch 1691: loss 0.056262\n",
      "batch 1692: loss 0.053080\n",
      "batch 1693: loss 0.083063\n",
      "batch 1694: loss 0.119164\n",
      "batch 1695: loss 0.067520\n",
      "batch 1696: loss 0.029850\n",
      "batch 1697: loss 0.151135\n",
      "batch 1698: loss 0.154548\n",
      "batch 1699: loss 0.254188\n",
      "batch 1700: loss 0.250771\n",
      "batch 1701: loss 0.028574\n",
      "batch 1702: loss 0.039886\n",
      "batch 1703: loss 0.204351\n",
      "batch 1704: loss 0.154442\n",
      "batch 1705: loss 0.221737\n",
      "batch 1706: loss 0.273470\n",
      "batch 1707: loss 0.192523\n",
      "batch 1708: loss 0.119498\n",
      "batch 1709: loss 0.115590\n",
      "batch 1710: loss 0.123084\n",
      "batch 1711: loss 0.097932\n",
      "batch 1712: loss 0.150834\n",
      "batch 1713: loss 0.076623\n",
      "batch 1714: loss 0.104476\n",
      "batch 1715: loss 0.110978\n",
      "batch 1716: loss 0.155764\n",
      "batch 1717: loss 0.061334\n",
      "batch 1718: loss 0.035091\n",
      "batch 1719: loss 0.285435\n",
      "batch 1720: loss 0.062327\n",
      "batch 1721: loss 0.101380\n",
      "batch 1722: loss 0.218172\n",
      "batch 1723: loss 0.124555\n",
      "batch 1724: loss 0.344617\n",
      "batch 1725: loss 0.117195\n",
      "batch 1726: loss 0.062393\n",
      "batch 1727: loss 0.078101\n",
      "batch 1728: loss 0.230548\n",
      "batch 1729: loss 0.144491\n",
      "batch 1730: loss 0.183320\n",
      "batch 1731: loss 0.256720\n",
      "batch 1732: loss 0.041697\n",
      "batch 1733: loss 0.066700\n",
      "batch 1734: loss 0.223385\n",
      "batch 1735: loss 0.142413\n",
      "batch 1736: loss 0.258838\n",
      "batch 1737: loss 0.071952\n",
      "batch 1738: loss 0.137357\n",
      "batch 1739: loss 0.090411\n",
      "batch 1740: loss 0.114847\n",
      "batch 1741: loss 0.226045\n",
      "batch 1742: loss 0.072451\n",
      "batch 1743: loss 0.072734\n",
      "batch 1744: loss 0.109358\n",
      "batch 1745: loss 0.100937\n",
      "batch 1746: loss 0.314200\n",
      "batch 1747: loss 0.144878\n",
      "batch 1748: loss 0.287818\n",
      "batch 1749: loss 0.241969\n",
      "batch 1750: loss 0.101370\n",
      "batch 1751: loss 0.146491\n",
      "batch 1752: loss 0.038197\n",
      "batch 1753: loss 0.107419\n",
      "batch 1754: loss 0.066638\n",
      "batch 1755: loss 0.098490\n",
      "batch 1756: loss 0.249433\n",
      "batch 1757: loss 0.075057\n",
      "batch 1758: loss 0.127318\n",
      "batch 1759: loss 0.217072\n",
      "batch 1760: loss 0.120724\n",
      "batch 1761: loss 0.055847\n",
      "batch 1762: loss 0.028402\n",
      "batch 1763: loss 0.082428\n",
      "batch 1764: loss 0.078439\n",
      "batch 1765: loss 0.148336\n",
      "batch 1766: loss 0.025508\n",
      "batch 1767: loss 0.118239\n",
      "batch 1768: loss 0.071168\n",
      "batch 1769: loss 0.091499\n",
      "batch 1770: loss 0.358775\n",
      "batch 1771: loss 0.059374\n",
      "batch 1772: loss 0.115680\n",
      "batch 1773: loss 0.203175\n",
      "batch 1774: loss 0.154302\n",
      "batch 1775: loss 0.163241\n",
      "batch 1776: loss 0.072270\n",
      "batch 1777: loss 0.116682\n",
      "batch 1778: loss 0.159481\n",
      "batch 1779: loss 0.137360\n",
      "batch 1780: loss 0.106079\n",
      "batch 1781: loss 0.058170\n",
      "batch 1782: loss 0.225252\n",
      "batch 1783: loss 0.076539\n",
      "batch 1784: loss 0.151751\n",
      "batch 1785: loss 0.232917\n",
      "batch 1786: loss 0.047367\n",
      "batch 1787: loss 0.082008\n",
      "batch 1788: loss 0.282975\n",
      "batch 1789: loss 0.129961\n",
      "batch 1790: loss 0.078634\n",
      "batch 1791: loss 0.146532\n",
      "batch 1792: loss 0.233030\n",
      "batch 1793: loss 0.141725\n",
      "batch 1794: loss 0.089482\n",
      "batch 1795: loss 0.114429\n",
      "batch 1796: loss 0.096537\n",
      "batch 1797: loss 0.085580\n",
      "batch 1798: loss 0.044703\n",
      "batch 1799: loss 0.140478\n",
      "batch 1800: loss 0.057905\n",
      "batch 1801: loss 0.094798\n",
      "batch 1802: loss 0.062106\n",
      "batch 1803: loss 0.114200\n",
      "batch 1804: loss 0.168610\n",
      "batch 1805: loss 0.079625\n",
      "batch 1806: loss 0.071282\n",
      "batch 1807: loss 0.126324\n",
      "batch 1808: loss 0.149152\n",
      "batch 1809: loss 0.055072\n",
      "batch 1810: loss 0.083943\n",
      "batch 1811: loss 0.137394\n",
      "batch 1812: loss 0.130596\n",
      "batch 1813: loss 0.071344\n",
      "batch 1814: loss 0.128434\n",
      "batch 1815: loss 0.096096\n",
      "batch 1816: loss 0.138958\n",
      "batch 1817: loss 0.081239\n",
      "batch 1818: loss 0.137106\n",
      "batch 1819: loss 0.027891\n",
      "batch 1820: loss 0.112103\n",
      "batch 1821: loss 0.126667\n",
      "batch 1822: loss 0.160715\n",
      "batch 1823: loss 0.061009\n",
      "batch 1824: loss 0.127445\n",
      "batch 1825: loss 0.050021\n",
      "batch 1826: loss 0.041484\n",
      "batch 1827: loss 0.141541\n",
      "batch 1828: loss 0.255114\n",
      "batch 1829: loss 0.071668\n",
      "batch 1830: loss 0.173817\n",
      "batch 1831: loss 0.140877\n",
      "batch 1832: loss 0.111004\n",
      "batch 1833: loss 0.093022\n",
      "batch 1834: loss 0.101791\n",
      "batch 1835: loss 0.392273\n",
      "batch 1836: loss 0.027989\n",
      "batch 1837: loss 0.112041\n",
      "batch 1838: loss 0.034832\n",
      "batch 1839: loss 0.059846\n",
      "batch 1840: loss 0.113665\n",
      "batch 1841: loss 0.088614\n",
      "batch 1842: loss 0.049353\n",
      "batch 1843: loss 0.149151\n",
      "batch 1844: loss 0.053685\n",
      "batch 1845: loss 0.189222\n",
      "batch 1846: loss 0.270891\n",
      "batch 1847: loss 0.078652\n",
      "batch 1848: loss 0.167397\n",
      "batch 1849: loss 0.170825\n",
      "batch 1850: loss 0.124722\n",
      "batch 1851: loss 0.043738\n",
      "batch 1852: loss 0.090219\n",
      "batch 1853: loss 0.080816\n",
      "batch 1854: loss 0.076403\n",
      "batch 1855: loss 0.071425\n",
      "batch 1856: loss 0.121516\n",
      "batch 1857: loss 0.058060\n",
      "batch 1858: loss 0.090492\n",
      "batch 1859: loss 0.094209\n",
      "batch 1860: loss 0.065624\n",
      "batch 1861: loss 0.163905\n",
      "batch 1862: loss 0.093914\n",
      "batch 1863: loss 0.044544\n",
      "batch 1864: loss 0.115653\n",
      "batch 1865: loss 0.203659\n",
      "batch 1866: loss 0.057209\n",
      "batch 1867: loss 0.086609\n",
      "batch 1868: loss 0.187458\n",
      "batch 1869: loss 0.165028\n",
      "batch 1870: loss 0.121304\n",
      "batch 1871: loss 0.391792\n",
      "batch 1872: loss 0.188297\n",
      "batch 1873: loss 0.039642\n",
      "batch 1874: loss 0.086757\n",
      "batch 1875: loss 0.089061\n",
      "batch 1876: loss 0.113999\n",
      "batch 1877: loss 0.110078\n",
      "batch 1878: loss 0.042940\n",
      "batch 1879: loss 0.171964\n",
      "batch 1880: loss 0.148314\n",
      "batch 1881: loss 0.055522\n",
      "batch 1882: loss 0.077918\n",
      "batch 1883: loss 0.166527\n",
      "batch 1884: loss 0.123057\n",
      "batch 1885: loss 0.061287\n",
      "batch 1886: loss 0.072482\n",
      "batch 1887: loss 0.036272\n",
      "batch 1888: loss 0.132825\n",
      "batch 1889: loss 0.091389\n",
      "batch 1890: loss 0.055100\n",
      "batch 1891: loss 0.173756\n",
      "batch 1892: loss 0.062499\n",
      "batch 1893: loss 0.047028\n",
      "batch 1894: loss 0.214868\n",
      "batch 1895: loss 0.159025\n",
      "batch 1896: loss 0.059726\n",
      "batch 1897: loss 0.139931\n",
      "batch 1898: loss 0.088623\n",
      "batch 1899: loss 0.333176\n",
      "batch 1900: loss 0.040315\n",
      "batch 1901: loss 0.236179\n",
      "batch 1902: loss 0.129549\n",
      "batch 1903: loss 0.114305\n",
      "batch 1904: loss 0.044810\n",
      "batch 1905: loss 0.111515\n",
      "batch 1906: loss 0.114165\n",
      "batch 1907: loss 0.161729\n",
      "batch 1908: loss 0.097713\n",
      "batch 1909: loss 0.047942\n",
      "batch 1910: loss 0.120169\n",
      "batch 1911: loss 0.260721\n",
      "batch 1912: loss 0.098886\n",
      "batch 1913: loss 0.044573\n",
      "batch 1914: loss 0.154315\n",
      "batch 1915: loss 0.128941\n",
      "batch 1916: loss 0.057614\n",
      "batch 1917: loss 0.141731\n",
      "batch 1918: loss 0.026671\n",
      "batch 1919: loss 0.022226\n",
      "batch 1920: loss 0.064387\n",
      "batch 1921: loss 0.050853\n",
      "batch 1922: loss 0.159697\n",
      "batch 1923: loss 0.090189\n",
      "batch 1924: loss 0.051199\n",
      "batch 1925: loss 0.133517\n",
      "batch 1926: loss 0.420900\n",
      "batch 1927: loss 0.057565\n",
      "batch 1928: loss 0.107346\n",
      "batch 1929: loss 0.176334\n",
      "batch 1930: loss 0.107709\n",
      "batch 1931: loss 0.055435\n",
      "batch 1932: loss 0.146040\n",
      "batch 1933: loss 0.036573\n",
      "batch 1934: loss 0.216235\n",
      "batch 1935: loss 0.083791\n",
      "batch 1936: loss 0.070214\n",
      "batch 1937: loss 0.050540\n",
      "batch 1938: loss 0.064994\n",
      "batch 1939: loss 0.206642\n",
      "batch 1940: loss 0.062101\n",
      "batch 1941: loss 0.085937\n",
      "batch 1942: loss 0.030970\n",
      "batch 1943: loss 0.131222\n",
      "batch 1944: loss 0.158740\n",
      "batch 1945: loss 0.053212\n",
      "batch 1946: loss 0.219487\n",
      "batch 1947: loss 0.056882\n",
      "batch 1948: loss 0.096120\n",
      "batch 1949: loss 0.150771\n",
      "batch 1950: loss 0.105994\n",
      "batch 1951: loss 0.162279\n",
      "batch 1952: loss 0.244497\n",
      "batch 1953: loss 0.045413\n",
      "batch 1954: loss 0.053523\n",
      "batch 1955: loss 0.128884\n",
      "batch 1956: loss 0.132265\n",
      "batch 1957: loss 0.135462\n",
      "batch 1958: loss 0.091029\n",
      "batch 1959: loss 0.156910\n",
      "batch 1960: loss 0.036217\n",
      "batch 1961: loss 0.059747\n",
      "batch 1962: loss 0.155209\n",
      "batch 1963: loss 0.379366\n",
      "batch 1964: loss 0.201813\n",
      "batch 1965: loss 0.055445\n",
      "batch 1966: loss 0.328896\n",
      "batch 1967: loss 0.091798\n",
      "batch 1968: loss 0.220724\n",
      "batch 1969: loss 0.182854\n",
      "batch 1970: loss 0.251115\n",
      "batch 1971: loss 0.121721\n",
      "batch 1972: loss 0.039354\n",
      "batch 1973: loss 0.095360\n",
      "batch 1974: loss 0.081299\n",
      "batch 1975: loss 0.090468\n",
      "batch 1976: loss 0.168475\n",
      "batch 1977: loss 0.093065\n",
      "batch 1978: loss 0.241430\n",
      "batch 1979: loss 0.028053\n",
      "batch 1980: loss 0.065802\n",
      "batch 1981: loss 0.281059\n",
      "batch 1982: loss 0.201733\n",
      "batch 1983: loss 0.040705\n",
      "batch 1984: loss 0.116611\n",
      "batch 1985: loss 0.084488\n",
      "batch 1986: loss 0.035092\n",
      "batch 1987: loss 0.142505\n",
      "batch 1988: loss 0.114262\n",
      "batch 1989: loss 0.226139\n",
      "batch 1990: loss 0.058453\n",
      "batch 1991: loss 0.059163\n",
      "batch 1992: loss 0.182209\n",
      "batch 1993: loss 0.127195\n",
      "batch 1994: loss 0.179510\n",
      "batch 1995: loss 0.156064\n",
      "batch 1996: loss 0.132553\n",
      "batch 1997: loss 0.134100\n",
      "batch 1998: loss 0.248157\n",
      "batch 1999: loss 0.329248\n",
      "batch 2000: loss 0.139995\n",
      "batch 2001: loss 0.154686\n",
      "batch 2002: loss 0.163438\n",
      "batch 2003: loss 0.117966\n",
      "batch 2004: loss 0.059681\n",
      "batch 2005: loss 0.232854\n",
      "batch 2006: loss 0.106134\n",
      "batch 2007: loss 0.106208\n",
      "batch 2008: loss 0.197631\n",
      "batch 2009: loss 0.104099\n",
      "batch 2010: loss 0.246790\n",
      "batch 2011: loss 0.189878\n",
      "batch 2012: loss 0.153708\n",
      "batch 2013: loss 0.220272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2014: loss 0.077986\n",
      "batch 2015: loss 0.033786\n",
      "batch 2016: loss 0.073165\n",
      "batch 2017: loss 0.125136\n",
      "batch 2018: loss 0.022333\n",
      "batch 2019: loss 0.241528\n",
      "batch 2020: loss 0.285413\n",
      "batch 2021: loss 0.038389\n",
      "batch 2022: loss 0.086380\n",
      "batch 2023: loss 0.211836\n",
      "batch 2024: loss 0.167577\n",
      "batch 2025: loss 0.167371\n",
      "batch 2026: loss 0.080038\n",
      "batch 2027: loss 0.052342\n",
      "batch 2028: loss 0.052427\n",
      "batch 2029: loss 0.117528\n",
      "batch 2030: loss 0.044624\n",
      "batch 2031: loss 0.179115\n",
      "batch 2032: loss 0.084399\n",
      "batch 2033: loss 0.081627\n",
      "batch 2034: loss 0.102387\n",
      "batch 2035: loss 0.044944\n",
      "batch 2036: loss 0.027815\n",
      "batch 2037: loss 0.061527\n",
      "batch 2038: loss 0.044473\n",
      "batch 2039: loss 0.140294\n",
      "batch 2040: loss 0.221369\n",
      "batch 2041: loss 0.079355\n",
      "batch 2042: loss 0.061175\n",
      "batch 2043: loss 0.157340\n",
      "batch 2044: loss 0.180752\n",
      "batch 2045: loss 0.052998\n",
      "batch 2046: loss 0.038037\n",
      "batch 2047: loss 0.128735\n",
      "batch 2048: loss 0.116784\n",
      "batch 2049: loss 0.122239\n",
      "batch 2050: loss 0.198094\n",
      "batch 2051: loss 0.247445\n",
      "batch 2052: loss 0.149909\n",
      "batch 2053: loss 0.122304\n",
      "batch 2054: loss 0.042182\n",
      "batch 2055: loss 0.358541\n",
      "batch 2056: loss 0.089521\n",
      "batch 2057: loss 0.108606\n",
      "batch 2058: loss 0.115376\n",
      "batch 2059: loss 0.095976\n",
      "batch 2060: loss 0.045297\n",
      "batch 2061: loss 0.048179\n",
      "batch 2062: loss 0.030464\n",
      "batch 2063: loss 0.082980\n",
      "batch 2064: loss 0.224368\n",
      "batch 2065: loss 0.111758\n",
      "batch 2066: loss 0.142683\n",
      "batch 2067: loss 0.077035\n",
      "batch 2068: loss 0.056214\n",
      "batch 2069: loss 0.172421\n",
      "batch 2070: loss 0.093221\n",
      "batch 2071: loss 0.100087\n",
      "batch 2072: loss 0.205630\n",
      "batch 2073: loss 0.079150\n",
      "batch 2074: loss 0.122228\n",
      "batch 2075: loss 0.145861\n",
      "batch 2076: loss 0.070203\n",
      "batch 2077: loss 0.088596\n",
      "batch 2078: loss 0.141361\n",
      "batch 2079: loss 0.047099\n",
      "batch 2080: loss 0.073777\n",
      "batch 2081: loss 0.201050\n",
      "batch 2082: loss 0.095198\n",
      "batch 2083: loss 0.035610\n",
      "batch 2084: loss 0.070038\n",
      "batch 2085: loss 0.090451\n",
      "batch 2086: loss 0.103931\n",
      "batch 2087: loss 0.166348\n",
      "batch 2088: loss 0.275205\n",
      "batch 2089: loss 0.138331\n",
      "batch 2090: loss 0.094354\n",
      "batch 2091: loss 0.035982\n",
      "batch 2092: loss 0.145465\n",
      "batch 2093: loss 0.136019\n",
      "batch 2094: loss 0.141701\n",
      "batch 2095: loss 0.149696\n",
      "batch 2096: loss 0.091537\n",
      "batch 2097: loss 0.140572\n",
      "batch 2098: loss 0.050128\n",
      "batch 2099: loss 0.046689\n",
      "batch 2100: loss 0.124967\n",
      "batch 2101: loss 0.047888\n",
      "batch 2102: loss 0.042099\n",
      "batch 2103: loss 0.104852\n",
      "batch 2104: loss 0.218727\n",
      "batch 2105: loss 0.150606\n",
      "batch 2106: loss 0.166708\n",
      "batch 2107: loss 0.146170\n",
      "batch 2108: loss 0.053600\n",
      "batch 2109: loss 0.026836\n",
      "batch 2110: loss 0.034199\n",
      "batch 2111: loss 0.081082\n",
      "batch 2112: loss 0.149238\n",
      "batch 2113: loss 0.191042\n",
      "batch 2114: loss 0.124487\n",
      "batch 2115: loss 0.094785\n",
      "batch 2116: loss 0.210127\n",
      "batch 2117: loss 0.088529\n",
      "batch 2118: loss 0.137585\n",
      "batch 2119: loss 0.192442\n",
      "batch 2120: loss 0.271259\n",
      "batch 2121: loss 0.095218\n",
      "batch 2122: loss 0.031684\n",
      "batch 2123: loss 0.033008\n",
      "batch 2124: loss 0.202515\n",
      "batch 2125: loss 0.083340\n",
      "batch 2126: loss 0.065247\n",
      "batch 2127: loss 0.126138\n",
      "batch 2128: loss 0.058844\n",
      "batch 2129: loss 0.237642\n",
      "batch 2130: loss 0.044099\n",
      "batch 2131: loss 0.356355\n",
      "batch 2132: loss 0.271174\n",
      "batch 2133: loss 0.106008\n",
      "batch 2134: loss 0.125903\n",
      "batch 2135: loss 0.205092\n",
      "batch 2136: loss 0.062874\n",
      "batch 2137: loss 0.124504\n",
      "batch 2138: loss 0.094735\n",
      "batch 2139: loss 0.103173\n",
      "batch 2140: loss 0.085317\n",
      "batch 2141: loss 0.132717\n",
      "batch 2142: loss 0.037018\n",
      "batch 2143: loss 0.220920\n",
      "batch 2144: loss 0.031011\n",
      "batch 2145: loss 0.036785\n",
      "batch 2146: loss 0.091033\n",
      "batch 2147: loss 0.133509\n",
      "batch 2148: loss 0.175435\n",
      "batch 2149: loss 0.070434\n",
      "batch 2150: loss 0.172149\n",
      "batch 2151: loss 0.172685\n",
      "batch 2152: loss 0.211146\n",
      "batch 2153: loss 0.163959\n",
      "batch 2154: loss 0.117808\n",
      "batch 2155: loss 0.189412\n",
      "batch 2156: loss 0.125243\n",
      "batch 2157: loss 0.081730\n",
      "batch 2158: loss 0.079366\n",
      "batch 2159: loss 0.049229\n",
      "batch 2160: loss 0.117963\n",
      "batch 2161: loss 0.036123\n",
      "batch 2162: loss 0.111032\n",
      "batch 2163: loss 0.047352\n",
      "batch 2164: loss 0.023133\n",
      "batch 2165: loss 0.052989\n",
      "batch 2166: loss 0.072974\n",
      "batch 2167: loss 0.024177\n",
      "batch 2168: loss 0.140727\n",
      "batch 2169: loss 0.356873\n",
      "batch 2170: loss 0.122826\n",
      "batch 2171: loss 0.066249\n",
      "batch 2172: loss 0.030519\n",
      "batch 2173: loss 0.118734\n",
      "batch 2174: loss 0.080081\n",
      "batch 2175: loss 0.069786\n",
      "batch 2176: loss 0.105439\n",
      "batch 2177: loss 0.055844\n",
      "batch 2178: loss 0.088593\n",
      "batch 2179: loss 0.027037\n",
      "batch 2180: loss 0.081131\n",
      "batch 2181: loss 0.057514\n",
      "batch 2182: loss 0.081615\n",
      "batch 2183: loss 0.162271\n",
      "batch 2184: loss 0.189955\n",
      "batch 2185: loss 0.129349\n",
      "batch 2186: loss 0.112996\n",
      "batch 2187: loss 0.109334\n",
      "batch 2188: loss 0.278243\n",
      "batch 2189: loss 0.140482\n",
      "batch 2190: loss 0.047056\n",
      "batch 2191: loss 0.065113\n",
      "batch 2192: loss 0.266168\n",
      "batch 2193: loss 0.077669\n",
      "batch 2194: loss 0.062636\n",
      "batch 2195: loss 0.123469\n",
      "batch 2196: loss 0.103578\n",
      "batch 2197: loss 0.093715\n",
      "batch 2198: loss 0.164609\n",
      "batch 2199: loss 0.079310\n",
      "batch 2200: loss 0.056708\n",
      "batch 2201: loss 0.063761\n",
      "batch 2202: loss 0.082689\n",
      "batch 2203: loss 0.032453\n",
      "batch 2204: loss 0.211195\n",
      "batch 2205: loss 0.041895\n",
      "batch 2206: loss 0.083182\n",
      "batch 2207: loss 0.081202\n",
      "batch 2208: loss 0.041938\n",
      "batch 2209: loss 0.083566\n",
      "batch 2210: loss 0.053172\n",
      "batch 2211: loss 0.061557\n",
      "batch 2212: loss 0.150931\n",
      "batch 2213: loss 0.051711\n",
      "batch 2214: loss 0.076212\n",
      "batch 2215: loss 0.184561\n",
      "batch 2216: loss 0.094751\n",
      "batch 2217: loss 0.256492\n",
      "batch 2218: loss 0.138727\n",
      "batch 2219: loss 0.043445\n",
      "batch 2220: loss 0.054973\n",
      "batch 2221: loss 0.084488\n",
      "batch 2222: loss 0.193542\n",
      "batch 2223: loss 0.085938\n",
      "batch 2224: loss 0.246872\n",
      "batch 2225: loss 0.063592\n",
      "batch 2226: loss 0.088577\n",
      "batch 2227: loss 0.170863\n",
      "batch 2228: loss 0.101340\n",
      "batch 2229: loss 0.039851\n",
      "batch 2230: loss 0.065212\n",
      "batch 2231: loss 0.105854\n",
      "batch 2232: loss 0.119512\n",
      "batch 2233: loss 0.199124\n",
      "batch 2234: loss 0.157563\n",
      "batch 2235: loss 0.094656\n",
      "batch 2236: loss 0.085826\n",
      "batch 2237: loss 0.025922\n",
      "batch 2238: loss 0.238329\n",
      "batch 2239: loss 0.119946\n",
      "batch 2240: loss 0.135901\n",
      "batch 2241: loss 0.245083\n",
      "batch 2242: loss 0.180744\n",
      "batch 2243: loss 0.073600\n",
      "batch 2244: loss 0.104177\n",
      "batch 2245: loss 0.199759\n",
      "batch 2246: loss 0.209099\n",
      "batch 2247: loss 0.105129\n",
      "batch 2248: loss 0.077017\n",
      "batch 2249: loss 0.060118\n",
      "batch 2250: loss 0.105116\n",
      "batch 2251: loss 0.320681\n",
      "batch 2252: loss 0.065034\n",
      "batch 2253: loss 0.058185\n",
      "batch 2254: loss 0.045639\n",
      "batch 2255: loss 0.025153\n",
      "batch 2256: loss 0.071454\n",
      "batch 2257: loss 0.162097\n",
      "batch 2258: loss 0.154277\n",
      "batch 2259: loss 0.070654\n",
      "batch 2260: loss 0.079286\n",
      "batch 2261: loss 0.244578\n",
      "batch 2262: loss 0.074886\n",
      "batch 2263: loss 0.151494\n",
      "batch 2264: loss 0.132559\n",
      "batch 2265: loss 0.071715\n",
      "batch 2266: loss 0.136459\n",
      "batch 2267: loss 0.101197\n",
      "batch 2268: loss 0.117205\n",
      "batch 2269: loss 0.055151\n",
      "batch 2270: loss 0.054099\n",
      "batch 2271: loss 0.195341\n",
      "batch 2272: loss 0.034303\n",
      "batch 2273: loss 0.240256\n",
      "batch 2274: loss 0.025312\n",
      "batch 2275: loss 0.180827\n",
      "batch 2276: loss 0.150567\n",
      "batch 2277: loss 0.198403\n",
      "batch 2278: loss 0.191193\n",
      "batch 2279: loss 0.070914\n",
      "batch 2280: loss 0.178030\n",
      "batch 2281: loss 0.112159\n",
      "batch 2282: loss 0.057292\n",
      "batch 2283: loss 0.086142\n",
      "batch 2284: loss 0.137433\n",
      "batch 2285: loss 0.052610\n",
      "batch 2286: loss 0.112105\n",
      "batch 2287: loss 0.067031\n",
      "batch 2288: loss 0.189631\n",
      "batch 2289: loss 0.023749\n",
      "batch 2290: loss 0.290806\n",
      "batch 2291: loss 0.030247\n",
      "batch 2292: loss 0.104561\n",
      "batch 2293: loss 0.163552\n",
      "batch 2294: loss 0.134369\n",
      "batch 2295: loss 0.093933\n",
      "batch 2296: loss 0.110988\n",
      "batch 2297: loss 0.151256\n",
      "batch 2298: loss 0.104150\n",
      "batch 2299: loss 0.196569\n",
      "batch 2300: loss 0.182878\n",
      "batch 2301: loss 0.100951\n",
      "batch 2302: loss 0.137187\n",
      "batch 2303: loss 0.034316\n",
      "batch 2304: loss 0.038038\n",
      "batch 2305: loss 0.242318\n",
      "batch 2306: loss 0.238591\n",
      "batch 2307: loss 0.039162\n",
      "batch 2308: loss 0.055371\n",
      "batch 2309: loss 0.104402\n",
      "batch 2310: loss 0.118950\n",
      "batch 2311: loss 0.020216\n",
      "batch 2312: loss 0.072989\n",
      "batch 2313: loss 0.090012\n",
      "batch 2314: loss 0.119209\n",
      "batch 2315: loss 0.091483\n",
      "batch 2316: loss 0.086721\n",
      "batch 2317: loss 0.138103\n",
      "batch 2318: loss 0.072759\n",
      "batch 2319: loss 0.048450\n",
      "batch 2320: loss 0.080166\n",
      "batch 2321: loss 0.224036\n",
      "batch 2322: loss 0.095615\n",
      "batch 2323: loss 0.120441\n",
      "batch 2324: loss 0.125068\n",
      "batch 2325: loss 0.029524\n",
      "batch 2326: loss 0.086935\n",
      "batch 2327: loss 0.281053\n",
      "batch 2328: loss 0.189079\n",
      "batch 2329: loss 0.158974\n",
      "batch 2330: loss 0.054686\n",
      "batch 2331: loss 0.121223\n",
      "batch 2332: loss 0.091284\n",
      "batch 2333: loss 0.037312\n",
      "batch 2334: loss 0.051622\n",
      "batch 2335: loss 0.118536\n",
      "batch 2336: loss 0.183005\n",
      "batch 2337: loss 0.056761\n",
      "batch 2338: loss 0.104870\n",
      "batch 2339: loss 0.077306\n",
      "batch 2340: loss 0.103030\n",
      "batch 2341: loss 0.483603\n",
      "batch 2342: loss 0.217342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2343: loss 0.148343\n",
      "batch 2344: loss 0.068670\n",
      "batch 2345: loss 0.026190\n",
      "batch 2346: loss 0.109798\n",
      "batch 2347: loss 0.112895\n",
      "batch 2348: loss 0.127004\n",
      "batch 2349: loss 0.086230\n",
      "batch 2350: loss 0.043898\n",
      "batch 2351: loss 0.065813\n",
      "batch 2352: loss 0.095528\n",
      "batch 2353: loss 0.110921\n",
      "batch 2354: loss 0.124681\n",
      "batch 2355: loss 0.120023\n",
      "batch 2356: loss 0.074536\n",
      "batch 2357: loss 0.190580\n",
      "batch 2358: loss 0.100361\n",
      "batch 2359: loss 0.079969\n",
      "batch 2360: loss 0.077012\n",
      "batch 2361: loss 0.056435\n",
      "batch 2362: loss 0.088743\n",
      "batch 2363: loss 0.199812\n",
      "batch 2364: loss 0.018364\n",
      "batch 2365: loss 0.152741\n",
      "batch 2366: loss 0.145925\n",
      "batch 2367: loss 0.054700\n",
      "batch 2368: loss 0.072773\n",
      "batch 2369: loss 0.125325\n",
      "batch 2370: loss 0.057196\n",
      "batch 2371: loss 0.041700\n",
      "batch 2372: loss 0.191131\n",
      "batch 2373: loss 0.018224\n",
      "batch 2374: loss 0.055630\n",
      "batch 2375: loss 0.077755\n",
      "batch 2376: loss 0.153765\n",
      "batch 2377: loss 0.102269\n",
      "batch 2378: loss 0.126227\n",
      "batch 2379: loss 0.114110\n",
      "batch 2380: loss 0.219170\n",
      "batch 2381: loss 0.104990\n",
      "batch 2382: loss 0.273544\n",
      "batch 2383: loss 0.055483\n",
      "batch 2384: loss 0.075702\n",
      "batch 2385: loss 0.155167\n",
      "batch 2386: loss 0.041808\n",
      "batch 2387: loss 0.051783\n",
      "batch 2388: loss 0.083768\n",
      "batch 2389: loss 0.129633\n",
      "batch 2390: loss 0.113028\n",
      "batch 2391: loss 0.119896\n",
      "batch 2392: loss 0.102101\n",
      "batch 2393: loss 0.064726\n",
      "batch 2394: loss 0.053092\n",
      "batch 2395: loss 0.202569\n",
      "batch 2396: loss 0.111225\n",
      "batch 2397: loss 0.123119\n",
      "batch 2398: loss 0.047622\n",
      "batch 2399: loss 0.062001\n",
      "batch 2400: loss 0.100648\n",
      "batch 2401: loss 0.068505\n",
      "batch 2402: loss 0.085620\n",
      "batch 2403: loss 0.110032\n",
      "batch 2404: loss 0.229101\n",
      "batch 2405: loss 0.031637\n",
      "batch 2406: loss 0.021783\n",
      "batch 2407: loss 0.108780\n",
      "batch 2408: loss 0.172172\n",
      "batch 2409: loss 0.035864\n",
      "batch 2410: loss 0.062040\n",
      "batch 2411: loss 0.030727\n",
      "batch 2412: loss 0.045682\n",
      "batch 2413: loss 0.128258\n",
      "batch 2414: loss 0.039390\n",
      "batch 2415: loss 0.101252\n",
      "batch 2416: loss 0.021516\n",
      "batch 2417: loss 0.071109\n",
      "batch 2418: loss 0.068969\n",
      "batch 2419: loss 0.071406\n",
      "batch 2420: loss 0.032955\n",
      "batch 2421: loss 0.043743\n",
      "batch 2422: loss 0.172641\n",
      "batch 2423: loss 0.099596\n",
      "batch 2424: loss 0.080028\n",
      "batch 2425: loss 0.238905\n",
      "batch 2426: loss 0.117225\n",
      "batch 2427: loss 0.228607\n",
      "batch 2428: loss 0.053984\n",
      "batch 2429: loss 0.075500\n",
      "batch 2430: loss 0.083559\n",
      "batch 2431: loss 0.069176\n",
      "batch 2432: loss 0.090238\n",
      "batch 2433: loss 0.071751\n",
      "batch 2434: loss 0.107556\n",
      "batch 2435: loss 0.091816\n",
      "batch 2436: loss 0.059279\n",
      "batch 2437: loss 0.053715\n",
      "batch 2438: loss 0.145251\n",
      "batch 2439: loss 0.187520\n",
      "batch 2440: loss 0.042977\n",
      "batch 2441: loss 0.122712\n",
      "batch 2442: loss 0.064417\n",
      "batch 2443: loss 0.194681\n",
      "batch 2444: loss 0.122485\n",
      "batch 2445: loss 0.182406\n",
      "batch 2446: loss 0.021045\n",
      "batch 2447: loss 0.059745\n",
      "batch 2448: loss 0.096880\n",
      "batch 2449: loss 0.041050\n",
      "batch 2450: loss 0.184378\n",
      "batch 2451: loss 0.124174\n",
      "batch 2452: loss 0.036285\n",
      "batch 2453: loss 0.081467\n",
      "batch 2454: loss 0.120921\n",
      "batch 2455: loss 0.044569\n",
      "batch 2456: loss 0.159412\n",
      "batch 2457: loss 0.083381\n",
      "batch 2458: loss 0.101454\n",
      "batch 2459: loss 0.024559\n",
      "batch 2460: loss 0.093806\n",
      "batch 2461: loss 0.117735\n",
      "batch 2462: loss 0.062655\n",
      "batch 2463: loss 0.236019\n",
      "batch 2464: loss 0.107313\n",
      "batch 2465: loss 0.187770\n",
      "batch 2466: loss 0.083838\n",
      "batch 2467: loss 0.074054\n",
      "batch 2468: loss 0.062013\n",
      "batch 2469: loss 0.137456\n",
      "batch 2470: loss 0.104463\n",
      "batch 2471: loss 0.026835\n",
      "batch 2472: loss 0.059770\n",
      "batch 2473: loss 0.199829\n",
      "batch 2474: loss 0.075863\n",
      "batch 2475: loss 0.141717\n",
      "batch 2476: loss 0.039110\n",
      "batch 2477: loss 0.033826\n",
      "batch 2478: loss 0.067519\n",
      "batch 2479: loss 0.047805\n",
      "batch 2480: loss 0.080444\n",
      "batch 2481: loss 0.025137\n",
      "batch 2482: loss 0.053969\n",
      "batch 2483: loss 0.028623\n",
      "batch 2484: loss 0.137498\n",
      "batch 2485: loss 0.210206\n",
      "batch 2486: loss 0.058276\n",
      "batch 2487: loss 0.062196\n",
      "batch 2488: loss 0.118355\n",
      "batch 2489: loss 0.048450\n",
      "batch 2490: loss 0.113230\n",
      "batch 2491: loss 0.101974\n",
      "batch 2492: loss 0.190119\n",
      "batch 2493: loss 0.155265\n",
      "batch 2494: loss 0.166611\n",
      "batch 2495: loss 0.155232\n",
      "batch 2496: loss 0.098263\n",
      "batch 2497: loss 0.115588\n",
      "batch 2498: loss 0.038288\n",
      "batch 2499: loss 0.074666\n",
      "batch 2500: loss 0.049542\n",
      "batch 2501: loss 0.156047\n",
      "batch 2502: loss 0.035210\n",
      "batch 2503: loss 0.077310\n",
      "batch 2504: loss 0.043061\n",
      "batch 2505: loss 0.057014\n",
      "batch 2506: loss 0.113916\n",
      "batch 2507: loss 0.166368\n",
      "batch 2508: loss 0.187721\n",
      "batch 2509: loss 0.065650\n",
      "batch 2510: loss 0.149631\n",
      "batch 2511: loss 0.128835\n",
      "batch 2512: loss 0.061402\n",
      "batch 2513: loss 0.129308\n",
      "batch 2514: loss 0.051957\n",
      "batch 2515: loss 0.067475\n",
      "batch 2516: loss 0.049393\n",
      "batch 2517: loss 0.050026\n",
      "batch 2518: loss 0.112153\n",
      "batch 2519: loss 0.079639\n",
      "batch 2520: loss 0.087395\n",
      "batch 2521: loss 0.049458\n",
      "batch 2522: loss 0.228944\n",
      "batch 2523: loss 0.179241\n",
      "batch 2524: loss 0.064944\n",
      "batch 2525: loss 0.122355\n",
      "batch 2526: loss 0.070900\n",
      "batch 2527: loss 0.062539\n",
      "batch 2528: loss 0.061378\n",
      "batch 2529: loss 0.036766\n",
      "batch 2530: loss 0.168405\n",
      "batch 2531: loss 0.131736\n",
      "batch 2532: loss 0.220182\n",
      "batch 2533: loss 0.037161\n",
      "batch 2534: loss 0.115981\n",
      "batch 2535: loss 0.029355\n",
      "batch 2536: loss 0.138572\n",
      "batch 2537: loss 0.024207\n",
      "batch 2538: loss 0.078435\n",
      "batch 2539: loss 0.184640\n",
      "batch 2540: loss 0.057801\n",
      "batch 2541: loss 0.140421\n",
      "batch 2542: loss 0.093611\n",
      "batch 2543: loss 0.116747\n",
      "batch 2544: loss 0.058585\n",
      "batch 2545: loss 0.147679\n",
      "batch 2546: loss 0.072031\n",
      "batch 2547: loss 0.079927\n",
      "batch 2548: loss 0.068462\n",
      "batch 2549: loss 0.061113\n",
      "batch 2550: loss 0.019013\n",
      "batch 2551: loss 0.036713\n",
      "batch 2552: loss 0.116059\n",
      "batch 2553: loss 0.204762\n",
      "batch 2554: loss 0.044689\n",
      "batch 2555: loss 0.025147\n",
      "batch 2556: loss 0.113435\n",
      "batch 2557: loss 0.087935\n",
      "batch 2558: loss 0.068857\n",
      "batch 2559: loss 0.093896\n",
      "batch 2560: loss 0.032806\n",
      "batch 2561: loss 0.046462\n",
      "batch 2562: loss 0.118262\n",
      "batch 2563: loss 0.132139\n",
      "batch 2564: loss 0.100273\n",
      "batch 2565: loss 0.113273\n",
      "batch 2566: loss 0.060634\n",
      "batch 2567: loss 0.025099\n",
      "batch 2568: loss 0.145861\n",
      "batch 2569: loss 0.027124\n",
      "batch 2570: loss 0.027800\n",
      "batch 2571: loss 0.059394\n",
      "batch 2572: loss 0.049612\n",
      "batch 2573: loss 0.128427\n",
      "batch 2574: loss 0.086442\n",
      "batch 2575: loss 0.065314\n",
      "batch 2576: loss 0.196275\n",
      "batch 2577: loss 0.139504\n",
      "batch 2578: loss 0.186204\n",
      "batch 2579: loss 0.114164\n",
      "batch 2580: loss 0.216590\n",
      "batch 2581: loss 0.080848\n",
      "batch 2582: loss 0.108148\n",
      "batch 2583: loss 0.096505\n",
      "batch 2584: loss 0.177605\n",
      "batch 2585: loss 0.041105\n",
      "batch 2586: loss 0.061637\n",
      "batch 2587: loss 0.054025\n",
      "batch 2588: loss 0.075893\n",
      "batch 2589: loss 0.090101\n",
      "batch 2590: loss 0.054573\n",
      "batch 2591: loss 0.215251\n",
      "batch 2592: loss 0.042504\n",
      "batch 2593: loss 0.045335\n",
      "batch 2594: loss 0.314491\n",
      "batch 2595: loss 0.197580\n",
      "batch 2596: loss 0.159993\n",
      "batch 2597: loss 0.070142\n",
      "batch 2598: loss 0.128929\n",
      "batch 2599: loss 0.031105\n",
      "batch 2600: loss 0.037718\n",
      "batch 2601: loss 0.150751\n",
      "batch 2602: loss 0.174817\n",
      "batch 2603: loss 0.031090\n",
      "batch 2604: loss 0.148027\n",
      "batch 2605: loss 0.106811\n",
      "batch 2606: loss 0.121341\n",
      "batch 2607: loss 0.115303\n",
      "batch 2608: loss 0.120049\n",
      "batch 2609: loss 0.124131\n",
      "batch 2610: loss 0.132689\n",
      "batch 2611: loss 0.087431\n",
      "batch 2612: loss 0.139371\n",
      "batch 2613: loss 0.046490\n",
      "batch 2614: loss 0.242480\n",
      "batch 2615: loss 0.118737\n",
      "batch 2616: loss 0.071758\n",
      "batch 2617: loss 0.075142\n",
      "batch 2618: loss 0.065774\n",
      "batch 2619: loss 0.087016\n",
      "batch 2620: loss 0.044958\n",
      "batch 2621: loss 0.035491\n",
      "batch 2622: loss 0.190204\n",
      "batch 2623: loss 0.069237\n",
      "batch 2624: loss 0.087699\n",
      "batch 2625: loss 0.167590\n",
      "batch 2626: loss 0.034460\n",
      "batch 2627: loss 0.113896\n",
      "batch 2628: loss 0.033285\n",
      "batch 2629: loss 0.073431\n",
      "batch 2630: loss 0.017459\n",
      "batch 2631: loss 0.216021\n",
      "batch 2632: loss 0.124740\n",
      "batch 2633: loss 0.029650\n",
      "batch 2634: loss 0.051884\n",
      "batch 2635: loss 0.039380\n",
      "batch 2636: loss 0.034960\n",
      "batch 2637: loss 0.327806\n",
      "batch 2638: loss 0.286686\n",
      "batch 2639: loss 0.111509\n",
      "batch 2640: loss 0.041387\n",
      "batch 2641: loss 0.065320\n",
      "batch 2642: loss 0.084411\n",
      "batch 2643: loss 0.131272\n",
      "batch 2644: loss 0.049423\n",
      "batch 2645: loss 0.052847\n",
      "batch 2646: loss 0.141023\n",
      "batch 2647: loss 0.181411\n",
      "batch 2648: loss 0.120916\n",
      "batch 2649: loss 0.164417\n",
      "batch 2650: loss 0.200790\n",
      "batch 2651: loss 0.086865\n",
      "batch 2652: loss 0.031673\n",
      "batch 2653: loss 0.071373\n",
      "batch 2654: loss 0.082325\n",
      "batch 2655: loss 0.028343\n",
      "batch 2656: loss 0.071021\n",
      "batch 2657: loss 0.052316\n",
      "batch 2658: loss 0.069832\n",
      "batch 2659: loss 0.071860\n",
      "batch 2660: loss 0.028722\n",
      "batch 2661: loss 0.069304\n",
      "batch 2662: loss 0.164148\n",
      "batch 2663: loss 0.185071\n",
      "batch 2664: loss 0.035667\n",
      "batch 2665: loss 0.125991\n",
      "batch 2666: loss 0.035074\n",
      "batch 2667: loss 0.099163\n",
      "batch 2668: loss 0.043940\n",
      "batch 2669: loss 0.033300\n",
      "batch 2670: loss 0.272808\n",
      "batch 2671: loss 0.094884\n",
      "batch 2672: loss 0.242976\n",
      "batch 2673: loss 0.021320\n",
      "batch 2674: loss 0.252483\n",
      "batch 2675: loss 0.032580\n",
      "batch 2676: loss 0.078268\n",
      "batch 2677: loss 0.073444\n",
      "batch 2678: loss 0.071388\n",
      "batch 2679: loss 0.031937\n",
      "batch 2680: loss 0.224641\n",
      "batch 2681: loss 0.126311\n",
      "batch 2682: loss 0.268994\n",
      "batch 2683: loss 0.062854\n",
      "batch 2684: loss 0.051368\n",
      "batch 2685: loss 0.110203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2686: loss 0.193357\n",
      "batch 2687: loss 0.072512\n",
      "batch 2688: loss 0.037051\n",
      "batch 2689: loss 0.230602\n",
      "batch 2690: loss 0.129437\n",
      "batch 2691: loss 0.036353\n",
      "batch 2692: loss 0.056784\n",
      "batch 2693: loss 0.077109\n",
      "batch 2694: loss 0.017400\n",
      "batch 2695: loss 0.037842\n",
      "batch 2696: loss 0.064757\n",
      "batch 2697: loss 0.276987\n",
      "batch 2698: loss 0.028605\n",
      "batch 2699: loss 0.058248\n",
      "batch 2700: loss 0.182365\n",
      "batch 2701: loss 0.045069\n",
      "batch 2702: loss 0.028862\n",
      "batch 2703: loss 0.018976\n",
      "batch 2704: loss 0.059971\n",
      "batch 2705: loss 0.061567\n",
      "batch 2706: loss 0.083826\n",
      "batch 2707: loss 0.078465\n",
      "batch 2708: loss 0.068854\n",
      "batch 2709: loss 0.062367\n",
      "batch 2710: loss 0.116384\n",
      "batch 2711: loss 0.218252\n",
      "batch 2712: loss 0.118222\n",
      "batch 2713: loss 0.027409\n",
      "batch 2714: loss 0.120208\n",
      "batch 2715: loss 0.051266\n",
      "batch 2716: loss 0.071235\n",
      "batch 2717: loss 0.073107\n",
      "batch 2718: loss 0.068530\n",
      "batch 2719: loss 0.033432\n",
      "batch 2720: loss 0.050853\n",
      "batch 2721: loss 0.057781\n",
      "batch 2722: loss 0.057664\n",
      "batch 2723: loss 0.092228\n",
      "batch 2724: loss 0.047123\n",
      "batch 2725: loss 0.052409\n",
      "batch 2726: loss 0.100339\n",
      "batch 2727: loss 0.008121\n",
      "batch 2728: loss 0.034521\n",
      "batch 2729: loss 0.066411\n",
      "batch 2730: loss 0.050430\n",
      "batch 2731: loss 0.067804\n",
      "batch 2732: loss 0.199696\n",
      "batch 2733: loss 0.120922\n",
      "batch 2734: loss 0.079808\n",
      "batch 2735: loss 0.040730\n",
      "batch 2736: loss 0.096613\n",
      "batch 2737: loss 0.080418\n",
      "batch 2738: loss 0.171164\n",
      "batch 2739: loss 0.080103\n",
      "batch 2740: loss 0.028692\n",
      "batch 2741: loss 0.096727\n",
      "batch 2742: loss 0.163152\n",
      "batch 2743: loss 0.146090\n",
      "batch 2744: loss 0.115303\n",
      "batch 2745: loss 0.143745\n",
      "batch 2746: loss 0.155135\n",
      "batch 2747: loss 0.015518\n",
      "batch 2748: loss 0.064671\n",
      "batch 2749: loss 0.247398\n",
      "batch 2750: loss 0.197501\n",
      "batch 2751: loss 0.309524\n",
      "batch 2752: loss 0.110682\n",
      "batch 2753: loss 0.031732\n",
      "batch 2754: loss 0.091308\n",
      "batch 2755: loss 0.029264\n",
      "batch 2756: loss 0.061076\n",
      "batch 2757: loss 0.052157\n",
      "batch 2758: loss 0.033809\n",
      "batch 2759: loss 0.022107\n",
      "batch 2760: loss 0.086464\n",
      "batch 2761: loss 0.240593\n",
      "batch 2762: loss 0.068948\n",
      "batch 2763: loss 0.085689\n",
      "batch 2764: loss 0.058124\n",
      "batch 2765: loss 0.094521\n",
      "batch 2766: loss 0.115387\n",
      "batch 2767: loss 0.030032\n",
      "batch 2768: loss 0.097594\n",
      "batch 2769: loss 0.061464\n",
      "batch 2770: loss 0.071233\n",
      "batch 2771: loss 0.045554\n",
      "batch 2772: loss 0.043251\n",
      "batch 2773: loss 0.100020\n",
      "batch 2774: loss 0.071715\n",
      "batch 2775: loss 0.094115\n",
      "batch 2776: loss 0.319235\n",
      "batch 2777: loss 0.107543\n",
      "batch 2778: loss 0.101520\n",
      "batch 2779: loss 0.113600\n",
      "batch 2780: loss 0.154092\n",
      "batch 2781: loss 0.082916\n",
      "batch 2782: loss 0.069030\n",
      "batch 2783: loss 0.082968\n",
      "batch 2784: loss 0.178968\n",
      "batch 2785: loss 0.108578\n",
      "batch 2786: loss 0.078721\n",
      "batch 2787: loss 0.072660\n",
      "batch 2788: loss 0.421992\n",
      "batch 2789: loss 0.096381\n",
      "batch 2790: loss 0.114374\n",
      "batch 2791: loss 0.029310\n",
      "batch 2792: loss 0.144733\n",
      "batch 2793: loss 0.175314\n",
      "batch 2794: loss 0.061985\n",
      "batch 2795: loss 0.036140\n",
      "batch 2796: loss 0.139173\n",
      "batch 2797: loss 0.089782\n",
      "batch 2798: loss 0.084736\n",
      "batch 2799: loss 0.072126\n",
      "batch 2800: loss 0.025384\n",
      "batch 2801: loss 0.080520\n",
      "batch 2802: loss 0.032776\n",
      "batch 2803: loss 0.114570\n",
      "batch 2804: loss 0.228168\n",
      "batch 2805: loss 0.177132\n",
      "batch 2806: loss 0.154169\n",
      "batch 2807: loss 0.057686\n",
      "batch 2808: loss 0.080050\n",
      "batch 2809: loss 0.055890\n",
      "batch 2810: loss 0.103308\n",
      "batch 2811: loss 0.105755\n",
      "batch 2812: loss 0.038628\n",
      "batch 2813: loss 0.145928\n",
      "batch 2814: loss 0.088249\n",
      "batch 2815: loss 0.051889\n",
      "batch 2816: loss 0.176857\n",
      "batch 2817: loss 0.185703\n",
      "batch 2818: loss 0.073119\n",
      "batch 2819: loss 0.053193\n",
      "batch 2820: loss 0.246753\n",
      "batch 2821: loss 0.167575\n",
      "batch 2822: loss 0.037269\n",
      "batch 2823: loss 0.064531\n",
      "batch 2824: loss 0.135174\n",
      "batch 2825: loss 0.110447\n",
      "batch 2826: loss 0.113172\n",
      "batch 2827: loss 0.069140\n",
      "batch 2828: loss 0.042292\n",
      "batch 2829: loss 0.106768\n",
      "batch 2830: loss 0.093151\n",
      "batch 2831: loss 0.142181\n",
      "batch 2832: loss 0.101267\n",
      "batch 2833: loss 0.139099\n",
      "batch 2834: loss 0.018980\n",
      "batch 2835: loss 0.053691\n",
      "batch 2836: loss 0.142831\n",
      "batch 2837: loss 0.050098\n",
      "batch 2838: loss 0.161372\n",
      "batch 2839: loss 0.090474\n",
      "batch 2840: loss 0.136871\n",
      "batch 2841: loss 0.076080\n",
      "batch 2842: loss 0.123063\n",
      "batch 2843: loss 0.025235\n",
      "batch 2844: loss 0.035925\n",
      "batch 2845: loss 0.094518\n",
      "batch 2846: loss 0.122478\n",
      "batch 2847: loss 0.021611\n",
      "batch 2848: loss 0.124242\n",
      "batch 2849: loss 0.033774\n",
      "batch 2850: loss 0.086520\n",
      "batch 2851: loss 0.026319\n",
      "batch 2852: loss 0.157334\n",
      "batch 2853: loss 0.090136\n",
      "batch 2854: loss 0.033180\n",
      "batch 2855: loss 0.125141\n",
      "batch 2856: loss 0.097076\n",
      "batch 2857: loss 0.031016\n",
      "batch 2858: loss 0.020690\n",
      "batch 2859: loss 0.249964\n",
      "batch 2860: loss 0.082016\n",
      "batch 2861: loss 0.030457\n",
      "batch 2862: loss 0.308287\n",
      "batch 2863: loss 0.137956\n",
      "batch 2864: loss 0.071270\n",
      "batch 2865: loss 0.069791\n",
      "batch 2866: loss 0.072445\n",
      "batch 2867: loss 0.100924\n",
      "batch 2868: loss 0.051116\n",
      "batch 2869: loss 0.027129\n",
      "batch 2870: loss 0.030334\n",
      "batch 2871: loss 0.024825\n",
      "batch 2872: loss 0.036620\n",
      "batch 2873: loss 0.095127\n",
      "batch 2874: loss 0.091825\n",
      "batch 2875: loss 0.082780\n",
      "batch 2876: loss 0.028615\n",
      "batch 2877: loss 0.175573\n",
      "batch 2878: loss 0.080754\n",
      "batch 2879: loss 0.240974\n",
      "batch 2880: loss 0.049587\n",
      "batch 2881: loss 0.037735\n",
      "batch 2882: loss 0.027810\n",
      "batch 2883: loss 0.076281\n",
      "batch 2884: loss 0.051028\n",
      "batch 2885: loss 0.101551\n",
      "batch 2886: loss 0.052868\n",
      "batch 2887: loss 0.194891\n",
      "batch 2888: loss 0.048849\n",
      "batch 2889: loss 0.062254\n",
      "batch 2890: loss 0.090945\n",
      "batch 2891: loss 0.054816\n",
      "batch 2892: loss 0.105269\n",
      "batch 2893: loss 0.149139\n",
      "batch 2894: loss 0.179823\n",
      "batch 2895: loss 0.144133\n",
      "batch 2896: loss 0.135720\n",
      "batch 2897: loss 0.014122\n",
      "batch 2898: loss 0.074813\n",
      "batch 2899: loss 0.036517\n",
      "batch 2900: loss 0.033043\n",
      "batch 2901: loss 0.176876\n",
      "batch 2902: loss 0.097340\n",
      "batch 2903: loss 0.052037\n",
      "batch 2904: loss 0.045602\n",
      "batch 2905: loss 0.017349\n",
      "batch 2906: loss 0.009122\n",
      "batch 2907: loss 0.097966\n",
      "batch 2908: loss 0.089706\n",
      "batch 2909: loss 0.255853\n",
      "batch 2910: loss 0.143415\n",
      "batch 2911: loss 0.043763\n",
      "batch 2912: loss 0.060779\n",
      "batch 2913: loss 0.126971\n",
      "batch 2914: loss 0.099244\n",
      "batch 2915: loss 0.059162\n",
      "batch 2916: loss 0.081774\n",
      "batch 2917: loss 0.163469\n",
      "batch 2918: loss 0.098311\n",
      "batch 2919: loss 0.140194\n",
      "batch 2920: loss 0.025532\n",
      "batch 2921: loss 0.137071\n",
      "batch 2922: loss 0.132378\n",
      "batch 2923: loss 0.150066\n",
      "batch 2924: loss 0.102102\n",
      "batch 2925: loss 0.217892\n",
      "batch 2926: loss 0.074224\n",
      "batch 2927: loss 0.144643\n",
      "batch 2928: loss 0.076415\n",
      "batch 2929: loss 0.114711\n",
      "batch 2930: loss 0.143857\n",
      "batch 2931: loss 0.095588\n",
      "batch 2932: loss 0.028189\n",
      "batch 2933: loss 0.037318\n",
      "batch 2934: loss 0.149980\n",
      "batch 2935: loss 0.129171\n",
      "batch 2936: loss 0.107964\n",
      "batch 2937: loss 0.093534\n",
      "batch 2938: loss 0.052827\n",
      "batch 2939: loss 0.034370\n",
      "batch 2940: loss 0.059890\n",
      "batch 2941: loss 0.043545\n",
      "batch 2942: loss 0.070079\n",
      "batch 2943: loss 0.103021\n",
      "batch 2944: loss 0.136730\n",
      "batch 2945: loss 0.090843\n",
      "batch 2946: loss 0.070247\n",
      "batch 2947: loss 0.104437\n",
      "batch 2948: loss 0.037760\n",
      "batch 2949: loss 0.137752\n",
      "batch 2950: loss 0.054428\n",
      "batch 2951: loss 0.134151\n",
      "batch 2952: loss 0.063138\n",
      "batch 2953: loss 0.084869\n",
      "batch 2954: loss 0.047430\n",
      "batch 2955: loss 0.095339\n",
      "batch 2956: loss 0.132204\n",
      "batch 2957: loss 0.122539\n",
      "batch 2958: loss 0.033606\n",
      "batch 2959: loss 0.035166\n",
      "batch 2960: loss 0.053458\n",
      "batch 2961: loss 0.206820\n",
      "batch 2962: loss 0.069443\n",
      "batch 2963: loss 0.049432\n",
      "batch 2964: loss 0.045185\n",
      "batch 2965: loss 0.182653\n",
      "batch 2966: loss 0.026034\n",
      "batch 2967: loss 0.054842\n",
      "batch 2968: loss 0.073421\n",
      "batch 2969: loss 0.116072\n",
      "batch 2970: loss 0.060198\n",
      "batch 2971: loss 0.078520\n",
      "batch 2972: loss 0.158317\n",
      "batch 2973: loss 0.061186\n",
      "batch 2974: loss 0.033809\n",
      "batch 2975: loss 0.162786\n",
      "batch 2976: loss 0.073790\n",
      "batch 2977: loss 0.065770\n",
      "batch 2978: loss 0.092247\n",
      "batch 2979: loss 0.182157\n",
      "batch 2980: loss 0.019073\n",
      "batch 2981: loss 0.081739\n",
      "batch 2982: loss 0.060719\n",
      "batch 2983: loss 0.148645\n",
      "batch 2984: loss 0.069466\n",
      "batch 2985: loss 0.039856\n",
      "batch 2986: loss 0.119772\n",
      "batch 2987: loss 0.078469\n",
      "batch 2988: loss 0.114481\n",
      "batch 2989: loss 0.107947\n",
      "batch 2990: loss 0.200650\n",
      "batch 2991: loss 0.086562\n",
      "batch 2992: loss 0.182868\n",
      "batch 2993: loss 0.075963\n",
      "batch 2994: loss 0.063748\n",
      "batch 2995: loss 0.046835\n",
      "batch 2996: loss 0.026034\n",
      "batch 2997: loss 0.119179\n",
      "batch 2998: loss 0.030594\n",
      "batch 2999: loss 0.065090\n",
      "batch 3000: loss 0.113575\n",
      "batch 3001: loss 0.187150\n",
      "batch 3002: loss 0.060354\n",
      "batch 3003: loss 0.080389\n",
      "batch 3004: loss 0.039629\n",
      "batch 3005: loss 0.157079\n",
      "batch 3006: loss 0.085741\n",
      "batch 3007: loss 0.288595\n",
      "batch 3008: loss 0.058967\n",
      "batch 3009: loss 0.121459\n",
      "batch 3010: loss 0.064038\n",
      "batch 3011: loss 0.064821\n",
      "batch 3012: loss 0.032923\n",
      "batch 3013: loss 0.080876\n",
      "batch 3014: loss 0.106935\n",
      "batch 3015: loss 0.055712\n",
      "batch 3016: loss 0.052188\n",
      "batch 3017: loss 0.048990\n",
      "batch 3018: loss 0.110672\n",
      "batch 3019: loss 0.055256\n",
      "batch 3020: loss 0.241468\n",
      "batch 3021: loss 0.319664\n",
      "batch 3022: loss 0.154901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3023: loss 0.244572\n",
      "batch 3024: loss 0.167608\n",
      "batch 3025: loss 0.139939\n",
      "batch 3026: loss 0.030579\n",
      "batch 3027: loss 0.020265\n",
      "batch 3028: loss 0.062796\n",
      "batch 3029: loss 0.045857\n",
      "batch 3030: loss 0.034191\n",
      "batch 3031: loss 0.086906\n",
      "batch 3032: loss 0.033416\n",
      "batch 3033: loss 0.206384\n",
      "batch 3034: loss 0.146306\n",
      "batch 3035: loss 0.106029\n",
      "batch 3036: loss 0.039111\n",
      "batch 3037: loss 0.118765\n",
      "batch 3038: loss 0.106163\n",
      "batch 3039: loss 0.014272\n",
      "batch 3040: loss 0.026002\n",
      "batch 3041: loss 0.090110\n",
      "batch 3042: loss 0.028876\n",
      "batch 3043: loss 0.049487\n",
      "batch 3044: loss 0.061588\n",
      "batch 3045: loss 0.026173\n",
      "batch 3046: loss 0.027316\n",
      "batch 3047: loss 0.099431\n",
      "batch 3048: loss 0.270686\n",
      "batch 3049: loss 0.053344\n",
      "batch 3050: loss 0.056869\n",
      "batch 3051: loss 0.152664\n",
      "batch 3052: loss 0.022791\n",
      "batch 3053: loss 0.083684\n",
      "batch 3054: loss 0.143906\n",
      "batch 3055: loss 0.032524\n",
      "batch 3056: loss 0.135060\n",
      "batch 3057: loss 0.074722\n",
      "batch 3058: loss 0.096699\n",
      "batch 3059: loss 0.025599\n",
      "batch 3060: loss 0.048137\n",
      "batch 3061: loss 0.048952\n",
      "batch 3062: loss 0.021809\n",
      "batch 3063: loss 0.031239\n",
      "batch 3064: loss 0.067383\n",
      "batch 3065: loss 0.036142\n",
      "batch 3066: loss 0.081741\n",
      "batch 3067: loss 0.052897\n",
      "batch 3068: loss 0.053326\n",
      "batch 3069: loss 0.053998\n",
      "batch 3070: loss 0.185056\n",
      "batch 3071: loss 0.105075\n",
      "batch 3072: loss 0.146088\n",
      "batch 3073: loss 0.123922\n",
      "batch 3074: loss 0.023154\n",
      "batch 3075: loss 0.117511\n",
      "batch 3076: loss 0.111754\n",
      "batch 3077: loss 0.036961\n",
      "batch 3078: loss 0.057150\n",
      "batch 3079: loss 0.086670\n",
      "batch 3080: loss 0.169015\n",
      "batch 3081: loss 0.056266\n",
      "batch 3082: loss 0.019131\n",
      "batch 3083: loss 0.076810\n",
      "batch 3084: loss 0.048020\n",
      "batch 3085: loss 0.055063\n",
      "batch 3086: loss 0.184533\n",
      "batch 3087: loss 0.030301\n",
      "batch 3088: loss 0.039453\n",
      "batch 3089: loss 0.105255\n",
      "batch 3090: loss 0.163976\n",
      "batch 3091: loss 0.080817\n",
      "batch 3092: loss 0.027847\n",
      "batch 3093: loss 0.038147\n",
      "batch 3094: loss 0.044125\n",
      "batch 3095: loss 0.076905\n",
      "batch 3096: loss 0.076269\n",
      "batch 3097: loss 0.104852\n",
      "batch 3098: loss 0.042242\n",
      "batch 3099: loss 0.215758\n",
      "batch 3100: loss 0.142120\n",
      "batch 3101: loss 0.064359\n",
      "batch 3102: loss 0.097337\n",
      "batch 3103: loss 0.121525\n",
      "batch 3104: loss 0.126798\n",
      "batch 3105: loss 0.028082\n",
      "batch 3106: loss 0.212062\n",
      "batch 3107: loss 0.097071\n",
      "batch 3108: loss 0.209151\n",
      "batch 3109: loss 0.038058\n",
      "batch 3110: loss 0.163515\n",
      "batch 3111: loss 0.099132\n",
      "batch 3112: loss 0.035236\n",
      "batch 3113: loss 0.196706\n",
      "batch 3114: loss 0.016225\n",
      "batch 3115: loss 0.079613\n",
      "batch 3116: loss 0.051572\n",
      "batch 3117: loss 0.060224\n",
      "batch 3118: loss 0.052381\n",
      "batch 3119: loss 0.100895\n",
      "batch 3120: loss 0.097648\n",
      "batch 3121: loss 0.062291\n",
      "batch 3122: loss 0.260342\n",
      "batch 3123: loss 0.145022\n",
      "batch 3124: loss 0.012314\n",
      "batch 3125: loss 0.034923\n",
      "batch 3126: loss 0.124380\n",
      "batch 3127: loss 0.035496\n",
      "batch 3128: loss 0.063429\n",
      "batch 3129: loss 0.162987\n",
      "batch 3130: loss 0.016949\n",
      "batch 3131: loss 0.132801\n",
      "batch 3132: loss 0.250084\n",
      "batch 3133: loss 0.117262\n",
      "batch 3134: loss 0.096094\n",
      "batch 3135: loss 0.072464\n",
      "batch 3136: loss 0.145639\n",
      "batch 3137: loss 0.179008\n",
      "batch 3138: loss 0.036942\n",
      "batch 3139: loss 0.122797\n",
      "batch 3140: loss 0.032893\n",
      "batch 3141: loss 0.145639\n",
      "batch 3142: loss 0.174972\n",
      "batch 3143: loss 0.042429\n",
      "batch 3144: loss 0.101060\n",
      "batch 3145: loss 0.067594\n",
      "batch 3146: loss 0.079920\n",
      "batch 3147: loss 0.066859\n",
      "batch 3148: loss 0.074750\n",
      "batch 3149: loss 0.030787\n",
      "batch 3150: loss 0.105441\n",
      "batch 3151: loss 0.030548\n",
      "batch 3152: loss 0.063193\n",
      "batch 3153: loss 0.105988\n",
      "batch 3154: loss 0.081443\n",
      "batch 3155: loss 0.093403\n",
      "batch 3156: loss 0.062370\n",
      "batch 3157: loss 0.085567\n",
      "batch 3158: loss 0.067418\n",
      "batch 3159: loss 0.022882\n",
      "batch 3160: loss 0.039063\n",
      "batch 3161: loss 0.092395\n",
      "batch 3162: loss 0.139452\n",
      "batch 3163: loss 0.038465\n",
      "batch 3164: loss 0.160010\n",
      "batch 3165: loss 0.057773\n",
      "batch 3166: loss 0.026253\n",
      "batch 3167: loss 0.138875\n",
      "batch 3168: loss 0.250977\n",
      "batch 3169: loss 0.074617\n",
      "batch 3170: loss 0.055999\n",
      "batch 3171: loss 0.052609\n",
      "batch 3172: loss 0.012934\n",
      "batch 3173: loss 0.044599\n",
      "batch 3174: loss 0.164734\n",
      "batch 3175: loss 0.117756\n",
      "batch 3176: loss 0.032886\n",
      "batch 3177: loss 0.197934\n",
      "batch 3178: loss 0.052949\n",
      "batch 3179: loss 0.050305\n",
      "batch 3180: loss 0.343999\n",
      "batch 3181: loss 0.188095\n",
      "batch 3182: loss 0.073166\n",
      "batch 3183: loss 0.102369\n",
      "batch 3184: loss 0.027619\n",
      "batch 3185: loss 0.128297\n",
      "batch 3186: loss 0.078919\n",
      "batch 3187: loss 0.026107\n",
      "batch 3188: loss 0.072442\n",
      "batch 3189: loss 0.065996\n",
      "batch 3190: loss 0.059035\n",
      "batch 3191: loss 0.238473\n",
      "batch 3192: loss 0.044798\n",
      "batch 3193: loss 0.065991\n",
      "batch 3194: loss 0.082765\n",
      "batch 3195: loss 0.056257\n",
      "batch 3196: loss 0.076812\n",
      "batch 3197: loss 0.047550\n",
      "batch 3198: loss 0.215415\n",
      "batch 3199: loss 0.148228\n",
      "batch 3200: loss 0.037014\n",
      "batch 3201: loss 0.018961\n",
      "batch 3202: loss 0.073450\n",
      "batch 3203: loss 0.171454\n",
      "batch 3204: loss 0.231341\n",
      "batch 3205: loss 0.056151\n",
      "batch 3206: loss 0.060416\n",
      "batch 3207: loss 0.028151\n",
      "batch 3208: loss 0.075923\n",
      "batch 3209: loss 0.020477\n",
      "batch 3210: loss 0.038436\n",
      "batch 3211: loss 0.197022\n",
      "batch 3212: loss 0.020615\n",
      "batch 3213: loss 0.214655\n",
      "batch 3214: loss 0.113206\n",
      "batch 3215: loss 0.065893\n",
      "batch 3216: loss 0.044848\n",
      "batch 3217: loss 0.168878\n",
      "batch 3218: loss 0.016010\n",
      "batch 3219: loss 0.089986\n",
      "batch 3220: loss 0.056907\n",
      "batch 3221: loss 0.105874\n",
      "batch 3222: loss 0.082419\n",
      "batch 3223: loss 0.121144\n",
      "batch 3224: loss 0.124352\n",
      "batch 3225: loss 0.282020\n",
      "batch 3226: loss 0.275066\n",
      "batch 3227: loss 0.058196\n",
      "batch 3228: loss 0.145300\n",
      "batch 3229: loss 0.019549\n",
      "batch 3230: loss 0.139953\n",
      "batch 3231: loss 0.028358\n",
      "batch 3232: loss 0.058204\n",
      "batch 3233: loss 0.065907\n",
      "batch 3234: loss 0.068031\n",
      "batch 3235: loss 0.052133\n",
      "batch 3236: loss 0.122088\n",
      "batch 3237: loss 0.124793\n",
      "batch 3238: loss 0.130592\n",
      "batch 3239: loss 0.018615\n",
      "batch 3240: loss 0.061337\n",
      "batch 3241: loss 0.045628\n",
      "batch 3242: loss 0.031967\n",
      "batch 3243: loss 0.058862\n",
      "batch 3244: loss 0.097705\n",
      "batch 3245: loss 0.081679\n",
      "batch 3246: loss 0.061410\n",
      "batch 3247: loss 0.037952\n",
      "batch 3248: loss 0.054155\n",
      "batch 3249: loss 0.075591\n",
      "batch 3250: loss 0.059149\n",
      "batch 3251: loss 0.264305\n",
      "batch 3252: loss 0.044591\n",
      "batch 3253: loss 0.046224\n",
      "batch 3254: loss 0.070675\n",
      "batch 3255: loss 0.042204\n",
      "batch 3256: loss 0.096286\n",
      "batch 3257: loss 0.019630\n",
      "batch 3258: loss 0.030410\n",
      "batch 3259: loss 0.056342\n",
      "batch 3260: loss 0.006132\n",
      "batch 3261: loss 0.069476\n",
      "batch 3262: loss 0.082778\n",
      "batch 3263: loss 0.119827\n",
      "batch 3264: loss 0.037299\n",
      "batch 3265: loss 0.111684\n",
      "batch 3266: loss 0.052264\n",
      "batch 3267: loss 0.053129\n",
      "batch 3268: loss 0.115284\n",
      "batch 3269: loss 0.025437\n",
      "batch 3270: loss 0.053900\n",
      "batch 3271: loss 0.031946\n",
      "batch 3272: loss 0.099110\n",
      "batch 3273: loss 0.015323\n",
      "batch 3274: loss 0.024439\n",
      "batch 3275: loss 0.114181\n",
      "batch 3276: loss 0.028000\n",
      "batch 3277: loss 0.144867\n",
      "batch 3278: loss 0.051901\n",
      "batch 3279: loss 0.033287\n",
      "batch 3280: loss 0.145498\n",
      "batch 3281: loss 0.036724\n",
      "batch 3282: loss 0.081458\n",
      "batch 3283: loss 0.200667\n",
      "batch 3284: loss 0.092395\n",
      "batch 3285: loss 0.032346\n",
      "batch 3286: loss 0.070627\n",
      "batch 3287: loss 0.056110\n",
      "batch 3288: loss 0.207485\n",
      "batch 3289: loss 0.028247\n",
      "batch 3290: loss 0.115836\n",
      "batch 3291: loss 0.137336\n",
      "batch 3292: loss 0.113273\n",
      "batch 3293: loss 0.092637\n",
      "batch 3294: loss 0.011816\n",
      "batch 3295: loss 0.089858\n",
      "batch 3296: loss 0.082008\n",
      "batch 3297: loss 0.026504\n",
      "batch 3298: loss 0.079020\n",
      "batch 3299: loss 0.042251\n",
      "batch 3300: loss 0.029620\n",
      "batch 3301: loss 0.039907\n",
      "batch 3302: loss 0.044403\n",
      "batch 3303: loss 0.038507\n",
      "batch 3304: loss 0.201422\n",
      "batch 3305: loss 0.046430\n",
      "batch 3306: loss 0.016341\n",
      "batch 3307: loss 0.219706\n",
      "batch 3308: loss 0.059521\n",
      "batch 3309: loss 0.024588\n",
      "batch 3310: loss 0.270614\n",
      "batch 3311: loss 0.029189\n",
      "batch 3312: loss 0.226735\n",
      "batch 3313: loss 0.122415\n",
      "batch 3314: loss 0.048637\n",
      "batch 3315: loss 0.123848\n",
      "batch 3316: loss 0.018578\n",
      "batch 3317: loss 0.014254\n",
      "batch 3318: loss 0.114636\n",
      "batch 3319: loss 0.030666\n",
      "batch 3320: loss 0.089317\n",
      "batch 3321: loss 0.065816\n",
      "batch 3322: loss 0.029452\n",
      "batch 3323: loss 0.052052\n",
      "batch 3324: loss 0.086957\n",
      "batch 3325: loss 0.099746\n",
      "batch 3326: loss 0.023286\n",
      "batch 3327: loss 0.104931\n",
      "batch 3328: loss 0.051442\n",
      "batch 3329: loss 0.084992\n",
      "batch 3330: loss 0.044058\n",
      "batch 3331: loss 0.067228\n",
      "batch 3332: loss 0.152576\n",
      "batch 3333: loss 0.059299\n",
      "batch 3334: loss 0.134753\n",
      "batch 3335: loss 0.170569\n",
      "batch 3336: loss 0.058042\n",
      "batch 3337: loss 0.082803\n",
      "batch 3338: loss 0.228893\n",
      "batch 3339: loss 0.100932\n",
      "batch 3340: loss 0.052666\n",
      "batch 3341: loss 0.048532\n",
      "batch 3342: loss 0.058961\n",
      "batch 3343: loss 0.111630\n",
      "batch 3344: loss 0.167446\n",
      "batch 3345: loss 0.046226\n",
      "batch 3346: loss 0.118256\n",
      "batch 3347: loss 0.128702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3348: loss 0.036646\n",
      "batch 3349: loss 0.013443\n",
      "batch 3350: loss 0.160181\n",
      "batch 3351: loss 0.041052\n",
      "batch 3352: loss 0.094571\n",
      "batch 3353: loss 0.018829\n",
      "batch 3354: loss 0.050475\n",
      "batch 3355: loss 0.153668\n",
      "batch 3356: loss 0.043831\n",
      "batch 3357: loss 0.026228\n",
      "batch 3358: loss 0.025341\n",
      "batch 3359: loss 0.022870\n",
      "batch 3360: loss 0.175371\n",
      "batch 3361: loss 0.135083\n",
      "batch 3362: loss 0.090149\n",
      "batch 3363: loss 0.022551\n",
      "batch 3364: loss 0.045861\n",
      "batch 3365: loss 0.014583\n",
      "batch 3366: loss 0.057841\n",
      "batch 3367: loss 0.053545\n",
      "batch 3368: loss 0.041294\n",
      "batch 3369: loss 0.165157\n",
      "batch 3370: loss 0.147391\n",
      "batch 3371: loss 0.061292\n",
      "batch 3372: loss 0.040104\n",
      "batch 3373: loss 0.052816\n",
      "batch 3374: loss 0.052282\n",
      "batch 3375: loss 0.104870\n",
      "batch 3376: loss 0.172330\n",
      "batch 3377: loss 0.011281\n",
      "batch 3378: loss 0.043427\n",
      "batch 3379: loss 0.119777\n",
      "batch 3380: loss 0.010174\n",
      "batch 3381: loss 0.103678\n",
      "batch 3382: loss 0.036479\n",
      "batch 3383: loss 0.098519\n",
      "batch 3384: loss 0.013362\n",
      "batch 3385: loss 0.052988\n",
      "batch 3386: loss 0.044645\n",
      "batch 3387: loss 0.033740\n",
      "batch 3388: loss 0.051055\n",
      "batch 3389: loss 0.067150\n",
      "batch 3390: loss 0.213221\n",
      "batch 3391: loss 0.029391\n",
      "batch 3392: loss 0.077960\n",
      "batch 3393: loss 0.097856\n",
      "batch 3394: loss 0.053558\n",
      "batch 3395: loss 0.015444\n",
      "batch 3396: loss 0.073417\n",
      "batch 3397: loss 0.228355\n",
      "batch 3398: loss 0.165619\n",
      "batch 3399: loss 0.194776\n",
      "batch 3400: loss 0.103245\n",
      "batch 3401: loss 0.084769\n",
      "batch 3402: loss 0.056186\n",
      "batch 3403: loss 0.042928\n",
      "batch 3404: loss 0.038922\n",
      "batch 3405: loss 0.023844\n",
      "batch 3406: loss 0.074861\n",
      "batch 3407: loss 0.057474\n",
      "batch 3408: loss 0.078441\n",
      "batch 3409: loss 0.084855\n",
      "batch 3410: loss 0.012659\n",
      "batch 3411: loss 0.015420\n",
      "batch 3412: loss 0.025346\n",
      "batch 3413: loss 0.047786\n",
      "batch 3414: loss 0.158272\n",
      "batch 3415: loss 0.082179\n",
      "batch 3416: loss 0.116763\n",
      "batch 3417: loss 0.036409\n",
      "batch 3418: loss 0.080560\n",
      "batch 3419: loss 0.101584\n",
      "batch 3420: loss 0.056161\n",
      "batch 3421: loss 0.106432\n",
      "batch 3422: loss 0.052793\n",
      "batch 3423: loss 0.043468\n",
      "batch 3424: loss 0.081789\n",
      "batch 3425: loss 0.022027\n",
      "batch 3426: loss 0.196153\n",
      "batch 3427: loss 0.127953\n",
      "batch 3428: loss 0.140338\n",
      "batch 3429: loss 0.042597\n",
      "batch 3430: loss 0.055806\n",
      "batch 3431: loss 0.110481\n",
      "batch 3432: loss 0.079421\n",
      "batch 3433: loss 0.042990\n",
      "batch 3434: loss 0.103249\n",
      "batch 3435: loss 0.081136\n",
      "batch 3436: loss 0.029968\n",
      "batch 3437: loss 0.031554\n",
      "batch 3438: loss 0.124499\n",
      "batch 3439: loss 0.067645\n",
      "batch 3440: loss 0.065234\n",
      "batch 3441: loss 0.050979\n",
      "batch 3442: loss 0.047790\n",
      "batch 3443: loss 0.168313\n",
      "batch 3444: loss 0.049835\n",
      "batch 3445: loss 0.067615\n",
      "batch 3446: loss 0.073873\n",
      "batch 3447: loss 0.023067\n",
      "batch 3448: loss 0.024845\n",
      "batch 3449: loss 0.018056\n",
      "batch 3450: loss 0.020883\n",
      "batch 3451: loss 0.034883\n",
      "batch 3452: loss 0.108989\n",
      "batch 3453: loss 0.035477\n",
      "batch 3454: loss 0.263720\n",
      "batch 3455: loss 0.056210\n",
      "batch 3456: loss 0.035266\n",
      "batch 3457: loss 0.049554\n",
      "batch 3458: loss 0.254295\n",
      "batch 3459: loss 0.056282\n",
      "batch 3460: loss 0.067334\n",
      "batch 3461: loss 0.125136\n",
      "batch 3462: loss 0.242418\n",
      "batch 3463: loss 0.013230\n",
      "batch 3464: loss 0.063997\n",
      "batch 3465: loss 0.050200\n",
      "batch 3466: loss 0.015525\n",
      "batch 3467: loss 0.021402\n",
      "batch 3468: loss 0.143552\n",
      "batch 3469: loss 0.040112\n",
      "batch 3470: loss 0.124309\n",
      "batch 3471: loss 0.058853\n",
      "batch 3472: loss 0.063870\n",
      "batch 3473: loss 0.039257\n",
      "batch 3474: loss 0.106047\n",
      "batch 3475: loss 0.053026\n",
      "batch 3476: loss 0.040711\n",
      "batch 3477: loss 0.046621\n",
      "batch 3478: loss 0.064601\n",
      "batch 3479: loss 0.036471\n",
      "batch 3480: loss 0.056782\n",
      "batch 3481: loss 0.097246\n",
      "batch 3482: loss 0.066855\n",
      "batch 3483: loss 0.045316\n",
      "batch 3484: loss 0.222466\n",
      "batch 3485: loss 0.120565\n",
      "batch 3486: loss 0.218045\n",
      "batch 3487: loss 0.088796\n",
      "batch 3488: loss 0.074950\n",
      "batch 3489: loss 0.100988\n",
      "batch 3490: loss 0.082862\n",
      "batch 3491: loss 0.043969\n",
      "batch 3492: loss 0.014263\n",
      "batch 3493: loss 0.140809\n",
      "batch 3494: loss 0.124491\n",
      "batch 3495: loss 0.036493\n",
      "batch 3496: loss 0.040170\n",
      "batch 3497: loss 0.200505\n",
      "batch 3498: loss 0.170494\n",
      "batch 3499: loss 0.087383\n",
      "batch 3500: loss 0.137426\n",
      "batch 3501: loss 0.113979\n",
      "batch 3502: loss 0.098048\n",
      "batch 3503: loss 0.029965\n",
      "batch 3504: loss 0.068988\n",
      "batch 3505: loss 0.156703\n",
      "batch 3506: loss 0.108668\n",
      "batch 3507: loss 0.022110\n",
      "batch 3508: loss 0.101254\n",
      "batch 3509: loss 0.099027\n",
      "batch 3510: loss 0.058681\n",
      "batch 3511: loss 0.016270\n",
      "batch 3512: loss 0.033072\n",
      "batch 3513: loss 0.178076\n",
      "batch 3514: loss 0.048799\n",
      "batch 3515: loss 0.091414\n",
      "batch 3516: loss 0.039638\n",
      "batch 3517: loss 0.011993\n",
      "batch 3518: loss 0.049764\n",
      "batch 3519: loss 0.155506\n",
      "batch 3520: loss 0.035761\n",
      "batch 3521: loss 0.034080\n",
      "batch 3522: loss 0.034279\n",
      "batch 3523: loss 0.015137\n",
      "batch 3524: loss 0.014591\n",
      "batch 3525: loss 0.070693\n",
      "batch 3526: loss 0.140596\n",
      "batch 3527: loss 0.130961\n",
      "batch 3528: loss 0.127706\n",
      "batch 3529: loss 0.047527\n",
      "batch 3530: loss 0.083397\n",
      "batch 3531: loss 0.033127\n",
      "batch 3532: loss 0.127121\n",
      "batch 3533: loss 0.247994\n",
      "batch 3534: loss 0.028990\n",
      "batch 3535: loss 0.018308\n",
      "batch 3536: loss 0.042328\n",
      "batch 3537: loss 0.105407\n",
      "batch 3538: loss 0.021782\n",
      "batch 3539: loss 0.095834\n",
      "batch 3540: loss 0.093642\n",
      "batch 3541: loss 0.175259\n",
      "batch 3542: loss 0.067804\n",
      "batch 3543: loss 0.059820\n",
      "batch 3544: loss 0.033559\n",
      "batch 3545: loss 0.045352\n",
      "batch 3546: loss 0.027921\n",
      "batch 3547: loss 0.097448\n",
      "batch 3548: loss 0.068383\n",
      "batch 3549: loss 0.031442\n",
      "batch 3550: loss 0.025056\n",
      "batch 3551: loss 0.019105\n",
      "batch 3552: loss 0.046934\n",
      "batch 3553: loss 0.081794\n",
      "batch 3554: loss 0.032405\n",
      "batch 3555: loss 0.076793\n",
      "batch 3556: loss 0.060685\n",
      "batch 3557: loss 0.055971\n",
      "batch 3558: loss 0.097065\n",
      "batch 3559: loss 0.051002\n",
      "batch 3560: loss 0.190517\n",
      "batch 3561: loss 0.012877\n",
      "batch 3562: loss 0.068315\n",
      "batch 3563: loss 0.108546\n",
      "batch 3564: loss 0.156978\n",
      "batch 3565: loss 0.056925\n",
      "batch 3566: loss 0.046099\n",
      "batch 3567: loss 0.028688\n",
      "batch 3568: loss 0.010269\n",
      "batch 3569: loss 0.039991\n",
      "batch 3570: loss 0.011646\n",
      "batch 3571: loss 0.055543\n",
      "batch 3572: loss 0.124065\n",
      "batch 3573: loss 0.156002\n",
      "batch 3574: loss 0.080968\n",
      "batch 3575: loss 0.249110\n",
      "batch 3576: loss 0.090068\n",
      "batch 3577: loss 0.079695\n",
      "batch 3578: loss 0.064221\n",
      "batch 3579: loss 0.170460\n",
      "batch 3580: loss 0.238439\n",
      "batch 3581: loss 0.372387\n",
      "batch 3582: loss 0.066145\n",
      "batch 3583: loss 0.043700\n",
      "batch 3584: loss 0.024285\n",
      "batch 3585: loss 0.112622\n",
      "batch 3586: loss 0.238282\n",
      "batch 3587: loss 0.034684\n",
      "batch 3588: loss 0.029867\n",
      "batch 3589: loss 0.061058\n",
      "batch 3590: loss 0.048062\n",
      "batch 3591: loss 0.152631\n",
      "batch 3592: loss 0.083124\n",
      "batch 3593: loss 0.149273\n",
      "batch 3594: loss 0.023647\n",
      "batch 3595: loss 0.036110\n",
      "batch 3596: loss 0.060253\n",
      "batch 3597: loss 0.016732\n",
      "batch 3598: loss 0.128346\n",
      "batch 3599: loss 0.112756\n",
      "batch 3600: loss 0.102053\n",
      "batch 3601: loss 0.056848\n",
      "batch 3602: loss 0.107199\n",
      "batch 3603: loss 0.028476\n",
      "batch 3604: loss 0.134524\n",
      "batch 3605: loss 0.059603\n",
      "batch 3606: loss 0.127345\n",
      "batch 3607: loss 0.068421\n",
      "batch 3608: loss 0.053333\n",
      "batch 3609: loss 0.109693\n",
      "batch 3610: loss 0.126453\n",
      "batch 3611: loss 0.028968\n",
      "batch 3612: loss 0.247356\n",
      "batch 3613: loss 0.055046\n",
      "batch 3614: loss 0.058706\n",
      "batch 3615: loss 0.040060\n",
      "batch 3616: loss 0.177527\n",
      "batch 3617: loss 0.167974\n",
      "batch 3618: loss 0.036353\n",
      "batch 3619: loss 0.050770\n",
      "batch 3620: loss 0.026638\n",
      "batch 3621: loss 0.064994\n",
      "batch 3622: loss 0.128540\n",
      "batch 3623: loss 0.116845\n",
      "batch 3624: loss 0.025442\n",
      "batch 3625: loss 0.044279\n",
      "batch 3626: loss 0.036931\n",
      "batch 3627: loss 0.050125\n",
      "batch 3628: loss 0.051855\n",
      "batch 3629: loss 0.021981\n",
      "batch 3630: loss 0.330698\n",
      "batch 3631: loss 0.044955\n",
      "batch 3632: loss 0.054269\n",
      "batch 3633: loss 0.020584\n",
      "batch 3634: loss 0.015660\n",
      "batch 3635: loss 0.162947\n",
      "batch 3636: loss 0.057515\n",
      "batch 3637: loss 0.024148\n",
      "batch 3638: loss 0.068128\n",
      "batch 3639: loss 0.073085\n",
      "batch 3640: loss 0.054910\n",
      "batch 3641: loss 0.100816\n",
      "batch 3642: loss 0.092947\n",
      "batch 3643: loss 0.018653\n",
      "batch 3644: loss 0.223428\n",
      "batch 3645: loss 0.013920\n",
      "batch 3646: loss 0.028370\n",
      "batch 3647: loss 0.189004\n",
      "batch 3648: loss 0.022646\n",
      "batch 3649: loss 0.038129\n",
      "batch 3650: loss 0.074645\n",
      "batch 3651: loss 0.117198\n",
      "batch 3652: loss 0.030413\n",
      "batch 3653: loss 0.017969\n",
      "batch 3654: loss 0.012363\n",
      "batch 3655: loss 0.028464\n",
      "batch 3656: loss 0.080932\n",
      "batch 3657: loss 0.020989\n",
      "batch 3658: loss 0.031427\n",
      "batch 3659: loss 0.027232\n",
      "batch 3660: loss 0.081783\n",
      "batch 3661: loss 0.101700\n",
      "batch 3662: loss 0.082702\n",
      "batch 3663: loss 0.045121\n",
      "batch 3664: loss 0.204946\n",
      "batch 3665: loss 0.083704\n",
      "batch 3666: loss 0.047927\n",
      "batch 3667: loss 0.032887\n",
      "batch 3668: loss 0.147488\n",
      "batch 3669: loss 0.029845\n",
      "batch 3670: loss 0.069574\n",
      "batch 3671: loss 0.075039\n",
      "batch 3672: loss 0.021133\n",
      "batch 3673: loss 0.054125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3674: loss 0.145214\n",
      "batch 3675: loss 0.053647\n",
      "batch 3676: loss 0.045171\n",
      "batch 3677: loss 0.076034\n",
      "batch 3678: loss 0.077588\n",
      "batch 3679: loss 0.081294\n",
      "batch 3680: loss 0.053183\n",
      "batch 3681: loss 0.086149\n",
      "batch 3682: loss 0.111082\n",
      "batch 3683: loss 0.153498\n",
      "batch 3684: loss 0.218930\n",
      "batch 3685: loss 0.132574\n",
      "batch 3686: loss 0.114169\n",
      "batch 3687: loss 0.127728\n",
      "batch 3688: loss 0.018451\n",
      "batch 3689: loss 0.046050\n",
      "batch 3690: loss 0.082012\n",
      "batch 3691: loss 0.042633\n",
      "batch 3692: loss 0.065710\n",
      "batch 3693: loss 0.129600\n",
      "batch 3694: loss 0.090681\n",
      "batch 3695: loss 0.039357\n",
      "batch 3696: loss 0.057671\n",
      "batch 3697: loss 0.021876\n",
      "batch 3698: loss 0.086130\n",
      "batch 3699: loss 0.048604\n",
      "batch 3700: loss 0.021875\n",
      "batch 3701: loss 0.028216\n",
      "batch 3702: loss 0.108014\n",
      "batch 3703: loss 0.035713\n",
      "batch 3704: loss 0.028260\n",
      "batch 3705: loss 0.056517\n",
      "batch 3706: loss 0.045610\n",
      "batch 3707: loss 0.046301\n",
      "batch 3708: loss 0.030747\n",
      "batch 3709: loss 0.063218\n",
      "batch 3710: loss 0.075246\n",
      "batch 3711: loss 0.058846\n",
      "batch 3712: loss 0.030236\n",
      "batch 3713: loss 0.014413\n",
      "batch 3714: loss 0.085482\n",
      "batch 3715: loss 0.108990\n",
      "batch 3716: loss 0.086799\n",
      "batch 3717: loss 0.136095\n",
      "batch 3718: loss 0.018160\n",
      "batch 3719: loss 0.037470\n",
      "batch 3720: loss 0.039197\n",
      "batch 3721: loss 0.028552\n",
      "batch 3722: loss 0.089276\n",
      "batch 3723: loss 0.033072\n",
      "batch 3724: loss 0.107620\n",
      "batch 3725: loss 0.034087\n",
      "batch 3726: loss 0.069460\n",
      "batch 3727: loss 0.130129\n",
      "batch 3728: loss 0.131691\n",
      "batch 3729: loss 0.014281\n",
      "batch 3730: loss 0.037191\n",
      "batch 3731: loss 0.093043\n",
      "batch 3732: loss 0.033445\n",
      "batch 3733: loss 0.204339\n",
      "batch 3734: loss 0.021465\n",
      "batch 3735: loss 0.111153\n",
      "batch 3736: loss 0.041621\n",
      "batch 3737: loss 0.087458\n",
      "batch 3738: loss 0.047892\n",
      "batch 3739: loss 0.085821\n",
      "batch 3740: loss 0.085189\n",
      "batch 3741: loss 0.106232\n",
      "batch 3742: loss 0.014590\n",
      "batch 3743: loss 0.120045\n",
      "batch 3744: loss 0.143314\n",
      "batch 3745: loss 0.089161\n",
      "batch 3746: loss 0.032647\n",
      "batch 3747: loss 0.055509\n",
      "batch 3748: loss 0.172662\n",
      "batch 3749: loss 0.137249\n",
      "batch 3750: loss 0.040657\n",
      "batch 3751: loss 0.099235\n",
      "batch 3752: loss 0.032275\n",
      "batch 3753: loss 0.191151\n",
      "batch 3754: loss 0.098150\n",
      "batch 3755: loss 0.064843\n",
      "batch 3756: loss 0.077224\n",
      "batch 3757: loss 0.016125\n",
      "batch 3758: loss 0.309949\n",
      "batch 3759: loss 0.121043\n",
      "batch 3760: loss 0.068161\n",
      "batch 3761: loss 0.142204\n",
      "batch 3762: loss 0.034378\n",
      "batch 3763: loss 0.051138\n",
      "batch 3764: loss 0.174128\n",
      "batch 3765: loss 0.088363\n",
      "batch 3766: loss 0.064665\n",
      "batch 3767: loss 0.131903\n",
      "batch 3768: loss 0.082007\n",
      "batch 3769: loss 0.036892\n",
      "batch 3770: loss 0.344935\n",
      "batch 3771: loss 0.060926\n",
      "batch 3772: loss 0.080904\n",
      "batch 3773: loss 0.040924\n",
      "batch 3774: loss 0.100327\n",
      "batch 3775: loss 0.040298\n",
      "batch 3776: loss 0.020582\n",
      "batch 3777: loss 0.052058\n",
      "batch 3778: loss 0.048151\n",
      "batch 3779: loss 0.018918\n",
      "batch 3780: loss 0.016210\n",
      "batch 3781: loss 0.100125\n",
      "batch 3782: loss 0.138003\n",
      "batch 3783: loss 0.067275\n",
      "batch 3784: loss 0.088036\n",
      "batch 3785: loss 0.124431\n",
      "batch 3786: loss 0.047741\n",
      "batch 3787: loss 0.086987\n",
      "batch 3788: loss 0.090641\n",
      "batch 3789: loss 0.099783\n",
      "batch 3790: loss 0.185978\n",
      "batch 3791: loss 0.070406\n",
      "batch 3792: loss 0.033343\n",
      "batch 3793: loss 0.031118\n",
      "batch 3794: loss 0.103074\n",
      "batch 3795: loss 0.095581\n",
      "batch 3796: loss 0.129171\n",
      "batch 3797: loss 0.024206\n",
      "batch 3798: loss 0.046140\n",
      "batch 3799: loss 0.049712\n",
      "batch 3800: loss 0.043240\n",
      "batch 3801: loss 0.161095\n",
      "batch 3802: loss 0.022351\n",
      "batch 3803: loss 0.018679\n",
      "batch 3804: loss 0.084344\n",
      "batch 3805: loss 0.066103\n",
      "batch 3806: loss 0.070442\n",
      "batch 3807: loss 0.024661\n",
      "batch 3808: loss 0.184146\n",
      "batch 3809: loss 0.228935\n",
      "batch 3810: loss 0.166944\n",
      "batch 3811: loss 0.091212\n",
      "batch 3812: loss 0.035387\n",
      "batch 3813: loss 0.208889\n",
      "batch 3814: loss 0.223034\n",
      "batch 3815: loss 0.171153\n",
      "batch 3816: loss 0.034317\n",
      "batch 3817: loss 0.170191\n",
      "batch 3818: loss 0.109951\n",
      "batch 3819: loss 0.037440\n",
      "batch 3820: loss 0.209780\n",
      "batch 3821: loss 0.036249\n",
      "batch 3822: loss 0.101362\n",
      "batch 3823: loss 0.064994\n",
      "batch 3824: loss 0.083103\n",
      "batch 3825: loss 0.146862\n",
      "batch 3826: loss 0.029133\n",
      "batch 3827: loss 0.034643\n",
      "batch 3828: loss 0.049590\n",
      "batch 3829: loss 0.068457\n",
      "batch 3830: loss 0.090939\n",
      "batch 3831: loss 0.092158\n",
      "batch 3832: loss 0.107086\n",
      "batch 3833: loss 0.015075\n",
      "batch 3834: loss 0.032228\n",
      "batch 3835: loss 0.031661\n",
      "batch 3836: loss 0.150022\n",
      "batch 3837: loss 0.014042\n",
      "batch 3838: loss 0.172912\n",
      "batch 3839: loss 0.053199\n",
      "batch 3840: loss 0.061046\n",
      "batch 3841: loss 0.084353\n",
      "batch 3842: loss 0.107992\n",
      "batch 3843: loss 0.097293\n",
      "batch 3844: loss 0.100393\n",
      "batch 3845: loss 0.043527\n",
      "batch 3846: loss 0.021193\n",
      "batch 3847: loss 0.174284\n",
      "batch 3848: loss 0.091962\n",
      "batch 3849: loss 0.298490\n",
      "batch 3850: loss 0.021041\n",
      "batch 3851: loss 0.125119\n",
      "batch 3852: loss 0.159516\n",
      "batch 3853: loss 0.091290\n",
      "batch 3854: loss 0.004345\n",
      "batch 3855: loss 0.029618\n",
      "batch 3856: loss 0.074364\n",
      "batch 3857: loss 0.072014\n",
      "batch 3858: loss 0.039921\n",
      "batch 3859: loss 0.039107\n",
      "batch 3860: loss 0.043480\n",
      "batch 3861: loss 0.026043\n",
      "batch 3862: loss 0.027225\n",
      "batch 3863: loss 0.040683\n",
      "batch 3864: loss 0.074994\n",
      "batch 3865: loss 0.060601\n",
      "batch 3866: loss 0.405850\n",
      "batch 3867: loss 0.053918\n",
      "batch 3868: loss 0.028936\n",
      "batch 3869: loss 0.081903\n",
      "batch 3870: loss 0.036492\n",
      "batch 3871: loss 0.027460\n",
      "batch 3872: loss 0.108304\n",
      "batch 3873: loss 0.111289\n",
      "batch 3874: loss 0.038871\n",
      "batch 3875: loss 0.023656\n",
      "batch 3876: loss 0.012706\n",
      "batch 3877: loss 0.013647\n",
      "batch 3878: loss 0.013916\n",
      "batch 3879: loss 0.043828\n",
      "batch 3880: loss 0.077937\n",
      "batch 3881: loss 0.129617\n",
      "batch 3882: loss 0.155804\n",
      "batch 3883: loss 0.029649\n",
      "batch 3884: loss 0.035226\n",
      "batch 3885: loss 0.114628\n",
      "batch 3886: loss 0.077593\n",
      "batch 3887: loss 0.095516\n",
      "batch 3888: loss 0.044517\n",
      "batch 3889: loss 0.073465\n",
      "batch 3890: loss 0.147576\n",
      "batch 3891: loss 0.117100\n",
      "batch 3892: loss 0.119555\n",
      "batch 3893: loss 0.050321\n",
      "batch 3894: loss 0.014971\n",
      "batch 3895: loss 0.059906\n",
      "batch 3896: loss 0.038950\n",
      "batch 3897: loss 0.093162\n",
      "batch 3898: loss 0.253901\n",
      "batch 3899: loss 0.050727\n",
      "batch 3900: loss 0.014175\n",
      "batch 3901: loss 0.197010\n",
      "batch 3902: loss 0.026961\n",
      "batch 3903: loss 0.203805\n",
      "batch 3904: loss 0.023541\n",
      "batch 3905: loss 0.198457\n",
      "batch 3906: loss 0.098224\n",
      "batch 3907: loss 0.210338\n",
      "batch 3908: loss 0.074359\n",
      "batch 3909: loss 0.051864\n",
      "batch 3910: loss 0.085525\n",
      "batch 3911: loss 0.058259\n",
      "batch 3912: loss 0.062730\n",
      "batch 3913: loss 0.045526\n",
      "batch 3914: loss 0.050821\n",
      "batch 3915: loss 0.068571\n",
      "batch 3916: loss 0.077620\n",
      "batch 3917: loss 0.033591\n",
      "batch 3918: loss 0.116433\n",
      "batch 3919: loss 0.060661\n",
      "batch 3920: loss 0.046237\n",
      "batch 3921: loss 0.029359\n",
      "batch 3922: loss 0.014705\n",
      "batch 3923: loss 0.045188\n",
      "batch 3924: loss 0.083237\n",
      "batch 3925: loss 0.071279\n",
      "batch 3926: loss 0.061546\n",
      "batch 3927: loss 0.025255\n",
      "batch 3928: loss 0.078067\n",
      "batch 3929: loss 0.124981\n",
      "batch 3930: loss 0.069480\n",
      "batch 3931: loss 0.014364\n",
      "batch 3932: loss 0.088188\n",
      "batch 3933: loss 0.123551\n",
      "batch 3934: loss 0.026780\n",
      "batch 3935: loss 0.121271\n",
      "batch 3936: loss 0.118160\n",
      "batch 3937: loss 0.094716\n",
      "batch 3938: loss 0.089328\n",
      "batch 3939: loss 0.027918\n",
      "batch 3940: loss 0.066872\n",
      "batch 3941: loss 0.133210\n",
      "batch 3942: loss 0.027303\n",
      "batch 3943: loss 0.051947\n",
      "batch 3944: loss 0.079090\n",
      "batch 3945: loss 0.055245\n",
      "batch 3946: loss 0.081761\n",
      "batch 3947: loss 0.049837\n",
      "batch 3948: loss 0.040723\n",
      "batch 3949: loss 0.036458\n",
      "batch 3950: loss 0.097952\n",
      "batch 3951: loss 0.079185\n",
      "batch 3952: loss 0.066803\n",
      "batch 3953: loss 0.050781\n",
      "batch 3954: loss 0.145182\n",
      "batch 3955: loss 0.191422\n",
      "batch 3956: loss 0.076381\n",
      "batch 3957: loss 0.137168\n",
      "batch 3958: loss 0.113678\n",
      "batch 3959: loss 0.123315\n",
      "batch 3960: loss 0.014525\n",
      "batch 3961: loss 0.031055\n",
      "batch 3962: loss 0.037264\n",
      "batch 3963: loss 0.329082\n",
      "batch 3964: loss 0.043687\n",
      "batch 3965: loss 0.200110\n",
      "batch 3966: loss 0.031159\n",
      "batch 3967: loss 0.106331\n",
      "batch 3968: loss 0.074196\n",
      "batch 3969: loss 0.045657\n",
      "batch 3970: loss 0.019211\n",
      "batch 3971: loss 0.015887\n",
      "batch 3972: loss 0.115051\n",
      "batch 3973: loss 0.045811\n",
      "batch 3974: loss 0.014822\n",
      "batch 3975: loss 0.027951\n",
      "batch 3976: loss 0.149006\n",
      "batch 3977: loss 0.278631\n",
      "batch 3978: loss 0.031447\n",
      "batch 3979: loss 0.259504\n",
      "batch 3980: loss 0.028092\n",
      "batch 3981: loss 0.032864\n",
      "batch 3982: loss 0.231110\n",
      "batch 3983: loss 0.035394\n",
      "batch 3984: loss 0.039570\n",
      "batch 3985: loss 0.034714\n",
      "batch 3986: loss 0.022929\n",
      "batch 3987: loss 0.054945\n",
      "batch 3988: loss 0.200131\n",
      "batch 3989: loss 0.035798\n",
      "batch 3990: loss 0.063679\n",
      "batch 3991: loss 0.034399\n",
      "batch 3992: loss 0.032983\n",
      "batch 3993: loss 0.010912\n",
      "batch 3994: loss 0.023639\n",
      "batch 3995: loss 0.042135\n",
      "batch 3996: loss 0.025540\n",
      "batch 3997: loss 0.068959\n",
      "batch 3998: loss 0.107131\n",
      "batch 3999: loss 0.050704\n",
      "batch 4000: loss 0.037277\n",
      "batch 4001: loss 0.115845\n",
      "batch 4002: loss 0.018654\n",
      "batch 4003: loss 0.018652\n",
      "batch 4004: loss 0.072176\n",
      "batch 4005: loss 0.086471\n",
      "batch 4006: loss 0.100376\n",
      "batch 4007: loss 0.179784\n",
      "batch 4008: loss 0.168966\n",
      "batch 4009: loss 0.191185\n",
      "batch 4010: loss 0.030769\n",
      "batch 4011: loss 0.191397\n",
      "batch 4012: loss 0.020039\n",
      "batch 4013: loss 0.078739\n",
      "batch 4014: loss 0.034366\n",
      "batch 4015: loss 0.127509\n",
      "batch 4016: loss 0.041688\n",
      "batch 4017: loss 0.111610\n",
      "batch 4018: loss 0.084495\n",
      "batch 4019: loss 0.052149\n",
      "batch 4020: loss 0.026392\n",
      "batch 4021: loss 0.085953\n",
      "batch 4022: loss 0.019529\n",
      "batch 4023: loss 0.146718\n",
      "batch 4024: loss 0.123890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4025: loss 0.032211\n",
      "batch 4026: loss 0.034158\n",
      "batch 4027: loss 0.135351\n",
      "batch 4028: loss 0.025127\n",
      "batch 4029: loss 0.102032\n",
      "batch 4030: loss 0.008494\n",
      "batch 4031: loss 0.074862\n",
      "batch 4032: loss 0.069110\n",
      "batch 4033: loss 0.020872\n",
      "batch 4034: loss 0.016223\n",
      "batch 4035: loss 0.185478\n",
      "batch 4036: loss 0.085164\n",
      "batch 4037: loss 0.042510\n",
      "batch 4038: loss 0.029293\n",
      "batch 4039: loss 0.093906\n",
      "batch 4040: loss 0.056563\n",
      "batch 4041: loss 0.116627\n",
      "batch 4042: loss 0.072881\n",
      "batch 4043: loss 0.113114\n",
      "batch 4044: loss 0.025265\n",
      "batch 4045: loss 0.037751\n",
      "batch 4046: loss 0.043395\n",
      "batch 4047: loss 0.010222\n",
      "batch 4048: loss 0.052541\n",
      "batch 4049: loss 0.069543\n",
      "batch 4050: loss 0.116998\n",
      "batch 4051: loss 0.073339\n",
      "batch 4052: loss 0.030252\n",
      "batch 4053: loss 0.027141\n",
      "batch 4054: loss 0.043177\n",
      "batch 4055: loss 0.008752\n",
      "batch 4056: loss 0.073324\n",
      "batch 4057: loss 0.041622\n",
      "batch 4058: loss 0.071492\n",
      "batch 4059: loss 0.027536\n",
      "batch 4060: loss 0.019765\n",
      "batch 4061: loss 0.063625\n",
      "batch 4062: loss 0.084696\n",
      "batch 4063: loss 0.095443\n",
      "batch 4064: loss 0.054385\n",
      "batch 4065: loss 0.057993\n",
      "batch 4066: loss 0.081516\n",
      "batch 4067: loss 0.067355\n",
      "batch 4068: loss 0.038445\n",
      "batch 4069: loss 0.028063\n",
      "batch 4070: loss 0.014644\n",
      "batch 4071: loss 0.064496\n",
      "batch 4072: loss 0.019976\n",
      "batch 4073: loss 0.010685\n",
      "batch 4074: loss 0.013996\n",
      "batch 4075: loss 0.039012\n",
      "batch 4076: loss 0.009535\n",
      "batch 4077: loss 0.038228\n",
      "batch 4078: loss 0.021176\n",
      "batch 4079: loss 0.085920\n",
      "batch 4080: loss 0.018297\n",
      "batch 4081: loss 0.058212\n",
      "batch 4082: loss 0.070975\n",
      "batch 4083: loss 0.044045\n",
      "batch 4084: loss 0.013714\n",
      "batch 4085: loss 0.022260\n",
      "batch 4086: loss 0.062419\n",
      "batch 4087: loss 0.026648\n",
      "batch 4088: loss 0.021965\n",
      "batch 4089: loss 0.037672\n",
      "batch 4090: loss 0.043416\n",
      "batch 4091: loss 0.027710\n",
      "batch 4092: loss 0.024663\n",
      "batch 4093: loss 0.029513\n",
      "batch 4094: loss 0.146093\n",
      "batch 4095: loss 0.043716\n",
      "batch 4096: loss 0.033546\n",
      "batch 4097: loss 0.012919\n",
      "batch 4098: loss 0.050917\n",
      "batch 4099: loss 0.147600\n",
      "batch 4100: loss 0.081602\n",
      "batch 4101: loss 0.109311\n",
      "batch 4102: loss 0.072858\n",
      "batch 4103: loss 0.080146\n",
      "batch 4104: loss 0.047649\n",
      "batch 4105: loss 0.290101\n",
      "batch 4106: loss 0.067265\n",
      "batch 4107: loss 0.029815\n",
      "batch 4108: loss 0.042904\n",
      "batch 4109: loss 0.019554\n",
      "batch 4110: loss 0.041813\n",
      "batch 4111: loss 0.208188\n",
      "batch 4112: loss 0.142723\n",
      "batch 4113: loss 0.061626\n",
      "batch 4114: loss 0.034920\n",
      "batch 4115: loss 0.020650\n",
      "batch 4116: loss 0.022389\n",
      "batch 4117: loss 0.058992\n",
      "batch 4118: loss 0.121106\n",
      "batch 4119: loss 0.198215\n",
      "batch 4120: loss 0.070762\n",
      "batch 4121: loss 0.024285\n",
      "batch 4122: loss 0.060685\n",
      "batch 4123: loss 0.034390\n",
      "batch 4124: loss 0.017623\n",
      "batch 4125: loss 0.077993\n",
      "batch 4126: loss 0.044161\n",
      "batch 4127: loss 0.112049\n",
      "batch 4128: loss 0.017763\n",
      "batch 4129: loss 0.057105\n",
      "batch 4130: loss 0.050773\n",
      "batch 4131: loss 0.086251\n",
      "batch 4132: loss 0.076846\n",
      "batch 4133: loss 0.035173\n",
      "batch 4134: loss 0.140717\n",
      "batch 4135: loss 0.043507\n",
      "batch 4136: loss 0.093890\n",
      "batch 4137: loss 0.108475\n",
      "batch 4138: loss 0.084338\n",
      "batch 4139: loss 0.018164\n",
      "batch 4140: loss 0.116268\n",
      "batch 4141: loss 0.070302\n",
      "batch 4142: loss 0.195099\n",
      "batch 4143: loss 0.105518\n",
      "batch 4144: loss 0.050636\n",
      "batch 4145: loss 0.140263\n",
      "batch 4146: loss 0.032986\n",
      "batch 4147: loss 0.076371\n",
      "batch 4148: loss 0.024604\n",
      "batch 4149: loss 0.028925\n",
      "batch 4150: loss 0.035255\n",
      "batch 4151: loss 0.114237\n",
      "batch 4152: loss 0.216360\n",
      "batch 4153: loss 0.039427\n",
      "batch 4154: loss 0.066064\n",
      "batch 4155: loss 0.049136\n",
      "batch 4156: loss 0.052700\n",
      "batch 4157: loss 0.055912\n",
      "batch 4158: loss 0.060013\n",
      "batch 4159: loss 0.071875\n",
      "batch 4160: loss 0.018835\n",
      "batch 4161: loss 0.070413\n",
      "batch 4162: loss 0.020508\n",
      "batch 4163: loss 0.141531\n",
      "batch 4164: loss 0.110021\n",
      "batch 4165: loss 0.012504\n",
      "batch 4166: loss 0.060186\n",
      "batch 4167: loss 0.092337\n",
      "batch 4168: loss 0.040420\n",
      "batch 4169: loss 0.077074\n",
      "batch 4170: loss 0.105065\n",
      "batch 4171: loss 0.238157\n",
      "batch 4172: loss 0.026537\n",
      "batch 4173: loss 0.097503\n",
      "batch 4174: loss 0.099009\n",
      "batch 4175: loss 0.023173\n",
      "batch 4176: loss 0.044048\n",
      "batch 4177: loss 0.140049\n",
      "batch 4178: loss 0.076484\n",
      "batch 4179: loss 0.040013\n",
      "batch 4180: loss 0.062228\n",
      "batch 4181: loss 0.026868\n",
      "batch 4182: loss 0.079995\n",
      "batch 4183: loss 0.033387\n",
      "batch 4184: loss 0.096289\n",
      "batch 4185: loss 0.087982\n",
      "batch 4186: loss 0.032167\n",
      "batch 4187: loss 0.015752\n",
      "batch 4188: loss 0.007756\n",
      "batch 4189: loss 0.124299\n",
      "batch 4190: loss 0.088874\n",
      "batch 4191: loss 0.124034\n",
      "batch 4192: loss 0.150109\n",
      "batch 4193: loss 0.021072\n",
      "batch 4194: loss 0.085682\n",
      "batch 4195: loss 0.047017\n",
      "batch 4196: loss 0.020259\n",
      "batch 4197: loss 0.046900\n",
      "batch 4198: loss 0.013289\n",
      "batch 4199: loss 0.319718\n",
      "batch 4200: loss 0.024791\n",
      "batch 4201: loss 0.112242\n",
      "batch 4202: loss 0.116096\n",
      "batch 4203: loss 0.058732\n",
      "batch 4204: loss 0.071548\n",
      "batch 4205: loss 0.140158\n",
      "batch 4206: loss 0.025063\n",
      "batch 4207: loss 0.112142\n",
      "batch 4208: loss 0.031553\n",
      "batch 4209: loss 0.105220\n",
      "batch 4210: loss 0.083142\n",
      "batch 4211: loss 0.017614\n",
      "batch 4212: loss 0.015736\n",
      "batch 4213: loss 0.011754\n",
      "batch 4214: loss 0.103189\n",
      "batch 4215: loss 0.032059\n",
      "batch 4216: loss 0.048004\n",
      "batch 4217: loss 0.027249\n",
      "batch 4218: loss 0.029474\n",
      "batch 4219: loss 0.045816\n",
      "batch 4220: loss 0.045603\n",
      "batch 4221: loss 0.180012\n",
      "batch 4222: loss 0.221696\n",
      "batch 4223: loss 0.081105\n",
      "batch 4224: loss 0.122062\n",
      "batch 4225: loss 0.041163\n",
      "batch 4226: loss 0.094652\n",
      "batch 4227: loss 0.102916\n",
      "batch 4228: loss 0.099033\n",
      "batch 4229: loss 0.021646\n",
      "batch 4230: loss 0.019096\n",
      "batch 4231: loss 0.057531\n",
      "batch 4232: loss 0.030176\n",
      "batch 4233: loss 0.123602\n",
      "batch 4234: loss 0.049521\n",
      "batch 4235: loss 0.059828\n",
      "batch 4236: loss 0.048221\n",
      "batch 4237: loss 0.034722\n",
      "batch 4238: loss 0.084324\n",
      "batch 4239: loss 0.140562\n",
      "batch 4240: loss 0.222602\n",
      "batch 4241: loss 0.096114\n",
      "batch 4242: loss 0.030926\n",
      "batch 4243: loss 0.024813\n",
      "batch 4244: loss 0.064902\n",
      "batch 4245: loss 0.041354\n",
      "batch 4246: loss 0.038682\n",
      "batch 4247: loss 0.033503\n",
      "batch 4248: loss 0.043714\n",
      "batch 4249: loss 0.061528\n",
      "batch 4250: loss 0.099694\n",
      "batch 4251: loss 0.119988\n",
      "batch 4252: loss 0.119594\n",
      "batch 4253: loss 0.018634\n",
      "batch 4254: loss 0.162906\n",
      "batch 4255: loss 0.024625\n",
      "batch 4256: loss 0.064039\n",
      "batch 4257: loss 0.023639\n",
      "batch 4258: loss 0.046829\n",
      "batch 4259: loss 0.092259\n",
      "batch 4260: loss 0.043966\n",
      "batch 4261: loss 0.039585\n",
      "batch 4262: loss 0.084121\n",
      "batch 4263: loss 0.097898\n",
      "batch 4264: loss 0.029535\n",
      "batch 4265: loss 0.022639\n",
      "batch 4266: loss 0.031816\n",
      "batch 4267: loss 0.013914\n",
      "batch 4268: loss 0.075331\n",
      "batch 4269: loss 0.163505\n",
      "batch 4270: loss 0.147003\n",
      "batch 4271: loss 0.175129\n",
      "batch 4272: loss 0.013466\n",
      "batch 4273: loss 0.068968\n",
      "batch 4274: loss 0.031694\n",
      "batch 4275: loss 0.047238\n",
      "batch 4276: loss 0.016564\n",
      "batch 4277: loss 0.230691\n",
      "batch 4278: loss 0.081371\n",
      "batch 4279: loss 0.068727\n",
      "batch 4280: loss 0.108128\n",
      "batch 4281: loss 0.098730\n",
      "batch 4282: loss 0.160588\n",
      "batch 4283: loss 0.026773\n",
      "batch 4284: loss 0.053041\n",
      "batch 4285: loss 0.019431\n",
      "batch 4286: loss 0.075504\n",
      "batch 4287: loss 0.024840\n",
      "batch 4288: loss 0.017815\n",
      "batch 4289: loss 0.077861\n",
      "batch 4290: loss 0.021881\n",
      "batch 4291: loss 0.018878\n",
      "batch 4292: loss 0.055438\n",
      "batch 4293: loss 0.042351\n",
      "batch 4294: loss 0.086573\n",
      "batch 4295: loss 0.062620\n",
      "batch 4296: loss 0.098483\n",
      "batch 4297: loss 0.077029\n",
      "batch 4298: loss 0.022577\n",
      "batch 4299: loss 0.087339\n",
      "batch 4300: loss 0.026703\n",
      "batch 4301: loss 0.058801\n",
      "batch 4302: loss 0.026932\n",
      "batch 4303: loss 0.268963\n",
      "batch 4304: loss 0.005707\n",
      "batch 4305: loss 0.101443\n",
      "batch 4306: loss 0.034028\n",
      "batch 4307: loss 0.012172\n",
      "batch 4308: loss 0.023811\n",
      "batch 4309: loss 0.016623\n",
      "batch 4310: loss 0.015699\n",
      "batch 4311: loss 0.015864\n",
      "batch 4312: loss 0.034859\n",
      "batch 4313: loss 0.062159\n",
      "batch 4314: loss 0.140280\n",
      "batch 4315: loss 0.021844\n",
      "batch 4316: loss 0.255351\n",
      "batch 4317: loss 0.061165\n",
      "batch 4318: loss 0.066969\n",
      "batch 4319: loss 0.204293\n",
      "batch 4320: loss 0.032123\n",
      "batch 4321: loss 0.034736\n",
      "batch 4322: loss 0.045196\n",
      "batch 4323: loss 0.109544\n",
      "batch 4324: loss 0.023861\n",
      "batch 4325: loss 0.067619\n",
      "batch 4326: loss 0.048809\n",
      "batch 4327: loss 0.062100\n",
      "batch 4328: loss 0.071017\n",
      "batch 4329: loss 0.019786\n",
      "batch 4330: loss 0.168719\n",
      "batch 4331: loss 0.089024\n",
      "batch 4332: loss 0.085555\n",
      "batch 4333: loss 0.051265\n",
      "batch 4334: loss 0.016500\n",
      "batch 4335: loss 0.223266\n",
      "batch 4336: loss 0.152076\n",
      "batch 4337: loss 0.138304\n",
      "batch 4338: loss 0.033620\n",
      "batch 4339: loss 0.025617\n",
      "batch 4340: loss 0.138070\n",
      "batch 4341: loss 0.081372\n",
      "batch 4342: loss 0.108558\n",
      "batch 4343: loss 0.102396\n",
      "batch 4344: loss 0.142741\n",
      "batch 4345: loss 0.140473\n",
      "batch 4346: loss 0.037249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4347: loss 0.029416\n",
      "batch 4348: loss 0.057020\n",
      "batch 4349: loss 0.115522\n",
      "batch 4350: loss 0.022944\n",
      "batch 4351: loss 0.019583\n",
      "batch 4352: loss 0.112000\n",
      "batch 4353: loss 0.039768\n",
      "batch 4354: loss 0.088347\n",
      "batch 4355: loss 0.078006\n",
      "batch 4356: loss 0.051527\n",
      "batch 4357: loss 0.031524\n",
      "batch 4358: loss 0.077274\n",
      "batch 4359: loss 0.052951\n",
      "batch 4360: loss 0.087340\n",
      "batch 4361: loss 0.140438\n",
      "batch 4362: loss 0.076472\n",
      "batch 4363: loss 0.025485\n",
      "batch 4364: loss 0.176236\n",
      "batch 4365: loss 0.020203\n",
      "batch 4366: loss 0.080142\n",
      "batch 4367: loss 0.096157\n",
      "batch 4368: loss 0.037261\n",
      "batch 4369: loss 0.093870\n",
      "batch 4370: loss 0.064888\n",
      "batch 4371: loss 0.120943\n",
      "batch 4372: loss 0.018174\n",
      "batch 4373: loss 0.019742\n",
      "batch 4374: loss 0.071638\n",
      "batch 4375: loss 0.012353\n",
      "batch 4376: loss 0.018842\n",
      "batch 4377: loss 0.182665\n",
      "batch 4378: loss 0.016771\n",
      "batch 4379: loss 0.094651\n",
      "batch 4380: loss 0.034545\n",
      "batch 4381: loss 0.044476\n",
      "batch 4382: loss 0.132737\n",
      "batch 4383: loss 0.032872\n",
      "batch 4384: loss 0.088424\n",
      "batch 4385: loss 0.143276\n",
      "batch 4386: loss 0.054310\n",
      "batch 4387: loss 0.126089\n",
      "batch 4388: loss 0.034061\n",
      "batch 4389: loss 0.070813\n",
      "batch 4390: loss 0.030632\n",
      "batch 4391: loss 0.014287\n",
      "batch 4392: loss 0.080184\n",
      "batch 4393: loss 0.106318\n",
      "batch 4394: loss 0.095038\n",
      "batch 4395: loss 0.079749\n",
      "batch 4396: loss 0.054925\n",
      "batch 4397: loss 0.046495\n",
      "batch 4398: loss 0.073901\n",
      "batch 4399: loss 0.026773\n",
      "batch 4400: loss 0.046388\n",
      "batch 4401: loss 0.063003\n",
      "batch 4402: loss 0.056548\n",
      "batch 4403: loss 0.029205\n",
      "batch 4404: loss 0.069517\n",
      "batch 4405: loss 0.059711\n",
      "batch 4406: loss 0.047333\n",
      "batch 4407: loss 0.173306\n",
      "batch 4408: loss 0.063494\n",
      "batch 4409: loss 0.021583\n",
      "batch 4410: loss 0.120512\n",
      "batch 4411: loss 0.091141\n",
      "batch 4412: loss 0.080970\n",
      "batch 4413: loss 0.049452\n",
      "batch 4414: loss 0.103497\n",
      "batch 4415: loss 0.064772\n",
      "batch 4416: loss 0.075183\n",
      "batch 4417: loss 0.090337\n",
      "batch 4418: loss 0.016462\n",
      "batch 4419: loss 0.052444\n",
      "batch 4420: loss 0.123028\n",
      "batch 4421: loss 0.050296\n",
      "batch 4422: loss 0.056365\n",
      "batch 4423: loss 0.008982\n",
      "batch 4424: loss 0.014964\n",
      "batch 4425: loss 0.085202\n",
      "batch 4426: loss 0.052666\n",
      "batch 4427: loss 0.122508\n",
      "batch 4428: loss 0.053566\n",
      "batch 4429: loss 0.022114\n",
      "batch 4430: loss 0.013858\n",
      "batch 4431: loss 0.093446\n",
      "batch 4432: loss 0.063359\n",
      "batch 4433: loss 0.046547\n",
      "batch 4434: loss 0.126419\n",
      "batch 4435: loss 0.198559\n",
      "batch 4436: loss 0.066313\n",
      "batch 4437: loss 0.068512\n",
      "batch 4438: loss 0.137870\n",
      "batch 4439: loss 0.089111\n",
      "batch 4440: loss 0.101751\n",
      "batch 4441: loss 0.141424\n",
      "batch 4442: loss 0.101614\n",
      "batch 4443: loss 0.039450\n",
      "batch 4444: loss 0.020488\n",
      "batch 4445: loss 0.011716\n",
      "batch 4446: loss 0.026766\n",
      "batch 4447: loss 0.085241\n",
      "batch 4448: loss 0.017161\n",
      "batch 4449: loss 0.059174\n",
      "batch 4450: loss 0.082489\n",
      "batch 4451: loss 0.091464\n",
      "batch 4452: loss 0.053841\n",
      "batch 4453: loss 0.039946\n",
      "batch 4454: loss 0.123182\n",
      "batch 4455: loss 0.027794\n",
      "batch 4456: loss 0.103908\n",
      "batch 4457: loss 0.060100\n",
      "batch 4458: loss 0.206455\n",
      "batch 4459: loss 0.096665\n",
      "batch 4460: loss 0.162920\n",
      "batch 4461: loss 0.027199\n",
      "batch 4462: loss 0.008823\n",
      "batch 4463: loss 0.051712\n",
      "batch 4464: loss 0.043195\n",
      "batch 4465: loss 0.023443\n",
      "batch 4466: loss 0.046667\n",
      "batch 4467: loss 0.028902\n",
      "batch 4468: loss 0.089628\n",
      "batch 4469: loss 0.032563\n",
      "batch 4470: loss 0.072920\n",
      "batch 4471: loss 0.248715\n",
      "batch 4472: loss 0.017065\n",
      "batch 4473: loss 0.045720\n",
      "batch 4474: loss 0.019970\n",
      "batch 4475: loss 0.091129\n",
      "batch 4476: loss 0.059110\n",
      "batch 4477: loss 0.051164\n",
      "batch 4478: loss 0.032045\n",
      "batch 4479: loss 0.105231\n",
      "batch 4480: loss 0.107865\n",
      "batch 4481: loss 0.018090\n",
      "batch 4482: loss 0.072766\n",
      "batch 4483: loss 0.052624\n",
      "batch 4484: loss 0.062605\n",
      "batch 4485: loss 0.029365\n",
      "batch 4486: loss 0.095377\n",
      "batch 4487: loss 0.181127\n",
      "batch 4488: loss 0.061208\n",
      "batch 4489: loss 0.015696\n",
      "batch 4490: loss 0.061664\n",
      "batch 4491: loss 0.161658\n",
      "batch 4492: loss 0.021483\n",
      "batch 4493: loss 0.039089\n",
      "batch 4494: loss 0.070880\n",
      "batch 4495: loss 0.015644\n",
      "batch 4496: loss 0.053063\n",
      "batch 4497: loss 0.218734\n",
      "batch 4498: loss 0.052818\n",
      "batch 4499: loss 0.019615\n",
      "batch 4500: loss 0.027141\n",
      "batch 4501: loss 0.021676\n",
      "batch 4502: loss 0.035843\n",
      "batch 4503: loss 0.023909\n",
      "batch 4504: loss 0.018175\n",
      "batch 4505: loss 0.045271\n",
      "batch 4506: loss 0.004113\n",
      "batch 4507: loss 0.058156\n",
      "batch 4508: loss 0.025015\n",
      "batch 4509: loss 0.026017\n",
      "batch 4510: loss 0.010423\n",
      "batch 4511: loss 0.029135\n",
      "batch 4512: loss 0.130241\n",
      "batch 4513: loss 0.068456\n",
      "batch 4514: loss 0.033190\n",
      "batch 4515: loss 0.035624\n",
      "batch 4516: loss 0.015755\n",
      "batch 4517: loss 0.076089\n",
      "batch 4518: loss 0.059335\n",
      "batch 4519: loss 0.039864\n",
      "batch 4520: loss 0.042176\n",
      "batch 4521: loss 0.114990\n",
      "batch 4522: loss 0.042809\n",
      "batch 4523: loss 0.088658\n",
      "batch 4524: loss 0.019924\n",
      "batch 4525: loss 0.110990\n",
      "batch 4526: loss 0.062919\n",
      "batch 4527: loss 0.015229\n",
      "batch 4528: loss 0.040554\n",
      "batch 4529: loss 0.069162\n",
      "batch 4530: loss 0.055718\n",
      "batch 4531: loss 0.072173\n",
      "batch 4532: loss 0.132722\n",
      "batch 4533: loss 0.020878\n",
      "batch 4534: loss 0.048128\n",
      "batch 4535: loss 0.042998\n",
      "batch 4536: loss 0.153969\n",
      "batch 4537: loss 0.063544\n",
      "batch 4538: loss 0.033681\n",
      "batch 4539: loss 0.142103\n",
      "batch 4540: loss 0.119154\n",
      "batch 4541: loss 0.024661\n",
      "batch 4542: loss 0.029217\n",
      "batch 4543: loss 0.037911\n",
      "batch 4544: loss 0.023041\n",
      "batch 4545: loss 0.044222\n",
      "batch 4546: loss 0.063927\n",
      "batch 4547: loss 0.066138\n",
      "batch 4548: loss 0.015787\n",
      "batch 4549: loss 0.008075\n",
      "batch 4550: loss 0.190955\n",
      "batch 4551: loss 0.051115\n",
      "batch 4552: loss 0.060013\n",
      "batch 4553: loss 0.060587\n",
      "batch 4554: loss 0.177455\n",
      "batch 4555: loss 0.097564\n",
      "batch 4556: loss 0.046201\n",
      "batch 4557: loss 0.213357\n",
      "batch 4558: loss 0.013437\n",
      "batch 4559: loss 0.009116\n",
      "batch 4560: loss 0.008829\n",
      "batch 4561: loss 0.112596\n",
      "batch 4562: loss 0.095629\n",
      "batch 4563: loss 0.292598\n",
      "batch 4564: loss 0.031305\n",
      "batch 4565: loss 0.032789\n",
      "batch 4566: loss 0.055662\n",
      "batch 4567: loss 0.071174\n",
      "batch 4568: loss 0.108070\n",
      "batch 4569: loss 0.095408\n",
      "batch 4570: loss 0.115764\n",
      "batch 4571: loss 0.110831\n",
      "batch 4572: loss 0.052362\n",
      "batch 4573: loss 0.059213\n",
      "batch 4574: loss 0.073965\n",
      "batch 4575: loss 0.025348\n",
      "batch 4576: loss 0.051054\n",
      "batch 4577: loss 0.045695\n",
      "batch 4578: loss 0.040785\n",
      "batch 4579: loss 0.071290\n",
      "batch 4580: loss 0.034554\n",
      "batch 4581: loss 0.018273\n",
      "batch 4582: loss 0.104808\n",
      "batch 4583: loss 0.045738\n",
      "batch 4584: loss 0.018993\n",
      "batch 4585: loss 0.024242\n",
      "batch 4586: loss 0.038047\n",
      "batch 4587: loss 0.120791\n",
      "batch 4588: loss 0.094810\n",
      "batch 4589: loss 0.028152\n",
      "batch 4590: loss 0.049831\n",
      "batch 4591: loss 0.120830\n",
      "batch 4592: loss 0.027516\n",
      "batch 4593: loss 0.052566\n",
      "batch 4594: loss 0.046668\n",
      "batch 4595: loss 0.164605\n",
      "batch 4596: loss 0.078270\n",
      "batch 4597: loss 0.060712\n",
      "batch 4598: loss 0.082277\n",
      "batch 4599: loss 0.061008\n",
      "batch 4600: loss 0.155660\n",
      "batch 4601: loss 0.242634\n",
      "batch 4602: loss 0.075853\n",
      "batch 4603: loss 0.050682\n",
      "batch 4604: loss 0.051753\n",
      "batch 4605: loss 0.035456\n",
      "batch 4606: loss 0.063378\n",
      "batch 4607: loss 0.064713\n",
      "batch 4608: loss 0.027210\n",
      "batch 4609: loss 0.028514\n",
      "batch 4610: loss 0.028700\n",
      "batch 4611: loss 0.025431\n",
      "batch 4612: loss 0.200479\n",
      "batch 4613: loss 0.179247\n",
      "batch 4614: loss 0.064691\n",
      "batch 4615: loss 0.042014\n",
      "batch 4616: loss 0.111322\n",
      "batch 4617: loss 0.108098\n",
      "batch 4618: loss 0.033638\n",
      "batch 4619: loss 0.027948\n",
      "batch 4620: loss 0.093924\n",
      "batch 4621: loss 0.034598\n",
      "batch 4622: loss 0.102339\n",
      "batch 4623: loss 0.024379\n",
      "batch 4624: loss 0.020411\n",
      "batch 4625: loss 0.112750\n",
      "batch 4626: loss 0.052054\n",
      "batch 4627: loss 0.043998\n",
      "batch 4628: loss 0.109678\n",
      "batch 4629: loss 0.039403\n",
      "batch 4630: loss 0.060283\n",
      "batch 4631: loss 0.092651\n",
      "batch 4632: loss 0.030001\n",
      "batch 4633: loss 0.142564\n",
      "batch 4634: loss 0.008217\n",
      "batch 4635: loss 0.048530\n",
      "batch 4636: loss 0.029774\n",
      "batch 4637: loss 0.064230\n",
      "batch 4638: loss 0.027543\n",
      "batch 4639: loss 0.021304\n",
      "batch 4640: loss 0.079901\n",
      "batch 4641: loss 0.076673\n",
      "batch 4642: loss 0.019193\n",
      "batch 4643: loss 0.058702\n",
      "batch 4644: loss 0.070747\n",
      "batch 4645: loss 0.114702\n",
      "batch 4646: loss 0.049714\n",
      "batch 4647: loss 0.010361\n",
      "batch 4648: loss 0.050577\n",
      "batch 4649: loss 0.022105\n",
      "batch 4650: loss 0.013725\n",
      "batch 4651: loss 0.155716\n",
      "batch 4652: loss 0.062434\n",
      "batch 4653: loss 0.014126\n",
      "batch 4654: loss 0.031513\n",
      "batch 4655: loss 0.019927\n",
      "batch 4656: loss 0.067820\n",
      "batch 4657: loss 0.134996\n",
      "batch 4658: loss 0.051336\n",
      "batch 4659: loss 0.177571\n",
      "batch 4660: loss 0.043929\n",
      "batch 4661: loss 0.067484\n",
      "batch 4662: loss 0.112641\n",
      "batch 4663: loss 0.030889\n",
      "batch 4664: loss 0.135104\n",
      "batch 4665: loss 0.051751\n",
      "batch 4666: loss 0.056293\n",
      "batch 4667: loss 0.044429\n",
      "batch 4668: loss 0.127810\n",
      "batch 4669: loss 0.024432\n",
      "batch 4670: loss 0.022227\n",
      "batch 4671: loss 0.160582\n",
      "batch 4672: loss 0.103715\n",
      "batch 4673: loss 0.017176\n",
      "batch 4674: loss 0.071871\n",
      "batch 4675: loss 0.078399\n",
      "batch 4676: loss 0.047113\n",
      "batch 4677: loss 0.054959\n",
      "batch 4678: loss 0.049159\n",
      "batch 4679: loss 0.042068\n",
      "batch 4680: loss 0.064381\n",
      "batch 4681: loss 0.116565\n",
      "batch 4682: loss 0.086551\n",
      "batch 4683: loss 0.008223\n",
      "batch 4684: loss 0.014953\n",
      "batch 4685: loss 0.238747\n",
      "batch 4686: loss 0.019908\n",
      "batch 4687: loss 0.076005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4688: loss 0.010835\n",
      "batch 4689: loss 0.033721\n",
      "batch 4690: loss 0.096444\n",
      "batch 4691: loss 0.040813\n",
      "batch 4692: loss 0.044528\n",
      "batch 4693: loss 0.015718\n",
      "batch 4694: loss 0.042009\n",
      "batch 4695: loss 0.011851\n",
      "batch 4696: loss 0.051492\n",
      "batch 4697: loss 0.167992\n",
      "batch 4698: loss 0.017238\n",
      "batch 4699: loss 0.064260\n",
      "batch 4700: loss 0.022510\n",
      "batch 4701: loss 0.009709\n",
      "batch 4702: loss 0.023593\n",
      "batch 4703: loss 0.015835\n",
      "batch 4704: loss 0.119046\n",
      "batch 4705: loss 0.015718\n",
      "batch 4706: loss 0.039922\n",
      "batch 4707: loss 0.041163\n",
      "batch 4708: loss 0.063659\n",
      "batch 4709: loss 0.011262\n",
      "batch 4710: loss 0.089558\n",
      "batch 4711: loss 0.045165\n",
      "batch 4712: loss 0.192878\n",
      "batch 4713: loss 0.017082\n",
      "batch 4714: loss 0.105356\n",
      "batch 4715: loss 0.069500\n",
      "batch 4716: loss 0.021483\n",
      "batch 4717: loss 0.035416\n",
      "batch 4718: loss 0.136590\n",
      "batch 4719: loss 0.008785\n",
      "batch 4720: loss 0.014541\n",
      "batch 4721: loss 0.126283\n",
      "batch 4722: loss 0.021651\n",
      "batch 4723: loss 0.096506\n",
      "batch 4724: loss 0.029263\n",
      "batch 4725: loss 0.033943\n",
      "batch 4726: loss 0.066927\n",
      "batch 4727: loss 0.108317\n",
      "batch 4728: loss 0.056715\n",
      "batch 4729: loss 0.010397\n",
      "batch 4730: loss 0.073176\n",
      "batch 4731: loss 0.024340\n",
      "batch 4732: loss 0.020249\n",
      "batch 4733: loss 0.056939\n",
      "batch 4734: loss 0.097417\n",
      "batch 4735: loss 0.197279\n",
      "batch 4736: loss 0.044169\n",
      "batch 4737: loss 0.042989\n",
      "batch 4738: loss 0.057371\n",
      "batch 4739: loss 0.011181\n",
      "batch 4740: loss 0.062063\n",
      "batch 4741: loss 0.016243\n",
      "batch 4742: loss 0.082251\n",
      "batch 4743: loss 0.021614\n",
      "batch 4744: loss 0.023909\n",
      "batch 4745: loss 0.022932\n",
      "batch 4746: loss 0.109590\n",
      "batch 4747: loss 0.031839\n",
      "batch 4748: loss 0.063535\n",
      "batch 4749: loss 0.049043\n",
      "batch 4750: loss 0.063434\n",
      "batch 4751: loss 0.039506\n",
      "batch 4752: loss 0.023658\n",
      "batch 4753: loss 0.020574\n",
      "batch 4754: loss 0.130625\n",
      "batch 4755: loss 0.121919\n",
      "batch 4756: loss 0.067897\n",
      "batch 4757: loss 0.042185\n",
      "batch 4758: loss 0.041665\n",
      "batch 4759: loss 0.047469\n",
      "batch 4760: loss 0.032175\n",
      "batch 4761: loss 0.025837\n",
      "batch 4762: loss 0.063524\n",
      "batch 4763: loss 0.023763\n",
      "batch 4764: loss 0.014577\n",
      "batch 4765: loss 0.092710\n",
      "batch 4766: loss 0.028672\n",
      "batch 4767: loss 0.203479\n",
      "batch 4768: loss 0.065050\n",
      "batch 4769: loss 0.016867\n",
      "batch 4770: loss 0.060480\n",
      "batch 4771: loss 0.181441\n",
      "batch 4772: loss 0.012852\n",
      "batch 4773: loss 0.178750\n",
      "batch 4774: loss 0.140967\n",
      "batch 4775: loss 0.131208\n",
      "batch 4776: loss 0.030199\n",
      "batch 4777: loss 0.174177\n",
      "batch 4778: loss 0.037624\n",
      "batch 4779: loss 0.020114\n",
      "batch 4780: loss 0.054319\n",
      "batch 4781: loss 0.005356\n",
      "batch 4782: loss 0.083327\n",
      "batch 4783: loss 0.131990\n",
      "batch 4784: loss 0.042788\n",
      "batch 4785: loss 0.049271\n",
      "batch 4786: loss 0.029429\n",
      "batch 4787: loss 0.112284\n",
      "batch 4788: loss 0.030332\n",
      "batch 4789: loss 0.075837\n",
      "batch 4790: loss 0.022845\n",
      "batch 4791: loss 0.052495\n",
      "batch 4792: loss 0.044394\n",
      "batch 4793: loss 0.055840\n",
      "batch 4794: loss 0.134166\n",
      "batch 4795: loss 0.016621\n",
      "batch 4796: loss 0.015772\n",
      "batch 4797: loss 0.031981\n",
      "batch 4798: loss 0.022028\n",
      "batch 4799: loss 0.112425\n",
      "batch 4800: loss 0.078389\n",
      "batch 4801: loss 0.059364\n",
      "batch 4802: loss 0.100084\n",
      "batch 4803: loss 0.126905\n",
      "batch 4804: loss 0.199226\n",
      "batch 4805: loss 0.023436\n",
      "batch 4806: loss 0.014124\n",
      "batch 4807: loss 0.103706\n",
      "batch 4808: loss 0.029154\n",
      "batch 4809: loss 0.160023\n",
      "batch 4810: loss 0.182196\n",
      "batch 4811: loss 0.018442\n",
      "batch 4812: loss 0.005233\n",
      "batch 4813: loss 0.066721\n",
      "batch 4814: loss 0.034895\n",
      "batch 4815: loss 0.012904\n",
      "batch 4816: loss 0.062779\n",
      "batch 4817: loss 0.087578\n",
      "batch 4818: loss 0.083868\n",
      "batch 4819: loss 0.005448\n",
      "batch 4820: loss 0.084408\n",
      "batch 4821: loss 0.046418\n",
      "batch 4822: loss 0.136981\n",
      "batch 4823: loss 0.015204\n",
      "batch 4824: loss 0.008473\n",
      "batch 4825: loss 0.072574\n",
      "batch 4826: loss 0.038188\n",
      "batch 4827: loss 0.072082\n",
      "batch 4828: loss 0.051423\n",
      "batch 4829: loss 0.029345\n",
      "batch 4830: loss 0.062198\n",
      "batch 4831: loss 0.077486\n",
      "batch 4832: loss 0.053529\n",
      "batch 4833: loss 0.014396\n",
      "batch 4834: loss 0.127609\n",
      "batch 4835: loss 0.008355\n",
      "batch 4836: loss 0.031989\n",
      "batch 4837: loss 0.011334\n",
      "batch 4838: loss 0.098771\n",
      "batch 4839: loss 0.035740\n",
      "batch 4840: loss 0.186259\n",
      "batch 4841: loss 0.040777\n",
      "batch 4842: loss 0.136278\n",
      "batch 4843: loss 0.019991\n",
      "batch 4844: loss 0.047607\n",
      "batch 4845: loss 0.100198\n",
      "batch 4846: loss 0.152460\n",
      "batch 4847: loss 0.025400\n",
      "batch 4848: loss 0.051524\n",
      "batch 4849: loss 0.061674\n",
      "batch 4850: loss 0.030715\n",
      "batch 4851: loss 0.053075\n",
      "batch 4852: loss 0.089461\n",
      "batch 4853: loss 0.020561\n",
      "batch 4854: loss 0.045301\n",
      "batch 4855: loss 0.070714\n",
      "batch 4856: loss 0.013250\n",
      "batch 4857: loss 0.108371\n",
      "batch 4858: loss 0.034880\n",
      "batch 4859: loss 0.043314\n",
      "batch 4860: loss 0.075657\n",
      "batch 4861: loss 0.009972\n",
      "batch 4862: loss 0.033450\n",
      "batch 4863: loss 0.030655\n",
      "batch 4864: loss 0.023985\n",
      "batch 4865: loss 0.004502\n",
      "batch 4866: loss 0.089120\n",
      "batch 4867: loss 0.012580\n",
      "batch 4868: loss 0.071361\n",
      "batch 4869: loss 0.014491\n",
      "batch 4870: loss 0.010871\n",
      "batch 4871: loss 0.037637\n",
      "batch 4872: loss 0.041202\n",
      "batch 4873: loss 0.079404\n",
      "batch 4874: loss 0.052280\n",
      "batch 4875: loss 0.067572\n",
      "batch 4876: loss 0.069322\n",
      "batch 4877: loss 0.010195\n",
      "batch 4878: loss 0.039064\n",
      "batch 4879: loss 0.013554\n",
      "batch 4880: loss 0.204630\n",
      "batch 4881: loss 0.049566\n",
      "batch 4882: loss 0.063158\n",
      "batch 4883: loss 0.019661\n",
      "batch 4884: loss 0.044857\n",
      "batch 4885: loss 0.083381\n",
      "batch 4886: loss 0.096197\n",
      "batch 4887: loss 0.062570\n",
      "batch 4888: loss 0.024788\n",
      "batch 4889: loss 0.016573\n",
      "batch 4890: loss 0.038072\n",
      "batch 4891: loss 0.142616\n",
      "batch 4892: loss 0.033424\n",
      "batch 4893: loss 0.032401\n",
      "batch 4894: loss 0.020268\n",
      "batch 4895: loss 0.025577\n",
      "batch 4896: loss 0.033750\n",
      "batch 4897: loss 0.125439\n",
      "batch 4898: loss 0.024272\n",
      "batch 4899: loss 0.076526\n",
      "batch 4900: loss 0.137280\n",
      "batch 4901: loss 0.011412\n",
      "batch 4902: loss 0.165192\n",
      "batch 4903: loss 0.019520\n",
      "batch 4904: loss 0.080948\n",
      "batch 4905: loss 0.024766\n",
      "batch 4906: loss 0.048570\n",
      "batch 4907: loss 0.063262\n",
      "batch 4908: loss 0.080989\n",
      "batch 4909: loss 0.030276\n",
      "batch 4910: loss 0.070821\n",
      "batch 4911: loss 0.167872\n",
      "batch 4912: loss 0.035315\n",
      "batch 4913: loss 0.031925\n",
      "batch 4914: loss 0.086778\n",
      "batch 4915: loss 0.022201\n",
      "batch 4916: loss 0.138523\n",
      "batch 4917: loss 0.040216\n",
      "batch 4918: loss 0.024806\n",
      "batch 4919: loss 0.018678\n",
      "batch 4920: loss 0.008408\n",
      "batch 4921: loss 0.058469\n",
      "batch 4922: loss 0.054712\n",
      "batch 4923: loss 0.067898\n",
      "batch 4924: loss 0.107869\n",
      "batch 4925: loss 0.083834\n",
      "batch 4926: loss 0.116366\n",
      "batch 4927: loss 0.116872\n",
      "batch 4928: loss 0.113202\n",
      "batch 4929: loss 0.116881\n",
      "batch 4930: loss 0.053848\n",
      "batch 4931: loss 0.021695\n",
      "batch 4932: loss 0.058658\n",
      "batch 4933: loss 0.062219\n",
      "batch 4934: loss 0.073587\n",
      "batch 4935: loss 0.189550\n",
      "batch 4936: loss 0.059015\n",
      "batch 4937: loss 0.007977\n",
      "batch 4938: loss 0.225041\n",
      "batch 4939: loss 0.052172\n",
      "batch 4940: loss 0.011171\n",
      "batch 4941: loss 0.072801\n",
      "batch 4942: loss 0.070300\n",
      "batch 4943: loss 0.225957\n",
      "batch 4944: loss 0.109665\n",
      "batch 4945: loss 0.089757\n",
      "batch 4946: loss 0.007595\n",
      "batch 4947: loss 0.031030\n",
      "batch 4948: loss 0.034063\n",
      "batch 4949: loss 0.049388\n",
      "batch 4950: loss 0.068876\n",
      "batch 4951: loss 0.126409\n",
      "batch 4952: loss 0.026276\n",
      "batch 4953: loss 0.111696\n",
      "batch 4954: loss 0.030814\n",
      "batch 4955: loss 0.250579\n",
      "batch 4956: loss 0.113310\n",
      "batch 4957: loss 0.048960\n",
      "batch 4958: loss 0.078836\n",
      "batch 4959: loss 0.026577\n",
      "batch 4960: loss 0.008357\n",
      "batch 4961: loss 0.046970\n",
      "batch 4962: loss 0.048899\n",
      "batch 4963: loss 0.049030\n",
      "batch 4964: loss 0.044455\n",
      "batch 4965: loss 0.048140\n",
      "batch 4966: loss 0.056454\n",
      "batch 4967: loss 0.055367\n",
      "batch 4968: loss 0.143256\n",
      "batch 4969: loss 0.064185\n",
      "batch 4970: loss 0.038819\n",
      "batch 4971: loss 0.023509\n",
      "batch 4972: loss 0.073006\n",
      "batch 4973: loss 0.096747\n",
      "batch 4974: loss 0.034723\n",
      "batch 4975: loss 0.098018\n",
      "batch 4976: loss 0.029495\n",
      "batch 4977: loss 0.036534\n",
      "batch 4978: loss 0.072767\n",
      "batch 4979: loss 0.100285\n",
      "batch 4980: loss 0.013216\n",
      "batch 4981: loss 0.068163\n",
      "batch 4982: loss 0.008030\n",
      "batch 4983: loss 0.241016\n",
      "batch 4984: loss 0.048059\n",
      "batch 4985: loss 0.120354\n",
      "batch 4986: loss 0.100376\n",
      "batch 4987: loss 0.084326\n",
      "batch 4988: loss 0.049648\n",
      "batch 4989: loss 0.003382\n",
      "batch 4990: loss 0.126357\n",
      "batch 4991: loss 0.155482\n",
      "batch 4992: loss 0.076096\n",
      "batch 4993: loss 0.030928\n",
      "batch 4994: loss 0.014651\n",
      "batch 4995: loss 0.030845\n",
      "batch 4996: loss 0.067907\n",
      "batch 4997: loss 0.026458\n",
      "batch 4998: loss 0.084638\n",
      "batch 4999: loss 0.067391\n",
      "batch 5000: loss 0.040135\n",
      "batch 5001: loss 0.025831\n",
      "batch 5002: loss 0.009311\n",
      "batch 5003: loss 0.034146\n",
      "batch 5004: loss 0.154817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5005: loss 0.051935\n",
      "batch 5006: loss 0.066690\n",
      "batch 5007: loss 0.073702\n",
      "batch 5008: loss 0.064164\n",
      "batch 5009: loss 0.008883\n",
      "batch 5010: loss 0.041584\n",
      "batch 5011: loss 0.119651\n",
      "batch 5012: loss 0.034572\n",
      "batch 5013: loss 0.081821\n",
      "batch 5014: loss 0.079495\n",
      "batch 5015: loss 0.034668\n",
      "batch 5016: loss 0.009789\n",
      "batch 5017: loss 0.035309\n",
      "batch 5018: loss 0.019712\n",
      "batch 5019: loss 0.029401\n",
      "batch 5020: loss 0.016298\n",
      "batch 5021: loss 0.058716\n",
      "batch 5022: loss 0.062317\n",
      "batch 5023: loss 0.019224\n",
      "batch 5024: loss 0.031702\n",
      "batch 5025: loss 0.146854\n",
      "batch 5026: loss 0.026937\n",
      "batch 5027: loss 0.051618\n",
      "batch 5028: loss 0.038016\n",
      "batch 5029: loss 0.020106\n",
      "batch 5030: loss 0.032176\n",
      "batch 5031: loss 0.059331\n",
      "batch 5032: loss 0.077548\n",
      "batch 5033: loss 0.010147\n",
      "batch 5034: loss 0.032305\n",
      "batch 5035: loss 0.061714\n",
      "batch 5036: loss 0.055271\n",
      "batch 5037: loss 0.049094\n",
      "batch 5038: loss 0.020020\n",
      "batch 5039: loss 0.012534\n",
      "batch 5040: loss 0.013121\n",
      "batch 5041: loss 0.039734\n",
      "batch 5042: loss 0.022150\n",
      "batch 5043: loss 0.008578\n",
      "batch 5044: loss 0.055290\n",
      "batch 5045: loss 0.039359\n",
      "batch 5046: loss 0.060212\n",
      "batch 5047: loss 0.078404\n",
      "batch 5048: loss 0.072248\n",
      "batch 5049: loss 0.029128\n",
      "batch 5050: loss 0.192855\n",
      "batch 5051: loss 0.042231\n",
      "batch 5052: loss 0.095016\n",
      "batch 5053: loss 0.099619\n",
      "batch 5054: loss 0.022732\n",
      "batch 5055: loss 0.005024\n",
      "batch 5056: loss 0.061793\n",
      "batch 5057: loss 0.011802\n",
      "batch 5058: loss 0.053986\n",
      "batch 5059: loss 0.091854\n",
      "batch 5060: loss 0.041263\n",
      "batch 5061: loss 0.048821\n",
      "batch 5062: loss 0.061089\n",
      "batch 5063: loss 0.225304\n",
      "batch 5064: loss 0.072835\n",
      "batch 5065: loss 0.041012\n",
      "batch 5066: loss 0.024175\n",
      "batch 5067: loss 0.042205\n",
      "batch 5068: loss 0.139093\n",
      "batch 5069: loss 0.045669\n",
      "batch 5070: loss 0.030949\n",
      "batch 5071: loss 0.067765\n",
      "batch 5072: loss 0.022447\n",
      "batch 5073: loss 0.035800\n",
      "batch 5074: loss 0.140813\n",
      "batch 5075: loss 0.074631\n",
      "batch 5076: loss 0.037971\n",
      "batch 5077: loss 0.025147\n",
      "batch 5078: loss 0.111200\n",
      "batch 5079: loss 0.014793\n",
      "batch 5080: loss 0.084038\n",
      "batch 5081: loss 0.028620\n",
      "batch 5082: loss 0.122069\n",
      "batch 5083: loss 0.053311\n",
      "batch 5084: loss 0.026545\n",
      "batch 5085: loss 0.070391\n",
      "batch 5086: loss 0.033401\n",
      "batch 5087: loss 0.026842\n",
      "batch 5088: loss 0.026738\n",
      "batch 5089: loss 0.033118\n",
      "batch 5090: loss 0.034107\n",
      "batch 5091: loss 0.063279\n",
      "batch 5092: loss 0.023880\n",
      "batch 5093: loss 0.039895\n",
      "batch 5094: loss 0.085879\n",
      "batch 5095: loss 0.043634\n",
      "batch 5096: loss 0.022661\n",
      "batch 5097: loss 0.080896\n",
      "batch 5098: loss 0.032001\n",
      "batch 5099: loss 0.042019\n",
      "batch 5100: loss 0.012717\n",
      "batch 5101: loss 0.039651\n",
      "batch 5102: loss 0.136652\n",
      "batch 5103: loss 0.004845\n",
      "batch 5104: loss 0.034072\n",
      "batch 5105: loss 0.136916\n",
      "batch 5106: loss 0.145741\n",
      "batch 5107: loss 0.104506\n",
      "batch 5108: loss 0.012572\n",
      "batch 5109: loss 0.008851\n",
      "batch 5110: loss 0.017668\n",
      "batch 5111: loss 0.016557\n",
      "batch 5112: loss 0.032768\n",
      "batch 5113: loss 0.018454\n",
      "batch 5114: loss 0.024524\n",
      "batch 5115: loss 0.051394\n",
      "batch 5116: loss 0.076907\n",
      "batch 5117: loss 0.072831\n",
      "batch 5118: loss 0.021900\n",
      "batch 5119: loss 0.019731\n",
      "batch 5120: loss 0.112076\n",
      "batch 5121: loss 0.007755\n",
      "batch 5122: loss 0.011052\n",
      "batch 5123: loss 0.015535\n",
      "batch 5124: loss 0.050915\n",
      "batch 5125: loss 0.015236\n",
      "batch 5126: loss 0.164675\n",
      "batch 5127: loss 0.099857\n",
      "batch 5128: loss 0.095099\n",
      "batch 5129: loss 0.026560\n",
      "batch 5130: loss 0.067308\n",
      "batch 5131: loss 0.088491\n",
      "batch 5132: loss 0.067321\n",
      "batch 5133: loss 0.015382\n",
      "batch 5134: loss 0.027950\n",
      "batch 5135: loss 0.050881\n",
      "batch 5136: loss 0.041586\n",
      "batch 5137: loss 0.280285\n",
      "batch 5138: loss 0.038542\n",
      "batch 5139: loss 0.013652\n",
      "batch 5140: loss 0.040207\n",
      "batch 5141: loss 0.013657\n",
      "batch 5142: loss 0.064609\n",
      "batch 5143: loss 0.050634\n",
      "batch 5144: loss 0.047855\n",
      "batch 5145: loss 0.024661\n",
      "batch 5146: loss 0.053248\n",
      "batch 5147: loss 0.351856\n",
      "batch 5148: loss 0.119101\n",
      "batch 5149: loss 0.013851\n",
      "batch 5150: loss 0.014370\n",
      "batch 5151: loss 0.016837\n",
      "batch 5152: loss 0.032172\n",
      "batch 5153: loss 0.038073\n",
      "batch 5154: loss 0.050924\n",
      "batch 5155: loss 0.014622\n",
      "batch 5156: loss 0.016334\n",
      "batch 5157: loss 0.036390\n",
      "batch 5158: loss 0.067311\n",
      "batch 5159: loss 0.063701\n",
      "batch 5160: loss 0.217016\n",
      "batch 5161: loss 0.108483\n",
      "batch 5162: loss 0.030553\n",
      "batch 5163: loss 0.044622\n",
      "batch 5164: loss 0.019798\n",
      "batch 5165: loss 0.075705\n",
      "batch 5166: loss 0.011572\n",
      "batch 5167: loss 0.026644\n",
      "batch 5168: loss 0.013956\n",
      "batch 5169: loss 0.021763\n",
      "batch 5170: loss 0.029149\n",
      "batch 5171: loss 0.050916\n",
      "batch 5172: loss 0.026216\n",
      "batch 5173: loss 0.035588\n",
      "batch 5174: loss 0.039938\n",
      "batch 5175: loss 0.065168\n",
      "batch 5176: loss 0.013672\n",
      "batch 5177: loss 0.072626\n",
      "batch 5178: loss 0.108405\n",
      "batch 5179: loss 0.042281\n",
      "batch 5180: loss 0.013870\n",
      "batch 5181: loss 0.057380\n",
      "batch 5182: loss 0.010551\n",
      "batch 5183: loss 0.100085\n",
      "batch 5184: loss 0.038847\n",
      "batch 5185: loss 0.011501\n",
      "batch 5186: loss 0.077498\n",
      "batch 5187: loss 0.077224\n",
      "batch 5188: loss 0.009547\n",
      "batch 5189: loss 0.048564\n",
      "batch 5190: loss 0.021315\n",
      "batch 5191: loss 0.066965\n",
      "batch 5192: loss 0.059719\n",
      "batch 5193: loss 0.020073\n",
      "batch 5194: loss 0.014527\n",
      "batch 5195: loss 0.022409\n",
      "batch 5196: loss 0.349275\n",
      "batch 5197: loss 0.027321\n",
      "batch 5198: loss 0.102695\n",
      "batch 5199: loss 0.086287\n",
      "batch 5200: loss 0.154462\n",
      "batch 5201: loss 0.096706\n",
      "batch 5202: loss 0.028017\n",
      "batch 5203: loss 0.017121\n",
      "batch 5204: loss 0.069939\n",
      "batch 5205: loss 0.168618\n",
      "batch 5206: loss 0.073616\n",
      "batch 5207: loss 0.071452\n",
      "batch 5208: loss 0.027046\n",
      "batch 5209: loss 0.091596\n",
      "batch 5210: loss 0.106030\n",
      "batch 5211: loss 0.009059\n",
      "batch 5212: loss 0.285347\n",
      "batch 5213: loss 0.060181\n",
      "batch 5214: loss 0.018825\n",
      "batch 5215: loss 0.025650\n",
      "batch 5216: loss 0.037750\n",
      "batch 5217: loss 0.006666\n",
      "batch 5218: loss 0.031440\n",
      "batch 5219: loss 0.045368\n",
      "batch 5220: loss 0.038818\n",
      "batch 5221: loss 0.015887\n",
      "batch 5222: loss 0.066857\n",
      "batch 5223: loss 0.204162\n",
      "batch 5224: loss 0.013859\n",
      "batch 5225: loss 0.054782\n",
      "batch 5226: loss 0.016110\n",
      "batch 5227: loss 0.019637\n",
      "batch 5228: loss 0.089600\n",
      "batch 5229: loss 0.026408\n",
      "batch 5230: loss 0.022771\n",
      "batch 5231: loss 0.069891\n",
      "batch 5232: loss 0.024492\n",
      "batch 5233: loss 0.054615\n",
      "batch 5234: loss 0.098562\n",
      "batch 5235: loss 0.045782\n",
      "batch 5236: loss 0.052326\n",
      "batch 5237: loss 0.111809\n",
      "batch 5238: loss 0.102379\n",
      "batch 5239: loss 0.011544\n",
      "batch 5240: loss 0.032567\n",
      "batch 5241: loss 0.008900\n",
      "batch 5242: loss 0.019596\n",
      "batch 5243: loss 0.018908\n",
      "batch 5244: loss 0.031967\n",
      "batch 5245: loss 0.118722\n",
      "batch 5246: loss 0.024282\n",
      "batch 5247: loss 0.017813\n",
      "batch 5248: loss 0.035666\n",
      "batch 5249: loss 0.072749\n",
      "batch 5250: loss 0.117994\n",
      "batch 5251: loss 0.089518\n",
      "batch 5252: loss 0.024232\n",
      "batch 5253: loss 0.019254\n",
      "batch 5254: loss 0.084781\n",
      "batch 5255: loss 0.043684\n",
      "batch 5256: loss 0.023412\n",
      "batch 5257: loss 0.064762\n",
      "batch 5258: loss 0.032370\n",
      "batch 5259: loss 0.119117\n",
      "batch 5260: loss 0.043888\n",
      "batch 5261: loss 0.030333\n",
      "batch 5262: loss 0.192140\n",
      "batch 5263: loss 0.137004\n",
      "batch 5264: loss 0.031490\n",
      "batch 5265: loss 0.015290\n",
      "batch 5266: loss 0.078870\n",
      "batch 5267: loss 0.010524\n",
      "batch 5268: loss 0.108791\n",
      "batch 5269: loss 0.047739\n",
      "batch 5270: loss 0.072765\n",
      "batch 5271: loss 0.018804\n",
      "batch 5272: loss 0.038640\n",
      "batch 5273: loss 0.030761\n",
      "batch 5274: loss 0.084465\n",
      "batch 5275: loss 0.030281\n",
      "batch 5276: loss 0.044672\n",
      "batch 5277: loss 0.019792\n",
      "batch 5278: loss 0.020131\n",
      "batch 5279: loss 0.029472\n",
      "batch 5280: loss 0.005206\n",
      "batch 5281: loss 0.045620\n",
      "batch 5282: loss 0.049620\n",
      "batch 5283: loss 0.134875\n",
      "batch 5284: loss 0.107179\n",
      "batch 5285: loss 0.084682\n",
      "batch 5286: loss 0.127749\n",
      "batch 5287: loss 0.020894\n",
      "batch 5288: loss 0.061894\n",
      "batch 5289: loss 0.070179\n",
      "batch 5290: loss 0.142733\n",
      "batch 5291: loss 0.058343\n",
      "batch 5292: loss 0.043901\n",
      "batch 5293: loss 0.097050\n",
      "batch 5294: loss 0.006251\n",
      "batch 5295: loss 0.051116\n",
      "batch 5296: loss 0.070923\n",
      "batch 5297: loss 0.046314\n",
      "batch 5298: loss 0.159678\n",
      "batch 5299: loss 0.055905\n",
      "batch 5300: loss 0.027934\n",
      "batch 5301: loss 0.090325\n",
      "batch 5302: loss 0.020615\n",
      "batch 5303: loss 0.027496\n",
      "batch 5304: loss 0.045364\n",
      "batch 5305: loss 0.025612\n",
      "batch 5306: loss 0.030284\n",
      "batch 5307: loss 0.023900\n",
      "batch 5308: loss 0.034890\n",
      "batch 5309: loss 0.009037\n",
      "batch 5310: loss 0.036585\n",
      "batch 5311: loss 0.056618\n",
      "batch 5312: loss 0.068573\n",
      "batch 5313: loss 0.079424\n",
      "batch 5314: loss 0.061966\n",
      "batch 5315: loss 0.065350\n",
      "batch 5316: loss 0.064524\n",
      "batch 5317: loss 0.046394\n",
      "batch 5318: loss 0.045210\n",
      "batch 5319: loss 0.056951\n",
      "batch 5320: loss 0.116935\n",
      "batch 5321: loss 0.025179\n",
      "batch 5322: loss 0.092795\n",
      "batch 5323: loss 0.045809\n",
      "batch 5324: loss 0.030266\n",
      "batch 5325: loss 0.090884\n",
      "batch 5326: loss 0.005829\n",
      "batch 5327: loss 0.018132\n",
      "batch 5328: loss 0.031967\n",
      "batch 5329: loss 0.037603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5330: loss 0.138638\n",
      "batch 5331: loss 0.056260\n",
      "batch 5332: loss 0.067022\n",
      "batch 5333: loss 0.099980\n",
      "batch 5334: loss 0.075328\n",
      "batch 5335: loss 0.011514\n",
      "batch 5336: loss 0.034556\n",
      "batch 5337: loss 0.125911\n",
      "batch 5338: loss 0.084765\n",
      "batch 5339: loss 0.052727\n",
      "batch 5340: loss 0.037836\n",
      "batch 5341: loss 0.023832\n",
      "batch 5342: loss 0.020909\n",
      "batch 5343: loss 0.054654\n",
      "batch 5344: loss 0.041454\n",
      "batch 5345: loss 0.066577\n",
      "batch 5346: loss 0.021023\n",
      "batch 5347: loss 0.057578\n",
      "batch 5348: loss 0.009875\n",
      "batch 5349: loss 0.044495\n",
      "batch 5350: loss 0.046006\n",
      "batch 5351: loss 0.032701\n",
      "batch 5352: loss 0.033488\n",
      "batch 5353: loss 0.030610\n",
      "batch 5354: loss 0.026291\n",
      "batch 5355: loss 0.025432\n",
      "batch 5356: loss 0.053662\n",
      "batch 5357: loss 0.012976\n",
      "batch 5358: loss 0.131215\n",
      "batch 5359: loss 0.130936\n",
      "batch 5360: loss 0.015193\n",
      "batch 5361: loss 0.045764\n",
      "batch 5362: loss 0.053929\n",
      "batch 5363: loss 0.055858\n",
      "batch 5364: loss 0.079335\n",
      "batch 5365: loss 0.056187\n",
      "batch 5366: loss 0.009735\n",
      "batch 5367: loss 0.278565\n",
      "batch 5368: loss 0.040640\n",
      "batch 5369: loss 0.040232\n",
      "batch 5370: loss 0.058255\n",
      "batch 5371: loss 0.042913\n",
      "batch 5372: loss 0.055573\n",
      "batch 5373: loss 0.210478\n",
      "batch 5374: loss 0.039382\n",
      "batch 5375: loss 0.112998\n",
      "batch 5376: loss 0.020709\n",
      "batch 5377: loss 0.024100\n",
      "batch 5378: loss 0.196198\n",
      "batch 5379: loss 0.078610\n",
      "batch 5380: loss 0.081561\n",
      "batch 5381: loss 0.024544\n",
      "batch 5382: loss 0.053548\n",
      "batch 5383: loss 0.072300\n",
      "batch 5384: loss 0.017688\n",
      "batch 5385: loss 0.025222\n",
      "batch 5386: loss 0.048546\n",
      "batch 5387: loss 0.065490\n",
      "batch 5388: loss 0.048746\n",
      "batch 5389: loss 0.062392\n",
      "batch 5390: loss 0.059645\n",
      "batch 5391: loss 0.019242\n",
      "batch 5392: loss 0.045313\n",
      "batch 5393: loss 0.064372\n",
      "batch 5394: loss 0.130157\n",
      "batch 5395: loss 0.054574\n",
      "batch 5396: loss 0.099975\n",
      "batch 5397: loss 0.072882\n",
      "batch 5398: loss 0.023828\n",
      "batch 5399: loss 0.019930\n",
      "batch 5400: loss 0.054578\n",
      "batch 5401: loss 0.008742\n",
      "batch 5402: loss 0.019347\n",
      "batch 5403: loss 0.028870\n",
      "batch 5404: loss 0.126300\n",
      "batch 5405: loss 0.153919\n",
      "batch 5406: loss 0.036987\n",
      "batch 5407: loss 0.010082\n",
      "batch 5408: loss 0.048132\n",
      "batch 5409: loss 0.046868\n",
      "batch 5410: loss 0.022944\n",
      "batch 5411: loss 0.068673\n",
      "batch 5412: loss 0.015711\n",
      "batch 5413: loss 0.023490\n",
      "batch 5414: loss 0.064649\n",
      "batch 5415: loss 0.049422\n",
      "batch 5416: loss 0.044303\n",
      "batch 5417: loss 0.061103\n",
      "batch 5418: loss 0.081974\n",
      "batch 5419: loss 0.027111\n",
      "batch 5420: loss 0.072475\n",
      "batch 5421: loss 0.046699\n",
      "batch 5422: loss 0.008713\n",
      "batch 5423: loss 0.056382\n",
      "batch 5424: loss 0.035156\n",
      "batch 5425: loss 0.088209\n",
      "batch 5426: loss 0.029199\n",
      "batch 5427: loss 0.040005\n",
      "batch 5428: loss 0.191806\n",
      "batch 5429: loss 0.026970\n",
      "batch 5430: loss 0.030255\n",
      "batch 5431: loss 0.064192\n",
      "batch 5432: loss 0.050703\n",
      "batch 5433: loss 0.074689\n",
      "batch 5434: loss 0.023620\n",
      "batch 5435: loss 0.032839\n",
      "batch 5436: loss 0.023222\n",
      "batch 5437: loss 0.041711\n",
      "batch 5438: loss 0.102725\n",
      "batch 5439: loss 0.094905\n",
      "batch 5440: loss 0.052954\n",
      "batch 5441: loss 0.039059\n",
      "batch 5442: loss 0.022709\n",
      "batch 5443: loss 0.014298\n",
      "batch 5444: loss 0.111839\n",
      "batch 5445: loss 0.060379\n",
      "batch 5446: loss 0.097048\n",
      "batch 5447: loss 0.034437\n",
      "batch 5448: loss 0.031135\n",
      "batch 5449: loss 0.018466\n",
      "batch 5450: loss 0.034418\n",
      "batch 5451: loss 0.041903\n",
      "batch 5452: loss 0.103733\n",
      "batch 5453: loss 0.082536\n",
      "batch 5454: loss 0.038570\n",
      "batch 5455: loss 0.051863\n",
      "batch 5456: loss 0.022522\n",
      "batch 5457: loss 0.012247\n",
      "batch 5458: loss 0.070030\n",
      "batch 5459: loss 0.131241\n",
      "batch 5460: loss 0.062866\n",
      "batch 5461: loss 0.069697\n",
      "batch 5462: loss 0.069306\n",
      "batch 5463: loss 0.073495\n",
      "batch 5464: loss 0.235202\n",
      "batch 5465: loss 0.072756\n",
      "batch 5466: loss 0.129920\n",
      "batch 5467: loss 0.043632\n",
      "batch 5468: loss 0.011449\n",
      "batch 5469: loss 0.184708\n",
      "batch 5470: loss 0.024715\n",
      "batch 5471: loss 0.056026\n",
      "batch 5472: loss 0.036326\n",
      "batch 5473: loss 0.033551\n",
      "batch 5474: loss 0.018706\n",
      "batch 5475: loss 0.073954\n",
      "batch 5476: loss 0.050629\n",
      "batch 5477: loss 0.111734\n",
      "batch 5478: loss 0.033032\n",
      "batch 5479: loss 0.166974\n",
      "batch 5480: loss 0.046089\n",
      "batch 5481: loss 0.032781\n",
      "batch 5482: loss 0.047890\n",
      "batch 5483: loss 0.036129\n",
      "batch 5484: loss 0.062963\n",
      "batch 5485: loss 0.056373\n",
      "batch 5486: loss 0.093528\n",
      "batch 5487: loss 0.066629\n",
      "batch 5488: loss 0.111974\n",
      "batch 5489: loss 0.059155\n",
      "batch 5490: loss 0.058831\n",
      "batch 5491: loss 0.072171\n",
      "batch 5492: loss 0.106120\n",
      "batch 5493: loss 0.026648\n",
      "batch 5494: loss 0.024553\n",
      "batch 5495: loss 0.144759\n",
      "batch 5496: loss 0.127165\n",
      "batch 5497: loss 0.020985\n",
      "batch 5498: loss 0.095570\n",
      "batch 5499: loss 0.021163\n",
      "batch 5500: loss 0.041772\n",
      "batch 5501: loss 0.006729\n",
      "batch 5502: loss 0.008869\n",
      "batch 5503: loss 0.078500\n",
      "batch 5504: loss 0.022867\n",
      "batch 5505: loss 0.058165\n",
      "batch 5506: loss 0.102009\n",
      "batch 5507: loss 0.077762\n",
      "batch 5508: loss 0.023407\n",
      "batch 5509: loss 0.009141\n",
      "batch 5510: loss 0.023282\n",
      "batch 5511: loss 0.041758\n",
      "batch 5512: loss 0.061806\n",
      "batch 5513: loss 0.055652\n",
      "batch 5514: loss 0.028785\n",
      "batch 5515: loss 0.043752\n",
      "batch 5516: loss 0.012005\n",
      "batch 5517: loss 0.009811\n",
      "batch 5518: loss 0.088119\n",
      "batch 5519: loss 0.016161\n",
      "batch 5520: loss 0.057568\n",
      "batch 5521: loss 0.094195\n",
      "batch 5522: loss 0.106947\n",
      "batch 5523: loss 0.035312\n",
      "batch 5524: loss 0.093355\n",
      "batch 5525: loss 0.034689\n",
      "batch 5526: loss 0.018761\n",
      "batch 5527: loss 0.095545\n",
      "batch 5528: loss 0.021953\n",
      "batch 5529: loss 0.068154\n",
      "batch 5530: loss 0.065114\n",
      "batch 5531: loss 0.025714\n",
      "batch 5532: loss 0.019102\n",
      "batch 5533: loss 0.024730\n",
      "batch 5534: loss 0.027511\n",
      "batch 5535: loss 0.045573\n",
      "batch 5536: loss 0.007577\n",
      "batch 5537: loss 0.202715\n",
      "batch 5538: loss 0.058829\n",
      "batch 5539: loss 0.022288\n",
      "batch 5540: loss 0.045509\n",
      "batch 5541: loss 0.068180\n",
      "batch 5542: loss 0.055856\n",
      "batch 5543: loss 0.018502\n",
      "batch 5544: loss 0.064221\n",
      "batch 5545: loss 0.103050\n",
      "batch 5546: loss 0.032147\n",
      "batch 5547: loss 0.069722\n",
      "batch 5548: loss 0.064226\n",
      "batch 5549: loss 0.043616\n",
      "batch 5550: loss 0.140020\n",
      "batch 5551: loss 0.019495\n",
      "batch 5552: loss 0.075547\n",
      "batch 5553: loss 0.129676\n",
      "batch 5554: loss 0.052235\n",
      "batch 5555: loss 0.048198\n",
      "batch 5556: loss 0.034059\n",
      "batch 5557: loss 0.008418\n",
      "batch 5558: loss 0.056297\n",
      "batch 5559: loss 0.086958\n",
      "batch 5560: loss 0.025032\n",
      "batch 5561: loss 0.095389\n",
      "batch 5562: loss 0.037378\n",
      "batch 5563: loss 0.040205\n",
      "batch 5564: loss 0.054358\n",
      "batch 5565: loss 0.029006\n",
      "batch 5566: loss 0.039391\n",
      "batch 5567: loss 0.005473\n",
      "batch 5568: loss 0.204965\n",
      "batch 5569: loss 0.033203\n",
      "batch 5570: loss 0.022756\n",
      "batch 5571: loss 0.036218\n",
      "batch 5572: loss 0.028098\n",
      "batch 5573: loss 0.014755\n",
      "batch 5574: loss 0.109778\n",
      "batch 5575: loss 0.013487\n",
      "batch 5576: loss 0.116369\n",
      "batch 5577: loss 0.010164\n",
      "batch 5578: loss 0.040628\n",
      "batch 5579: loss 0.029125\n",
      "batch 5580: loss 0.016778\n",
      "batch 5581: loss 0.019357\n",
      "batch 5582: loss 0.014557\n",
      "batch 5583: loss 0.116294\n",
      "batch 5584: loss 0.082522\n",
      "batch 5585: loss 0.085580\n",
      "batch 5586: loss 0.050199\n",
      "batch 5587: loss 0.011727\n",
      "batch 5588: loss 0.029191\n",
      "batch 5589: loss 0.004832\n",
      "batch 5590: loss 0.193292\n",
      "batch 5591: loss 0.020801\n",
      "batch 5592: loss 0.026551\n",
      "batch 5593: loss 0.041496\n",
      "batch 5594: loss 0.057232\n",
      "batch 5595: loss 0.026612\n",
      "batch 5596: loss 0.008795\n",
      "batch 5597: loss 0.025871\n",
      "batch 5598: loss 0.012094\n",
      "batch 5599: loss 0.042898\n",
      "batch 5600: loss 0.051586\n",
      "batch 5601: loss 0.016570\n",
      "batch 5602: loss 0.158041\n",
      "batch 5603: loss 0.019042\n",
      "batch 5604: loss 0.006693\n",
      "batch 5605: loss 0.031540\n",
      "batch 5606: loss 0.035912\n",
      "batch 5607: loss 0.032087\n",
      "batch 5608: loss 0.003660\n",
      "batch 5609: loss 0.054312\n",
      "batch 5610: loss 0.019152\n",
      "batch 5611: loss 0.038723\n",
      "batch 5612: loss 0.139717\n",
      "batch 5613: loss 0.191149\n",
      "batch 5614: loss 0.048185\n",
      "batch 5615: loss 0.015423\n",
      "batch 5616: loss 0.116762\n",
      "batch 5617: loss 0.017576\n",
      "batch 5618: loss 0.080453\n",
      "batch 5619: loss 0.057124\n",
      "batch 5620: loss 0.068610\n",
      "batch 5621: loss 0.100568\n",
      "batch 5622: loss 0.026789\n",
      "batch 5623: loss 0.008287\n",
      "batch 5624: loss 0.013844\n",
      "batch 5625: loss 0.052790\n",
      "batch 5626: loss 0.021462\n",
      "batch 5627: loss 0.021477\n",
      "batch 5628: loss 0.048060\n",
      "batch 5629: loss 0.141648\n",
      "batch 5630: loss 0.087855\n",
      "batch 5631: loss 0.032662\n",
      "batch 5632: loss 0.004743\n",
      "batch 5633: loss 0.029096\n",
      "batch 5634: loss 0.069555\n",
      "batch 5635: loss 0.043044\n",
      "batch 5636: loss 0.027955\n",
      "batch 5637: loss 0.090226\n",
      "batch 5638: loss 0.118091\n",
      "batch 5639: loss 0.111689\n",
      "batch 5640: loss 0.013409\n",
      "batch 5641: loss 0.015729\n",
      "batch 5642: loss 0.058716\n",
      "batch 5643: loss 0.054355\n",
      "batch 5644: loss 0.022611\n",
      "batch 5645: loss 0.023513\n",
      "batch 5646: loss 0.054104\n",
      "batch 5647: loss 0.064565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5648: loss 0.009918\n",
      "batch 5649: loss 0.088782\n",
      "batch 5650: loss 0.034077\n",
      "batch 5651: loss 0.029142\n",
      "batch 5652: loss 0.036339\n",
      "batch 5653: loss 0.183491\n",
      "batch 5654: loss 0.027499\n",
      "batch 5655: loss 0.084182\n",
      "batch 5656: loss 0.035210\n",
      "batch 5657: loss 0.025723\n",
      "batch 5658: loss 0.026662\n",
      "batch 5659: loss 0.010839\n",
      "batch 5660: loss 0.147627\n",
      "batch 5661: loss 0.130856\n",
      "batch 5662: loss 0.049302\n",
      "batch 5663: loss 0.148680\n",
      "batch 5664: loss 0.029604\n",
      "batch 5665: loss 0.012229\n",
      "batch 5666: loss 0.046049\n",
      "batch 5667: loss 0.038164\n",
      "batch 5668: loss 0.100307\n",
      "batch 5669: loss 0.051232\n",
      "batch 5670: loss 0.003519\n",
      "batch 5671: loss 0.042456\n",
      "batch 5672: loss 0.024043\n",
      "batch 5673: loss 0.026665\n",
      "batch 5674: loss 0.069380\n",
      "batch 5675: loss 0.032642\n",
      "batch 5676: loss 0.005717\n",
      "batch 5677: loss 0.030245\n",
      "batch 5678: loss 0.043042\n",
      "batch 5679: loss 0.047187\n",
      "batch 5680: loss 0.123730\n",
      "batch 5681: loss 0.027349\n",
      "batch 5682: loss 0.067337\n",
      "batch 5683: loss 0.007291\n",
      "batch 5684: loss 0.093999\n",
      "batch 5685: loss 0.027872\n",
      "batch 5686: loss 0.058148\n",
      "batch 5687: loss 0.018410\n",
      "batch 5688: loss 0.032645\n",
      "batch 5689: loss 0.035824\n",
      "batch 5690: loss 0.091978\n",
      "batch 5691: loss 0.090599\n",
      "batch 5692: loss 0.095363\n",
      "batch 5693: loss 0.062721\n",
      "batch 5694: loss 0.039306\n",
      "batch 5695: loss 0.014919\n",
      "batch 5696: loss 0.051419\n",
      "batch 5697: loss 0.071180\n",
      "batch 5698: loss 0.035524\n",
      "batch 5699: loss 0.022592\n",
      "batch 5700: loss 0.067244\n",
      "batch 5701: loss 0.050713\n",
      "batch 5702: loss 0.153113\n",
      "batch 5703: loss 0.049265\n",
      "batch 5704: loss 0.051109\n",
      "batch 5705: loss 0.023927\n",
      "batch 5706: loss 0.120406\n",
      "batch 5707: loss 0.124765\n",
      "batch 5708: loss 0.015456\n",
      "batch 5709: loss 0.047415\n",
      "batch 5710: loss 0.013257\n",
      "batch 5711: loss 0.117885\n",
      "batch 5712: loss 0.067633\n",
      "batch 5713: loss 0.088123\n",
      "batch 5714: loss 0.101232\n",
      "batch 5715: loss 0.038356\n",
      "batch 5716: loss 0.020838\n",
      "batch 5717: loss 0.008797\n",
      "batch 5718: loss 0.023696\n",
      "batch 5719: loss 0.021201\n",
      "batch 5720: loss 0.114966\n",
      "batch 5721: loss 0.030090\n",
      "batch 5722: loss 0.150288\n",
      "batch 5723: loss 0.216929\n",
      "batch 5724: loss 0.064050\n",
      "batch 5725: loss 0.016126\n",
      "batch 5726: loss 0.039288\n",
      "batch 5727: loss 0.055343\n",
      "batch 5728: loss 0.166659\n",
      "batch 5729: loss 0.051240\n",
      "batch 5730: loss 0.028675\n",
      "batch 5731: loss 0.119719\n",
      "batch 5732: loss 0.035240\n",
      "batch 5733: loss 0.090885\n",
      "batch 5734: loss 0.025697\n",
      "batch 5735: loss 0.061212\n",
      "batch 5736: loss 0.020272\n",
      "batch 5737: loss 0.107214\n",
      "batch 5738: loss 0.024699\n",
      "batch 5739: loss 0.138007\n",
      "batch 5740: loss 0.060414\n",
      "batch 5741: loss 0.017750\n",
      "batch 5742: loss 0.024477\n",
      "batch 5743: loss 0.026247\n",
      "batch 5744: loss 0.035204\n",
      "batch 5745: loss 0.018590\n",
      "batch 5746: loss 0.012846\n",
      "batch 5747: loss 0.017661\n",
      "batch 5748: loss 0.064227\n",
      "batch 5749: loss 0.017311\n",
      "batch 5750: loss 0.186542\n",
      "batch 5751: loss 0.010060\n",
      "batch 5752: loss 0.056210\n",
      "batch 5753: loss 0.090015\n",
      "batch 5754: loss 0.071392\n",
      "batch 5755: loss 0.029701\n",
      "batch 5756: loss 0.053848\n",
      "batch 5757: loss 0.046888\n",
      "batch 5758: loss 0.124490\n",
      "batch 5759: loss 0.143463\n",
      "batch 5760: loss 0.016397\n",
      "batch 5761: loss 0.072037\n",
      "batch 5762: loss 0.033656\n",
      "batch 5763: loss 0.012294\n",
      "batch 5764: loss 0.030777\n",
      "batch 5765: loss 0.185312\n",
      "batch 5766: loss 0.242516\n",
      "batch 5767: loss 0.143261\n",
      "batch 5768: loss 0.106231\n",
      "batch 5769: loss 0.036856\n",
      "batch 5770: loss 0.071582\n",
      "batch 5771: loss 0.084560\n",
      "batch 5772: loss 0.127048\n",
      "batch 5773: loss 0.104073\n",
      "batch 5774: loss 0.042656\n",
      "batch 5775: loss 0.149454\n",
      "batch 5776: loss 0.004540\n",
      "batch 5777: loss 0.026710\n",
      "batch 5778: loss 0.055325\n",
      "batch 5779: loss 0.191705\n",
      "batch 5780: loss 0.015133\n",
      "batch 5781: loss 0.012662\n",
      "batch 5782: loss 0.009269\n",
      "batch 5783: loss 0.008165\n",
      "batch 5784: loss 0.137277\n",
      "batch 5785: loss 0.032952\n",
      "batch 5786: loss 0.091218\n",
      "batch 5787: loss 0.043041\n",
      "batch 5788: loss 0.043544\n",
      "batch 5789: loss 0.017649\n",
      "batch 5790: loss 0.110091\n",
      "batch 5791: loss 0.066047\n",
      "batch 5792: loss 0.020683\n",
      "batch 5793: loss 0.122539\n",
      "batch 5794: loss 0.034919\n",
      "batch 5795: loss 0.041942\n",
      "batch 5796: loss 0.032657\n",
      "batch 5797: loss 0.019055\n",
      "batch 5798: loss 0.065785\n",
      "batch 5799: loss 0.068403\n",
      "batch 5800: loss 0.016949\n",
      "batch 5801: loss 0.108126\n",
      "batch 5802: loss 0.084189\n",
      "batch 5803: loss 0.060380\n",
      "batch 5804: loss 0.009656\n",
      "batch 5805: loss 0.036846\n",
      "batch 5806: loss 0.007031\n",
      "batch 5807: loss 0.006459\n",
      "batch 5808: loss 0.013398\n",
      "batch 5809: loss 0.021636\n",
      "batch 5810: loss 0.012409\n",
      "batch 5811: loss 0.106109\n",
      "batch 5812: loss 0.012412\n",
      "batch 5813: loss 0.052331\n",
      "batch 5814: loss 0.124620\n",
      "batch 5815: loss 0.054492\n",
      "batch 5816: loss 0.018501\n",
      "batch 5817: loss 0.030870\n",
      "batch 5818: loss 0.045943\n",
      "batch 5819: loss 0.012743\n",
      "batch 5820: loss 0.027557\n",
      "batch 5821: loss 0.037075\n",
      "batch 5822: loss 0.015534\n",
      "batch 5823: loss 0.024179\n",
      "batch 5824: loss 0.018539\n",
      "batch 5825: loss 0.028588\n",
      "batch 5826: loss 0.019711\n",
      "batch 5827: loss 0.081697\n",
      "batch 5828: loss 0.045867\n",
      "batch 5829: loss 0.053602\n",
      "batch 5830: loss 0.041622\n",
      "batch 5831: loss 0.016027\n",
      "batch 5832: loss 0.137715\n",
      "batch 5833: loss 0.082012\n",
      "batch 5834: loss 0.026077\n",
      "batch 5835: loss 0.016776\n",
      "batch 5836: loss 0.014641\n",
      "batch 5837: loss 0.020380\n",
      "batch 5838: loss 0.052315\n",
      "batch 5839: loss 0.016134\n",
      "batch 5840: loss 0.010357\n",
      "batch 5841: loss 0.064538\n",
      "batch 5842: loss 0.047212\n",
      "batch 5843: loss 0.112608\n",
      "batch 5844: loss 0.065336\n",
      "batch 5845: loss 0.048550\n",
      "batch 5846: loss 0.055853\n",
      "batch 5847: loss 0.087100\n",
      "batch 5848: loss 0.029171\n",
      "batch 5849: loss 0.016250\n",
      "batch 5850: loss 0.036317\n",
      "batch 5851: loss 0.017222\n",
      "batch 5852: loss 0.033904\n",
      "batch 5853: loss 0.028238\n",
      "batch 5854: loss 0.016841\n",
      "batch 5855: loss 0.051317\n",
      "batch 5856: loss 0.039284\n",
      "batch 5857: loss 0.058144\n",
      "batch 5858: loss 0.014198\n",
      "batch 5859: loss 0.039811\n",
      "batch 5860: loss 0.021261\n",
      "batch 5861: loss 0.067217\n",
      "batch 5862: loss 0.035868\n",
      "batch 5863: loss 0.109727\n",
      "batch 5864: loss 0.008233\n",
      "batch 5865: loss 0.034328\n",
      "batch 5866: loss 0.079976\n",
      "batch 5867: loss 0.016140\n",
      "batch 5868: loss 0.077197\n",
      "batch 5869: loss 0.054805\n",
      "batch 5870: loss 0.008391\n",
      "batch 5871: loss 0.040654\n",
      "batch 5872: loss 0.062204\n",
      "batch 5873: loss 0.021069\n",
      "batch 5874: loss 0.098528\n",
      "batch 5875: loss 0.029732\n",
      "batch 5876: loss 0.010232\n",
      "batch 5877: loss 0.031117\n",
      "batch 5878: loss 0.166823\n",
      "batch 5879: loss 0.037651\n",
      "batch 5880: loss 0.088730\n",
      "batch 5881: loss 0.042438\n",
      "batch 5882: loss 0.191160\n",
      "batch 5883: loss 0.009158\n",
      "batch 5884: loss 0.060475\n",
      "batch 5885: loss 0.165634\n",
      "batch 5886: loss 0.033020\n",
      "batch 5887: loss 0.088808\n",
      "batch 5888: loss 0.089622\n",
      "batch 5889: loss 0.133993\n",
      "batch 5890: loss 0.015585\n",
      "batch 5891: loss 0.092972\n",
      "batch 5892: loss 0.105920\n",
      "batch 5893: loss 0.030081\n",
      "batch 5894: loss 0.104753\n",
      "batch 5895: loss 0.179030\n",
      "batch 5896: loss 0.081270\n",
      "batch 5897: loss 0.049369\n",
      "batch 5898: loss 0.018653\n",
      "batch 5899: loss 0.026720\n",
      "batch 5900: loss 0.043977\n",
      "batch 5901: loss 0.017989\n",
      "batch 5902: loss 0.012745\n",
      "batch 5903: loss 0.163658\n",
      "batch 5904: loss 0.027406\n",
      "batch 5905: loss 0.007651\n",
      "batch 5906: loss 0.085462\n",
      "batch 5907: loss 0.062938\n",
      "batch 5908: loss 0.021545\n",
      "batch 5909: loss 0.013946\n",
      "batch 5910: loss 0.013577\n",
      "batch 5911: loss 0.012151\n",
      "batch 5912: loss 0.013342\n",
      "batch 5913: loss 0.011368\n",
      "batch 5914: loss 0.062211\n",
      "batch 5915: loss 0.073616\n",
      "batch 5916: loss 0.025495\n",
      "batch 5917: loss 0.022975\n",
      "batch 5918: loss 0.008108\n",
      "batch 5919: loss 0.090483\n",
      "batch 5920: loss 0.043595\n",
      "batch 5921: loss 0.012280\n",
      "batch 5922: loss 0.086222\n",
      "batch 5923: loss 0.019116\n",
      "batch 5924: loss 0.020898\n",
      "batch 5925: loss 0.174966\n",
      "batch 5926: loss 0.017554\n",
      "batch 5927: loss 0.009859\n",
      "batch 5928: loss 0.039829\n",
      "batch 5929: loss 0.164779\n",
      "batch 5930: loss 0.021599\n",
      "batch 5931: loss 0.015479\n",
      "batch 5932: loss 0.014061\n",
      "batch 5933: loss 0.032675\n",
      "batch 5934: loss 0.113364\n",
      "batch 5935: loss 0.009381\n",
      "batch 5936: loss 0.026665\n",
      "batch 5937: loss 0.029556\n",
      "batch 5938: loss 0.038246\n",
      "batch 5939: loss 0.089108\n",
      "batch 5940: loss 0.008727\n",
      "batch 5941: loss 0.025650\n",
      "batch 5942: loss 0.021977\n",
      "batch 5943: loss 0.020563\n",
      "batch 5944: loss 0.044699\n",
      "batch 5945: loss 0.041401\n",
      "batch 5946: loss 0.057041\n",
      "batch 5947: loss 0.018268\n",
      "batch 5948: loss 0.014021\n",
      "batch 5949: loss 0.018639\n",
      "batch 5950: loss 0.085174\n",
      "batch 5951: loss 0.019743\n",
      "batch 5952: loss 0.037309\n",
      "batch 5953: loss 0.043987\n",
      "batch 5954: loss 0.156617\n",
      "batch 5955: loss 0.078415\n",
      "batch 5956: loss 0.057314\n",
      "batch 5957: loss 0.104195\n",
      "batch 5958: loss 0.094748\n",
      "batch 5959: loss 0.033152\n",
      "batch 5960: loss 0.026770\n",
      "batch 5961: loss 0.160827\n",
      "batch 5962: loss 0.027025\n",
      "batch 5963: loss 0.095579\n",
      "batch 5964: loss 0.012130\n",
      "batch 5965: loss 0.080788\n",
      "batch 5966: loss 0.137910\n",
      "batch 5967: loss 0.027232\n",
      "batch 5968: loss 0.064010\n",
      "batch 5969: loss 0.023731\n",
      "batch 5970: loss 0.143379\n",
      "batch 5971: loss 0.011615\n",
      "batch 5972: loss 0.020257\n",
      "batch 5973: loss 0.016121\n",
      "batch 5974: loss 0.027232\n",
      "batch 5975: loss 0.070739\n",
      "batch 5976: loss 0.027127\n",
      "batch 5977: loss 0.062935\n",
      "batch 5978: loss 0.026244\n",
      "batch 5979: loss 0.038064\n",
      "batch 5980: loss 0.033901\n",
      "batch 5981: loss 0.070151\n",
      "batch 5982: loss 0.010500\n",
      "batch 5983: loss 0.058256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5984: loss 0.023469\n",
      "batch 5985: loss 0.037589\n",
      "batch 5986: loss 0.065944\n",
      "batch 5987: loss 0.017074\n",
      "batch 5988: loss 0.007879\n",
      "batch 5989: loss 0.113597\n",
      "batch 5990: loss 0.045278\n",
      "batch 5991: loss 0.035593\n",
      "batch 5992: loss 0.010741\n",
      "batch 5993: loss 0.009367\n",
      "batch 5994: loss 0.009336\n",
      "batch 5995: loss 0.054289\n",
      "batch 5996: loss 0.076576\n",
      "batch 5997: loss 0.081481\n",
      "batch 5998: loss 0.162793\n",
      "batch 5999: loss 0.069778\n",
      "test accuracy: 0.972900\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    \n",
    "\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 調整批次大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.268107\n",
      "batch 1: loss 2.369123\n",
      "batch 2: loss 2.184296\n",
      "batch 3: loss 2.153343\n",
      "batch 4: loss 2.027103\n",
      "batch 5: loss 2.003873\n",
      "batch 6: loss 1.883962\n",
      "batch 7: loss 1.824250\n",
      "batch 8: loss 1.806315\n",
      "batch 9: loss 1.769227\n",
      "batch 10: loss 1.661200\n",
      "batch 11: loss 1.721964\n",
      "batch 12: loss 1.586843\n",
      "batch 13: loss 1.546063\n",
      "batch 14: loss 1.506375\n",
      "batch 15: loss 1.403386\n",
      "batch 16: loss 1.340148\n",
      "batch 17: loss 1.281515\n",
      "batch 18: loss 1.244667\n",
      "batch 19: loss 1.270779\n",
      "batch 20: loss 1.148414\n",
      "batch 21: loss 1.133960\n",
      "batch 22: loss 1.141349\n",
      "batch 23: loss 1.141044\n",
      "batch 24: loss 0.979978\n",
      "batch 25: loss 0.968790\n",
      "batch 26: loss 0.891630\n",
      "batch 27: loss 0.966170\n",
      "batch 28: loss 0.796832\n",
      "batch 29: loss 0.880812\n",
      "batch 30: loss 0.891926\n",
      "batch 31: loss 0.686706\n",
      "batch 32: loss 0.702578\n",
      "batch 33: loss 0.797099\n",
      "batch 34: loss 0.854170\n",
      "batch 35: loss 0.737359\n",
      "batch 36: loss 0.771933\n",
      "batch 37: loss 0.741399\n",
      "batch 38: loss 0.651643\n",
      "batch 39: loss 0.708703\n",
      "batch 40: loss 0.694723\n",
      "batch 41: loss 0.578891\n",
      "batch 42: loss 0.579990\n",
      "batch 43: loss 0.583234\n",
      "batch 44: loss 0.639703\n",
      "batch 45: loss 0.623184\n",
      "batch 46: loss 0.578778\n",
      "batch 47: loss 0.545482\n",
      "batch 48: loss 0.577844\n",
      "batch 49: loss 0.568471\n",
      "batch 50: loss 0.534416\n",
      "batch 51: loss 0.571858\n",
      "batch 52: loss 0.522665\n",
      "batch 53: loss 0.530052\n",
      "batch 54: loss 0.622369\n",
      "batch 55: loss 0.539306\n",
      "batch 56: loss 0.397121\n",
      "batch 57: loss 0.738412\n",
      "batch 58: loss 0.495598\n",
      "batch 59: loss 0.443894\n",
      "batch 60: loss 0.552335\n",
      "batch 61: loss 0.394213\n",
      "batch 62: loss 0.452011\n",
      "batch 63: loss 0.435043\n",
      "batch 64: loss 0.557246\n",
      "batch 65: loss 0.323589\n",
      "batch 66: loss 0.405214\n",
      "batch 67: loss 0.458992\n",
      "batch 68: loss 0.440449\n",
      "batch 69: loss 0.548343\n",
      "batch 70: loss 0.420533\n",
      "batch 71: loss 0.405642\n",
      "batch 72: loss 0.427746\n",
      "batch 73: loss 0.541850\n",
      "batch 74: loss 0.435061\n",
      "batch 75: loss 0.633670\n",
      "batch 76: loss 0.414887\n",
      "batch 77: loss 0.389191\n",
      "batch 78: loss 0.405954\n",
      "batch 79: loss 0.401049\n",
      "batch 80: loss 0.398359\n",
      "batch 81: loss 0.400089\n",
      "batch 82: loss 0.463184\n",
      "batch 83: loss 0.370894\n",
      "batch 84: loss 0.406385\n",
      "batch 85: loss 0.399697\n",
      "batch 86: loss 0.372413\n",
      "batch 87: loss 0.489226\n",
      "batch 88: loss 0.430209\n",
      "batch 89: loss 0.461774\n",
      "batch 90: loss 0.316797\n",
      "batch 91: loss 0.265388\n",
      "batch 92: loss 0.326901\n",
      "batch 93: loss 0.374402\n",
      "batch 94: loss 0.334276\n",
      "batch 95: loss 0.351200\n",
      "batch 96: loss 0.468232\n",
      "batch 97: loss 0.349225\n",
      "batch 98: loss 0.368990\n",
      "batch 99: loss 0.381194\n",
      "batch 100: loss 0.365071\n",
      "batch 101: loss 0.494304\n",
      "batch 102: loss 0.348399\n",
      "batch 103: loss 0.337384\n",
      "batch 104: loss 0.318143\n",
      "batch 105: loss 0.327907\n",
      "batch 106: loss 0.497445\n",
      "batch 107: loss 0.440492\n",
      "batch 108: loss 0.264233\n",
      "batch 109: loss 0.281979\n",
      "batch 110: loss 0.352132\n",
      "batch 111: loss 0.287726\n",
      "batch 112: loss 0.458846\n",
      "batch 113: loss 0.378040\n",
      "batch 114: loss 0.390763\n",
      "batch 115: loss 0.292728\n",
      "batch 116: loss 0.294369\n",
      "batch 117: loss 0.414708\n",
      "batch 118: loss 0.273680\n",
      "batch 119: loss 0.471072\n",
      "batch 120: loss 0.376314\n",
      "batch 121: loss 0.296215\n",
      "batch 122: loss 0.234875\n",
      "batch 123: loss 0.350146\n",
      "batch 124: loss 0.355108\n",
      "batch 125: loss 0.270591\n",
      "batch 126: loss 0.362443\n",
      "batch 127: loss 0.288315\n",
      "batch 128: loss 0.358876\n",
      "batch 129: loss 0.437101\n",
      "batch 130: loss 0.343761\n",
      "batch 131: loss 0.388722\n",
      "batch 132: loss 0.277293\n",
      "batch 133: loss 0.277333\n",
      "batch 134: loss 0.346486\n",
      "batch 135: loss 0.321141\n",
      "batch 136: loss 0.341740\n",
      "batch 137: loss 0.254255\n",
      "batch 138: loss 0.276872\n",
      "batch 139: loss 0.341631\n",
      "batch 140: loss 0.339371\n",
      "batch 141: loss 0.267121\n",
      "batch 142: loss 0.437059\n",
      "batch 143: loss 0.520707\n",
      "batch 144: loss 0.254762\n",
      "batch 145: loss 0.284405\n",
      "batch 146: loss 0.271286\n",
      "batch 147: loss 0.336865\n",
      "batch 148: loss 0.262718\n",
      "batch 149: loss 0.448935\n",
      "batch 150: loss 0.421954\n",
      "batch 151: loss 0.307021\n",
      "batch 152: loss 0.264215\n",
      "batch 153: loss 0.407277\n",
      "batch 154: loss 0.377681\n",
      "batch 155: loss 0.403057\n",
      "batch 156: loss 0.377434\n",
      "batch 157: loss 0.261584\n",
      "batch 158: loss 0.359689\n",
      "batch 159: loss 0.399116\n",
      "batch 160: loss 0.326551\n",
      "batch 161: loss 0.445051\n",
      "batch 162: loss 0.229964\n",
      "batch 163: loss 0.396038\n",
      "batch 164: loss 0.588507\n",
      "batch 165: loss 0.349959\n",
      "batch 166: loss 0.269634\n",
      "batch 167: loss 0.482437\n",
      "batch 168: loss 0.268848\n",
      "batch 169: loss 0.273360\n",
      "batch 170: loss 0.253422\n",
      "batch 171: loss 0.421551\n",
      "batch 172: loss 0.274098\n",
      "batch 173: loss 0.328259\n",
      "batch 174: loss 0.325175\n",
      "batch 175: loss 0.348454\n",
      "batch 176: loss 0.278880\n",
      "batch 177: loss 0.265869\n",
      "batch 178: loss 0.366828\n",
      "batch 179: loss 0.241347\n",
      "batch 180: loss 0.311791\n",
      "batch 181: loss 0.318156\n",
      "batch 182: loss 0.264240\n",
      "batch 183: loss 0.390712\n",
      "batch 184: loss 0.401014\n",
      "batch 185: loss 0.412159\n",
      "batch 186: loss 0.426185\n",
      "batch 187: loss 0.212833\n",
      "batch 188: loss 0.238377\n",
      "batch 189: loss 0.296635\n",
      "batch 190: loss 0.333463\n",
      "batch 191: loss 0.325380\n",
      "batch 192: loss 0.254604\n",
      "batch 193: loss 0.332688\n",
      "batch 194: loss 0.520181\n",
      "batch 195: loss 0.274571\n",
      "batch 196: loss 0.305790\n",
      "batch 197: loss 0.232335\n",
      "batch 198: loss 0.243472\n",
      "batch 199: loss 0.312736\n",
      "batch 200: loss 0.282870\n",
      "batch 201: loss 0.324813\n",
      "batch 202: loss 0.328329\n",
      "batch 203: loss 0.168579\n",
      "batch 204: loss 0.294711\n",
      "batch 205: loss 0.253563\n",
      "batch 206: loss 0.253987\n",
      "batch 207: loss 0.372130\n",
      "batch 208: loss 0.279959\n",
      "batch 209: loss 0.208238\n",
      "batch 210: loss 0.213729\n",
      "batch 211: loss 0.339692\n",
      "batch 212: loss 0.296846\n",
      "batch 213: loss 0.284377\n",
      "batch 214: loss 0.223294\n",
      "batch 215: loss 0.389061\n",
      "batch 216: loss 0.324054\n",
      "batch 217: loss 0.244166\n",
      "batch 218: loss 0.216645\n",
      "batch 219: loss 0.293606\n",
      "batch 220: loss 0.292500\n",
      "batch 221: loss 0.261335\n",
      "batch 222: loss 0.321299\n",
      "batch 223: loss 0.173153\n",
      "batch 224: loss 0.449659\n",
      "batch 225: loss 0.230928\n",
      "batch 226: loss 0.295520\n",
      "batch 227: loss 0.213676\n",
      "batch 228: loss 0.255762\n",
      "batch 229: loss 0.278225\n",
      "batch 230: loss 0.348202\n",
      "batch 231: loss 0.244809\n",
      "batch 232: loss 0.287826\n",
      "batch 233: loss 0.192989\n",
      "batch 234: loss 0.255656\n",
      "batch 235: loss 0.359685\n",
      "batch 236: loss 0.228712\n",
      "batch 237: loss 0.316008\n",
      "batch 238: loss 0.334005\n",
      "batch 239: loss 0.341564\n",
      "batch 240: loss 0.379577\n",
      "batch 241: loss 0.323664\n",
      "batch 242: loss 0.225037\n",
      "batch 243: loss 0.312463\n",
      "batch 244: loss 0.185477\n",
      "batch 245: loss 0.261553\n",
      "batch 246: loss 0.257611\n",
      "batch 247: loss 0.200822\n",
      "batch 248: loss 0.216570\n",
      "batch 249: loss 0.160216\n",
      "batch 250: loss 0.273121\n",
      "batch 251: loss 0.277163\n",
      "batch 252: loss 0.448665\n",
      "batch 253: loss 0.222660\n",
      "batch 254: loss 0.230152\n",
      "batch 255: loss 0.242100\n",
      "batch 256: loss 0.225227\n",
      "batch 257: loss 0.344933\n",
      "batch 258: loss 0.275304\n",
      "batch 259: loss 0.230419\n",
      "batch 260: loss 0.239182\n",
      "batch 261: loss 0.253017\n",
      "batch 262: loss 0.310436\n",
      "batch 263: loss 0.187746\n",
      "batch 264: loss 0.235893\n",
      "batch 265: loss 0.283452\n",
      "batch 266: loss 0.223783\n",
      "batch 267: loss 0.275304\n",
      "batch 268: loss 0.198760\n",
      "batch 269: loss 0.343807\n",
      "batch 270: loss 0.191961\n",
      "batch 271: loss 0.335367\n",
      "batch 272: loss 0.258795\n",
      "batch 273: loss 0.176772\n",
      "batch 274: loss 0.240961\n",
      "batch 275: loss 0.346423\n",
      "batch 276: loss 0.369719\n",
      "batch 277: loss 0.134007\n",
      "batch 278: loss 0.302275\n",
      "batch 279: loss 0.379711\n",
      "batch 280: loss 0.194696\n",
      "batch 281: loss 0.302246\n",
      "batch 282: loss 0.180709\n",
      "batch 283: loss 0.208525\n",
      "batch 284: loss 0.167529\n",
      "batch 285: loss 0.183131\n",
      "batch 286: loss 0.285547\n",
      "batch 287: loss 0.230493\n",
      "batch 288: loss 0.322075\n",
      "batch 289: loss 0.321023\n",
      "batch 290: loss 0.272321\n",
      "batch 291: loss 0.229461\n",
      "batch 292: loss 0.183797\n",
      "batch 293: loss 0.184381\n",
      "batch 294: loss 0.367610\n",
      "batch 295: loss 0.339770\n",
      "batch 296: loss 0.316274\n",
      "batch 297: loss 0.202632\n",
      "batch 298: loss 0.354018\n",
      "batch 299: loss 0.340155\n",
      "batch 300: loss 0.265556\n",
      "batch 301: loss 0.238472\n",
      "batch 302: loss 0.277727\n",
      "batch 303: loss 0.155839\n",
      "batch 304: loss 0.285357\n",
      "batch 305: loss 0.175558\n",
      "batch 306: loss 0.257247\n",
      "batch 307: loss 0.327842\n",
      "batch 308: loss 0.265437\n",
      "batch 309: loss 0.323462\n",
      "batch 310: loss 0.134806\n",
      "batch 311: loss 0.218136\n",
      "batch 312: loss 0.314850\n",
      "batch 313: loss 0.239911\n",
      "batch 314: loss 0.315979\n",
      "batch 315: loss 0.262526\n",
      "batch 316: loss 0.201737\n",
      "batch 317: loss 0.137027\n",
      "batch 318: loss 0.144611\n",
      "batch 319: loss 0.282101\n",
      "batch 320: loss 0.166426\n",
      "batch 321: loss 0.328472\n",
      "batch 322: loss 0.259672\n",
      "batch 323: loss 0.243310\n",
      "batch 324: loss 0.139496\n",
      "batch 325: loss 0.156787\n",
      "batch 326: loss 0.238483\n",
      "batch 327: loss 0.227721\n",
      "batch 328: loss 0.227196\n",
      "batch 329: loss 0.179134\n",
      "batch 330: loss 0.167568\n",
      "batch 331: loss 0.201818\n",
      "batch 332: loss 0.241414\n",
      "batch 333: loss 0.188342\n",
      "batch 334: loss 0.296451\n",
      "batch 335: loss 0.304586\n",
      "batch 336: loss 0.246364\n",
      "batch 337: loss 0.250254\n",
      "batch 338: loss 0.182578\n",
      "batch 339: loss 0.184372\n",
      "batch 340: loss 0.155847\n",
      "batch 341: loss 0.217164\n",
      "batch 342: loss 0.209119\n",
      "batch 343: loss 0.205021\n",
      "batch 344: loss 0.262251\n",
      "batch 345: loss 0.303025\n",
      "batch 346: loss 0.106902\n",
      "batch 347: loss 0.396477\n",
      "batch 348: loss 0.289490\n",
      "batch 349: loss 0.226247\n",
      "batch 350: loss 0.146612\n",
      "batch 351: loss 0.338801\n",
      "batch 352: loss 0.239871\n",
      "batch 353: loss 0.175627\n",
      "batch 354: loss 0.216254\n",
      "batch 355: loss 0.278294\n",
      "batch 356: loss 0.179405\n",
      "batch 357: loss 0.155220\n",
      "batch 358: loss 0.238852\n",
      "batch 359: loss 0.372919\n",
      "batch 360: loss 0.295434\n",
      "batch 361: loss 0.174137\n",
      "batch 362: loss 0.276132\n",
      "batch 363: loss 0.180073\n",
      "batch 364: loss 0.221738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 365: loss 0.219654\n",
      "batch 366: loss 0.189133\n",
      "batch 367: loss 0.199331\n",
      "batch 368: loss 0.246833\n",
      "batch 369: loss 0.165083\n",
      "batch 370: loss 0.232165\n",
      "batch 371: loss 0.233712\n",
      "batch 372: loss 0.179789\n",
      "batch 373: loss 0.212283\n",
      "batch 374: loss 0.253342\n",
      "batch 375: loss 0.230164\n",
      "batch 376: loss 0.223392\n",
      "batch 377: loss 0.220821\n",
      "batch 378: loss 0.161029\n",
      "batch 379: loss 0.171070\n",
      "batch 380: loss 0.258386\n",
      "batch 381: loss 0.144237\n",
      "batch 382: loss 0.225907\n",
      "batch 383: loss 0.288229\n",
      "batch 384: loss 0.312895\n",
      "batch 385: loss 0.184389\n",
      "batch 386: loss 0.165485\n",
      "batch 387: loss 0.180737\n",
      "batch 388: loss 0.169027\n",
      "batch 389: loss 0.215756\n",
      "batch 390: loss 0.158001\n",
      "batch 391: loss 0.299298\n",
      "batch 392: loss 0.387830\n",
      "batch 393: loss 0.169279\n",
      "batch 394: loss 0.222340\n",
      "batch 395: loss 0.279988\n",
      "batch 396: loss 0.173269\n",
      "batch 397: loss 0.278304\n",
      "batch 398: loss 0.278237\n",
      "batch 399: loss 0.318877\n",
      "batch 400: loss 0.233154\n",
      "batch 401: loss 0.156342\n",
      "batch 402: loss 0.207866\n",
      "batch 403: loss 0.210797\n",
      "batch 404: loss 0.311273\n",
      "batch 405: loss 0.309402\n",
      "batch 406: loss 0.313557\n",
      "batch 407: loss 0.183927\n",
      "batch 408: loss 0.192827\n",
      "batch 409: loss 0.235694\n",
      "batch 410: loss 0.246791\n",
      "batch 411: loss 0.202029\n",
      "batch 412: loss 0.110315\n",
      "batch 413: loss 0.161187\n",
      "batch 414: loss 0.220615\n",
      "batch 415: loss 0.225155\n",
      "batch 416: loss 0.293917\n",
      "batch 417: loss 0.299059\n",
      "batch 418: loss 0.174668\n",
      "batch 419: loss 0.183014\n",
      "batch 420: loss 0.196616\n",
      "batch 421: loss 0.207222\n",
      "batch 422: loss 0.173665\n",
      "batch 423: loss 0.145209\n",
      "batch 424: loss 0.244074\n",
      "batch 425: loss 0.238275\n",
      "batch 426: loss 0.175887\n",
      "batch 427: loss 0.231915\n",
      "batch 428: loss 0.136449\n",
      "batch 429: loss 0.153882\n",
      "batch 430: loss 0.264096\n",
      "batch 431: loss 0.327318\n",
      "batch 432: loss 0.218624\n",
      "batch 433: loss 0.119691\n",
      "batch 434: loss 0.147988\n",
      "batch 435: loss 0.130975\n",
      "batch 436: loss 0.150043\n",
      "batch 437: loss 0.285462\n",
      "batch 438: loss 0.135231\n",
      "batch 439: loss 0.126811\n",
      "batch 440: loss 0.137421\n",
      "batch 441: loss 0.197295\n",
      "batch 442: loss 0.195388\n",
      "batch 443: loss 0.271567\n",
      "batch 444: loss 0.155350\n",
      "batch 445: loss 0.388751\n",
      "batch 446: loss 0.190369\n",
      "batch 447: loss 0.169375\n",
      "batch 448: loss 0.246079\n",
      "batch 449: loss 0.213811\n",
      "batch 450: loss 0.184043\n",
      "batch 451: loss 0.200388\n",
      "batch 452: loss 0.147759\n",
      "batch 453: loss 0.177972\n",
      "batch 454: loss 0.181413\n",
      "batch 455: loss 0.248737\n",
      "batch 456: loss 0.323970\n",
      "batch 457: loss 0.130985\n",
      "batch 458: loss 0.259631\n",
      "batch 459: loss 0.241444\n",
      "batch 460: loss 0.196316\n",
      "batch 461: loss 0.179667\n",
      "batch 462: loss 0.334164\n",
      "batch 463: loss 0.275674\n",
      "batch 464: loss 0.246615\n",
      "batch 465: loss 0.219120\n",
      "batch 466: loss 0.182715\n",
      "batch 467: loss 0.208825\n",
      "batch 468: loss 0.202304\n",
      "batch 469: loss 0.154924\n",
      "batch 470: loss 0.147457\n",
      "batch 471: loss 0.221332\n",
      "batch 472: loss 0.131874\n",
      "batch 473: loss 0.233248\n",
      "batch 474: loss 0.148022\n",
      "batch 475: loss 0.170204\n",
      "batch 476: loss 0.238344\n",
      "batch 477: loss 0.280553\n",
      "batch 478: loss 0.305234\n",
      "batch 479: loss 0.167134\n",
      "batch 480: loss 0.129688\n",
      "batch 481: loss 0.134573\n",
      "batch 482: loss 0.129456\n",
      "batch 483: loss 0.214647\n",
      "batch 484: loss 0.150862\n",
      "batch 485: loss 0.161985\n",
      "batch 486: loss 0.170294\n",
      "batch 487: loss 0.268690\n",
      "batch 488: loss 0.291279\n",
      "batch 489: loss 0.143796\n",
      "batch 490: loss 0.189343\n",
      "batch 491: loss 0.219018\n",
      "batch 492: loss 0.268321\n",
      "batch 493: loss 0.302794\n",
      "batch 494: loss 0.229961\n",
      "batch 495: loss 0.156650\n",
      "batch 496: loss 0.229621\n",
      "batch 497: loss 0.138896\n",
      "batch 498: loss 0.140222\n",
      "batch 499: loss 0.236964\n",
      "batch 500: loss 0.175924\n",
      "batch 501: loss 0.212363\n",
      "batch 502: loss 0.174080\n",
      "batch 503: loss 0.279945\n",
      "batch 504: loss 0.166757\n",
      "batch 505: loss 0.190825\n",
      "batch 506: loss 0.235702\n",
      "batch 507: loss 0.216857\n",
      "batch 508: loss 0.251380\n",
      "batch 509: loss 0.228338\n",
      "batch 510: loss 0.087931\n",
      "batch 511: loss 0.173916\n",
      "batch 512: loss 0.182807\n",
      "batch 513: loss 0.108253\n",
      "batch 514: loss 0.184226\n",
      "batch 515: loss 0.192440\n",
      "batch 516: loss 0.267682\n",
      "batch 517: loss 0.213031\n",
      "batch 518: loss 0.108590\n",
      "batch 519: loss 0.207443\n",
      "batch 520: loss 0.154614\n",
      "batch 521: loss 0.242157\n",
      "batch 522: loss 0.235736\n",
      "batch 523: loss 0.191717\n",
      "batch 524: loss 0.245623\n",
      "batch 525: loss 0.231934\n",
      "batch 526: loss 0.246470\n",
      "batch 527: loss 0.107131\n",
      "batch 528: loss 0.250475\n",
      "batch 529: loss 0.262365\n",
      "batch 530: loss 0.218430\n",
      "batch 531: loss 0.220111\n",
      "batch 532: loss 0.164505\n",
      "batch 533: loss 0.142005\n",
      "batch 534: loss 0.189445\n",
      "batch 535: loss 0.162648\n",
      "batch 536: loss 0.225000\n",
      "batch 537: loss 0.158792\n",
      "batch 538: loss 0.154956\n",
      "batch 539: loss 0.109782\n",
      "batch 540: loss 0.333567\n",
      "batch 541: loss 0.112264\n",
      "batch 542: loss 0.227144\n",
      "batch 543: loss 0.187488\n",
      "batch 544: loss 0.114973\n",
      "batch 545: loss 0.254639\n",
      "batch 546: loss 0.278084\n",
      "batch 547: loss 0.249333\n",
      "batch 548: loss 0.227348\n",
      "batch 549: loss 0.167262\n",
      "batch 550: loss 0.180637\n",
      "batch 551: loss 0.158691\n",
      "batch 552: loss 0.203995\n",
      "batch 553: loss 0.174520\n",
      "batch 554: loss 0.155788\n",
      "batch 555: loss 0.118503\n",
      "batch 556: loss 0.119403\n",
      "batch 557: loss 0.145326\n",
      "batch 558: loss 0.205791\n",
      "batch 559: loss 0.225107\n",
      "batch 560: loss 0.127338\n",
      "batch 561: loss 0.210718\n",
      "batch 562: loss 0.287501\n",
      "batch 563: loss 0.238058\n",
      "batch 564: loss 0.141589\n",
      "batch 565: loss 0.198411\n",
      "batch 566: loss 0.190512\n",
      "batch 567: loss 0.160077\n",
      "batch 568: loss 0.229601\n",
      "batch 569: loss 0.136456\n",
      "batch 570: loss 0.156495\n",
      "batch 571: loss 0.137133\n",
      "batch 572: loss 0.196600\n",
      "batch 573: loss 0.225370\n",
      "batch 574: loss 0.105252\n",
      "batch 575: loss 0.233622\n",
      "batch 576: loss 0.136545\n",
      "batch 577: loss 0.122015\n",
      "batch 578: loss 0.275364\n",
      "batch 579: loss 0.115733\n",
      "batch 580: loss 0.163645\n",
      "batch 581: loss 0.149264\n",
      "batch 582: loss 0.205010\n",
      "batch 583: loss 0.138009\n",
      "batch 584: loss 0.156313\n",
      "batch 585: loss 0.187807\n",
      "batch 586: loss 0.244257\n",
      "batch 587: loss 0.269320\n",
      "batch 588: loss 0.127256\n",
      "batch 589: loss 0.249471\n",
      "batch 590: loss 0.276641\n",
      "batch 591: loss 0.269690\n",
      "batch 592: loss 0.102672\n",
      "batch 593: loss 0.225870\n",
      "batch 594: loss 0.312810\n",
      "batch 595: loss 0.318603\n",
      "batch 596: loss 0.128522\n",
      "batch 597: loss 0.126193\n",
      "batch 598: loss 0.210201\n",
      "batch 599: loss 0.185992\n",
      "batch 600: loss 0.161441\n",
      "batch 601: loss 0.246355\n",
      "batch 602: loss 0.086972\n",
      "batch 603: loss 0.325509\n",
      "batch 604: loss 0.099217\n",
      "batch 605: loss 0.157170\n",
      "batch 606: loss 0.101434\n",
      "batch 607: loss 0.185724\n",
      "batch 608: loss 0.140466\n",
      "batch 609: loss 0.204027\n",
      "batch 610: loss 0.171768\n",
      "batch 611: loss 0.172430\n",
      "batch 612: loss 0.175871\n",
      "batch 613: loss 0.178488\n",
      "batch 614: loss 0.136483\n",
      "batch 615: loss 0.153012\n",
      "batch 616: loss 0.138467\n",
      "batch 617: loss 0.277265\n",
      "batch 618: loss 0.155203\n",
      "batch 619: loss 0.266549\n",
      "batch 620: loss 0.246394\n",
      "batch 621: loss 0.164201\n",
      "batch 622: loss 0.143285\n",
      "batch 623: loss 0.087161\n",
      "batch 624: loss 0.211647\n",
      "batch 625: loss 0.195276\n",
      "batch 626: loss 0.195560\n",
      "batch 627: loss 0.288054\n",
      "batch 628: loss 0.141637\n",
      "batch 629: loss 0.271747\n",
      "batch 630: loss 0.203956\n",
      "batch 631: loss 0.137557\n",
      "batch 632: loss 0.177601\n",
      "batch 633: loss 0.126197\n",
      "batch 634: loss 0.242673\n",
      "batch 635: loss 0.148346\n",
      "batch 636: loss 0.187242\n",
      "batch 637: loss 0.119376\n",
      "batch 638: loss 0.277743\n",
      "batch 639: loss 0.107918\n",
      "batch 640: loss 0.206723\n",
      "batch 641: loss 0.162272\n",
      "batch 642: loss 0.229458\n",
      "batch 643: loss 0.165755\n",
      "batch 644: loss 0.095944\n",
      "batch 645: loss 0.201033\n",
      "batch 646: loss 0.241183\n",
      "batch 647: loss 0.154157\n",
      "batch 648: loss 0.222949\n",
      "batch 649: loss 0.257657\n",
      "batch 650: loss 0.168725\n",
      "batch 651: loss 0.195514\n",
      "batch 652: loss 0.277141\n",
      "batch 653: loss 0.124277\n",
      "batch 654: loss 0.226853\n",
      "batch 655: loss 0.228238\n",
      "batch 656: loss 0.221769\n",
      "batch 657: loss 0.109602\n",
      "batch 658: loss 0.190948\n",
      "batch 659: loss 0.077864\n",
      "batch 660: loss 0.167167\n",
      "batch 661: loss 0.182819\n",
      "batch 662: loss 0.241044\n",
      "batch 663: loss 0.140465\n",
      "batch 664: loss 0.193631\n",
      "batch 665: loss 0.154632\n",
      "batch 666: loss 0.182529\n",
      "batch 667: loss 0.251029\n",
      "batch 668: loss 0.140106\n",
      "batch 669: loss 0.140314\n",
      "batch 670: loss 0.323167\n",
      "batch 671: loss 0.258180\n",
      "batch 672: loss 0.201803\n",
      "batch 673: loss 0.120293\n",
      "batch 674: loss 0.160730\n",
      "batch 675: loss 0.200388\n",
      "batch 676: loss 0.120869\n",
      "batch 677: loss 0.160734\n",
      "batch 678: loss 0.157938\n",
      "batch 679: loss 0.158308\n",
      "batch 680: loss 0.119848\n",
      "batch 681: loss 0.152786\n",
      "batch 682: loss 0.265529\n",
      "batch 683: loss 0.190283\n",
      "batch 684: loss 0.177495\n",
      "batch 685: loss 0.196504\n",
      "batch 686: loss 0.185545\n",
      "batch 687: loss 0.244008\n",
      "batch 688: loss 0.211151\n",
      "batch 689: loss 0.223155\n",
      "batch 690: loss 0.165505\n",
      "batch 691: loss 0.104186\n",
      "batch 692: loss 0.157034\n",
      "batch 693: loss 0.198412\n",
      "batch 694: loss 0.129267\n",
      "batch 695: loss 0.193190\n",
      "batch 696: loss 0.171607\n",
      "batch 697: loss 0.140164\n",
      "batch 698: loss 0.123082\n",
      "batch 699: loss 0.292808\n",
      "batch 700: loss 0.151174\n",
      "batch 701: loss 0.117819\n",
      "batch 702: loss 0.207719\n",
      "batch 703: loss 0.177663\n",
      "batch 704: loss 0.151015\n",
      "batch 705: loss 0.139664\n",
      "batch 706: loss 0.139360\n",
      "batch 707: loss 0.173130\n",
      "batch 708: loss 0.175471\n",
      "batch 709: loss 0.129625\n",
      "batch 710: loss 0.108426\n",
      "batch 711: loss 0.194778\n",
      "batch 712: loss 0.161633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 713: loss 0.115679\n",
      "batch 714: loss 0.149195\n",
      "batch 715: loss 0.192046\n",
      "batch 716: loss 0.151172\n",
      "batch 717: loss 0.259163\n",
      "batch 718: loss 0.151179\n",
      "batch 719: loss 0.149148\n",
      "batch 720: loss 0.141798\n",
      "batch 721: loss 0.233724\n",
      "batch 722: loss 0.205652\n",
      "batch 723: loss 0.183886\n",
      "batch 724: loss 0.202176\n",
      "batch 725: loss 0.155029\n",
      "batch 726: loss 0.238858\n",
      "batch 727: loss 0.214046\n",
      "batch 728: loss 0.109997\n",
      "batch 729: loss 0.203179\n",
      "batch 730: loss 0.145471\n",
      "batch 731: loss 0.171944\n",
      "batch 732: loss 0.122946\n",
      "batch 733: loss 0.199401\n",
      "batch 734: loss 0.154609\n",
      "batch 735: loss 0.228987\n",
      "batch 736: loss 0.176079\n",
      "batch 737: loss 0.119647\n",
      "batch 738: loss 0.240785\n",
      "batch 739: loss 0.200298\n",
      "batch 740: loss 0.121283\n",
      "batch 741: loss 0.160616\n",
      "batch 742: loss 0.314649\n",
      "batch 743: loss 0.323051\n",
      "batch 744: loss 0.119476\n",
      "batch 745: loss 0.196731\n",
      "batch 746: loss 0.099474\n",
      "batch 747: loss 0.138097\n",
      "batch 748: loss 0.169179\n",
      "batch 749: loss 0.124525\n",
      "batch 750: loss 0.151682\n",
      "batch 751: loss 0.261733\n",
      "batch 752: loss 0.157250\n",
      "batch 753: loss 0.206808\n",
      "batch 754: loss 0.214649\n",
      "batch 755: loss 0.159021\n",
      "batch 756: loss 0.119746\n",
      "batch 757: loss 0.169173\n",
      "batch 758: loss 0.116051\n",
      "batch 759: loss 0.244705\n",
      "batch 760: loss 0.274788\n",
      "batch 761: loss 0.138151\n",
      "batch 762: loss 0.238690\n",
      "batch 763: loss 0.241570\n",
      "batch 764: loss 0.218481\n",
      "batch 765: loss 0.152929\n",
      "batch 766: loss 0.146431\n",
      "batch 767: loss 0.306374\n",
      "batch 768: loss 0.221475\n",
      "batch 769: loss 0.187336\n",
      "batch 770: loss 0.196861\n",
      "batch 771: loss 0.247890\n",
      "batch 772: loss 0.160946\n",
      "batch 773: loss 0.126696\n",
      "batch 774: loss 0.197545\n",
      "batch 775: loss 0.285868\n",
      "batch 776: loss 0.142856\n",
      "batch 777: loss 0.124341\n",
      "batch 778: loss 0.090046\n",
      "batch 779: loss 0.124132\n",
      "batch 780: loss 0.167712\n",
      "batch 781: loss 0.124878\n",
      "batch 782: loss 0.221927\n",
      "batch 783: loss 0.120296\n",
      "batch 784: loss 0.220704\n",
      "batch 785: loss 0.129771\n",
      "batch 786: loss 0.219479\n",
      "batch 787: loss 0.148247\n",
      "batch 788: loss 0.118984\n",
      "batch 789: loss 0.128723\n",
      "batch 790: loss 0.098859\n",
      "batch 791: loss 0.146734\n",
      "batch 792: loss 0.175274\n",
      "batch 793: loss 0.163071\n",
      "batch 794: loss 0.226018\n",
      "batch 795: loss 0.156923\n",
      "batch 796: loss 0.135886\n",
      "batch 797: loss 0.110114\n",
      "batch 798: loss 0.138656\n",
      "batch 799: loss 0.066171\n",
      "batch 800: loss 0.103654\n",
      "batch 801: loss 0.097156\n",
      "batch 802: loss 0.093699\n",
      "batch 803: loss 0.117924\n",
      "batch 804: loss 0.249334\n",
      "batch 805: loss 0.149893\n",
      "batch 806: loss 0.118781\n",
      "batch 807: loss 0.083010\n",
      "batch 808: loss 0.199578\n",
      "batch 809: loss 0.088005\n",
      "batch 810: loss 0.127957\n",
      "batch 811: loss 0.205562\n",
      "batch 812: loss 0.241782\n",
      "batch 813: loss 0.092336\n",
      "batch 814: loss 0.168910\n",
      "batch 815: loss 0.107319\n",
      "batch 816: loss 0.241254\n",
      "batch 817: loss 0.227482\n",
      "batch 818: loss 0.178174\n",
      "batch 819: loss 0.117905\n",
      "batch 820: loss 0.157340\n",
      "batch 821: loss 0.119064\n",
      "batch 822: loss 0.194715\n",
      "batch 823: loss 0.144133\n",
      "batch 824: loss 0.121972\n",
      "batch 825: loss 0.120175\n",
      "batch 826: loss 0.090369\n",
      "batch 827: loss 0.179758\n",
      "batch 828: loss 0.203485\n",
      "batch 829: loss 0.150590\n",
      "batch 830: loss 0.153895\n",
      "batch 831: loss 0.090237\n",
      "batch 832: loss 0.126985\n",
      "batch 833: loss 0.150924\n",
      "batch 834: loss 0.233366\n",
      "batch 835: loss 0.162870\n",
      "batch 836: loss 0.131878\n",
      "batch 837: loss 0.229049\n",
      "batch 838: loss 0.205854\n",
      "batch 839: loss 0.247063\n",
      "batch 840: loss 0.073716\n",
      "batch 841: loss 0.232618\n",
      "batch 842: loss 0.114802\n",
      "batch 843: loss 0.148261\n",
      "batch 844: loss 0.113747\n",
      "batch 845: loss 0.134365\n",
      "batch 846: loss 0.182524\n",
      "batch 847: loss 0.156667\n",
      "batch 848: loss 0.114091\n",
      "batch 849: loss 0.157980\n",
      "batch 850: loss 0.109427\n",
      "batch 851: loss 0.142099\n",
      "batch 852: loss 0.085439\n",
      "batch 853: loss 0.115349\n",
      "batch 854: loss 0.157075\n",
      "batch 855: loss 0.221947\n",
      "batch 856: loss 0.237274\n",
      "batch 857: loss 0.119618\n",
      "batch 858: loss 0.145114\n",
      "batch 859: loss 0.145459\n",
      "batch 860: loss 0.133345\n",
      "batch 861: loss 0.152859\n",
      "batch 862: loss 0.243392\n",
      "batch 863: loss 0.185035\n",
      "batch 864: loss 0.160028\n",
      "batch 865: loss 0.099032\n",
      "batch 866: loss 0.118654\n",
      "batch 867: loss 0.088206\n",
      "batch 868: loss 0.179289\n",
      "batch 869: loss 0.178940\n",
      "batch 870: loss 0.092123\n",
      "batch 871: loss 0.177412\n",
      "batch 872: loss 0.136141\n",
      "batch 873: loss 0.150672\n",
      "batch 874: loss 0.229192\n",
      "batch 875: loss 0.181112\n",
      "batch 876: loss 0.092540\n",
      "batch 877: loss 0.262757\n",
      "batch 878: loss 0.140275\n",
      "batch 879: loss 0.235896\n",
      "batch 880: loss 0.116787\n",
      "batch 881: loss 0.140412\n",
      "batch 882: loss 0.173622\n",
      "batch 883: loss 0.097077\n",
      "batch 884: loss 0.175796\n",
      "batch 885: loss 0.066880\n",
      "batch 886: loss 0.131751\n",
      "batch 887: loss 0.123968\n",
      "batch 888: loss 0.216145\n",
      "batch 889: loss 0.170166\n",
      "batch 890: loss 0.209155\n",
      "batch 891: loss 0.108837\n",
      "batch 892: loss 0.180408\n",
      "batch 893: loss 0.194449\n",
      "batch 894: loss 0.170080\n",
      "batch 895: loss 0.101935\n",
      "batch 896: loss 0.133317\n",
      "batch 897: loss 0.188271\n",
      "batch 898: loss 0.220878\n",
      "batch 899: loss 0.123376\n",
      "batch 900: loss 0.183317\n",
      "batch 901: loss 0.071248\n",
      "batch 902: loss 0.136469\n",
      "batch 903: loss 0.167574\n",
      "batch 904: loss 0.214274\n",
      "batch 905: loss 0.149518\n",
      "batch 906: loss 0.131124\n",
      "batch 907: loss 0.169240\n",
      "batch 908: loss 0.109355\n",
      "batch 909: loss 0.181407\n",
      "batch 910: loss 0.121100\n",
      "batch 911: loss 0.200972\n",
      "batch 912: loss 0.180311\n",
      "batch 913: loss 0.103392\n",
      "batch 914: loss 0.130492\n",
      "batch 915: loss 0.075787\n",
      "batch 916: loss 0.079689\n",
      "batch 917: loss 0.174465\n",
      "batch 918: loss 0.086257\n",
      "batch 919: loss 0.123772\n",
      "batch 920: loss 0.107119\n",
      "batch 921: loss 0.103641\n",
      "batch 922: loss 0.076508\n",
      "batch 923: loss 0.191834\n",
      "batch 924: loss 0.210679\n",
      "batch 925: loss 0.103082\n",
      "batch 926: loss 0.142101\n",
      "batch 927: loss 0.152673\n",
      "batch 928: loss 0.168074\n",
      "batch 929: loss 0.168390\n",
      "batch 930: loss 0.149869\n",
      "batch 931: loss 0.109327\n",
      "batch 932: loss 0.143640\n",
      "batch 933: loss 0.208821\n",
      "batch 934: loss 0.125759\n",
      "batch 935: loss 0.103813\n",
      "batch 936: loss 0.188683\n",
      "batch 937: loss 0.120207\n",
      "batch 938: loss 0.119743\n",
      "batch 939: loss 0.184432\n",
      "batch 940: loss 0.120734\n",
      "batch 941: loss 0.106144\n",
      "batch 942: loss 0.104016\n",
      "batch 943: loss 0.154195\n",
      "batch 944: loss 0.138061\n",
      "batch 945: loss 0.194026\n",
      "batch 946: loss 0.133611\n",
      "batch 947: loss 0.080484\n",
      "batch 948: loss 0.109180\n",
      "batch 949: loss 0.290755\n",
      "batch 950: loss 0.178933\n",
      "batch 951: loss 0.079257\n",
      "batch 952: loss 0.213881\n",
      "batch 953: loss 0.131264\n",
      "batch 954: loss 0.150179\n",
      "batch 955: loss 0.147260\n",
      "batch 956: loss 0.137128\n",
      "batch 957: loss 0.071644\n",
      "batch 958: loss 0.121605\n",
      "batch 959: loss 0.180684\n",
      "batch 960: loss 0.130469\n",
      "batch 961: loss 0.179529\n",
      "batch 962: loss 0.165083\n",
      "batch 963: loss 0.088818\n",
      "batch 964: loss 0.109542\n",
      "batch 965: loss 0.085071\n",
      "batch 966: loss 0.096284\n",
      "batch 967: loss 0.238667\n",
      "batch 968: loss 0.166956\n",
      "batch 969: loss 0.166210\n",
      "batch 970: loss 0.209175\n",
      "batch 971: loss 0.142272\n",
      "batch 972: loss 0.159639\n",
      "batch 973: loss 0.196878\n",
      "batch 974: loss 0.121279\n",
      "batch 975: loss 0.166251\n",
      "batch 976: loss 0.126804\n",
      "batch 977: loss 0.130766\n",
      "batch 978: loss 0.112358\n",
      "batch 979: loss 0.142011\n",
      "batch 980: loss 0.180240\n",
      "batch 981: loss 0.147814\n",
      "batch 982: loss 0.132160\n",
      "batch 983: loss 0.248331\n",
      "batch 984: loss 0.253788\n",
      "batch 985: loss 0.266509\n",
      "batch 986: loss 0.163290\n",
      "batch 987: loss 0.130282\n",
      "batch 988: loss 0.121337\n",
      "batch 989: loss 0.150985\n",
      "batch 990: loss 0.130152\n",
      "batch 991: loss 0.130303\n",
      "batch 992: loss 0.118610\n",
      "batch 993: loss 0.197665\n",
      "batch 994: loss 0.134080\n",
      "batch 995: loss 0.080847\n",
      "batch 996: loss 0.061443\n",
      "batch 997: loss 0.191941\n",
      "batch 998: loss 0.107075\n",
      "batch 999: loss 0.111429\n",
      "batch 1000: loss 0.113461\n",
      "batch 1001: loss 0.115576\n",
      "batch 1002: loss 0.096990\n",
      "batch 1003: loss 0.160534\n",
      "batch 1004: loss 0.121755\n",
      "batch 1005: loss 0.120859\n",
      "batch 1006: loss 0.253310\n",
      "batch 1007: loss 0.091006\n",
      "batch 1008: loss 0.127737\n",
      "batch 1009: loss 0.067229\n",
      "batch 1010: loss 0.084880\n",
      "batch 1011: loss 0.208427\n",
      "batch 1012: loss 0.088400\n",
      "batch 1013: loss 0.187449\n",
      "batch 1014: loss 0.115428\n",
      "batch 1015: loss 0.114552\n",
      "batch 1016: loss 0.175397\n",
      "batch 1017: loss 0.139863\n",
      "batch 1018: loss 0.100412\n",
      "batch 1019: loss 0.098901\n",
      "batch 1020: loss 0.063422\n",
      "batch 1021: loss 0.198379\n",
      "batch 1022: loss 0.205585\n",
      "batch 1023: loss 0.084998\n",
      "batch 1024: loss 0.137468\n",
      "batch 1025: loss 0.138045\n",
      "batch 1026: loss 0.148291\n",
      "batch 1027: loss 0.181569\n",
      "batch 1028: loss 0.132486\n",
      "batch 1029: loss 0.066615\n",
      "batch 1030: loss 0.123009\n",
      "batch 1031: loss 0.060588\n",
      "batch 1032: loss 0.092837\n",
      "batch 1033: loss 0.175208\n",
      "batch 1034: loss 0.098868\n",
      "batch 1035: loss 0.203515\n",
      "batch 1036: loss 0.145099\n",
      "batch 1037: loss 0.119756\n",
      "batch 1038: loss 0.162274\n",
      "batch 1039: loss 0.147784\n",
      "batch 1040: loss 0.134528\n",
      "batch 1041: loss 0.111792\n",
      "batch 1042: loss 0.109625\n",
      "batch 1043: loss 0.080345\n",
      "batch 1044: loss 0.215459\n",
      "batch 1045: loss 0.174685\n",
      "batch 1046: loss 0.110375\n",
      "batch 1047: loss 0.191960\n",
      "batch 1048: loss 0.130232\n",
      "batch 1049: loss 0.131626\n",
      "batch 1050: loss 0.182366\n",
      "batch 1051: loss 0.159696\n",
      "batch 1052: loss 0.265347\n",
      "batch 1053: loss 0.098356\n",
      "batch 1054: loss 0.065850\n",
      "batch 1055: loss 0.111581\n",
      "batch 1056: loss 0.153455\n",
      "batch 1057: loss 0.151399\n",
      "batch 1058: loss 0.107972\n",
      "batch 1059: loss 0.144358\n",
      "batch 1060: loss 0.103913\n",
      "batch 1061: loss 0.334040\n",
      "batch 1062: loss 0.087636\n",
      "batch 1063: loss 0.058027\n",
      "batch 1064: loss 0.130774\n",
      "batch 1065: loss 0.109825\n",
      "batch 1066: loss 0.180964\n",
      "batch 1067: loss 0.048309\n",
      "batch 1068: loss 0.263319\n",
      "batch 1069: loss 0.186953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1070: loss 0.123717\n",
      "batch 1071: loss 0.088589\n",
      "batch 1072: loss 0.236520\n",
      "batch 1073: loss 0.103225\n",
      "batch 1074: loss 0.133843\n",
      "batch 1075: loss 0.177033\n",
      "batch 1076: loss 0.213772\n",
      "batch 1077: loss 0.113789\n",
      "batch 1078: loss 0.119251\n",
      "batch 1079: loss 0.102967\n",
      "batch 1080: loss 0.066579\n",
      "batch 1081: loss 0.178038\n",
      "batch 1082: loss 0.107301\n",
      "batch 1083: loss 0.187393\n",
      "batch 1084: loss 0.071944\n",
      "batch 1085: loss 0.194159\n",
      "batch 1086: loss 0.140308\n",
      "batch 1087: loss 0.055425\n",
      "batch 1088: loss 0.069027\n",
      "batch 1089: loss 0.127895\n",
      "batch 1090: loss 0.178255\n",
      "batch 1091: loss 0.111818\n",
      "batch 1092: loss 0.140404\n",
      "batch 1093: loss 0.092839\n",
      "batch 1094: loss 0.121677\n",
      "batch 1095: loss 0.156801\n",
      "batch 1096: loss 0.120235\n",
      "batch 1097: loss 0.097356\n",
      "batch 1098: loss 0.178133\n",
      "batch 1099: loss 0.103706\n",
      "batch 1100: loss 0.104848\n",
      "batch 1101: loss 0.110507\n",
      "batch 1102: loss 0.172541\n",
      "batch 1103: loss 0.179371\n",
      "batch 1104: loss 0.067248\n",
      "batch 1105: loss 0.233585\n",
      "batch 1106: loss 0.161937\n",
      "batch 1107: loss 0.179160\n",
      "batch 1108: loss 0.161553\n",
      "batch 1109: loss 0.259032\n",
      "batch 1110: loss 0.110649\n",
      "batch 1111: loss 0.122708\n",
      "batch 1112: loss 0.112566\n",
      "batch 1113: loss 0.140469\n",
      "batch 1114: loss 0.103279\n",
      "batch 1115: loss 0.088138\n",
      "batch 1116: loss 0.117351\n",
      "batch 1117: loss 0.133632\n",
      "batch 1118: loss 0.071043\n",
      "batch 1119: loss 0.092712\n",
      "batch 1120: loss 0.155808\n",
      "batch 1121: loss 0.049338\n",
      "batch 1122: loss 0.176108\n",
      "batch 1123: loss 0.143295\n",
      "batch 1124: loss 0.179579\n",
      "batch 1125: loss 0.277026\n",
      "batch 1126: loss 0.145106\n",
      "batch 1127: loss 0.104038\n",
      "batch 1128: loss 0.125935\n",
      "batch 1129: loss 0.161076\n",
      "batch 1130: loss 0.115702\n",
      "batch 1131: loss 0.115697\n",
      "batch 1132: loss 0.140461\n",
      "batch 1133: loss 0.096166\n",
      "batch 1134: loss 0.134351\n",
      "batch 1135: loss 0.163976\n",
      "batch 1136: loss 0.097456\n",
      "batch 1137: loss 0.105306\n",
      "batch 1138: loss 0.117596\n",
      "batch 1139: loss 0.088703\n",
      "batch 1140: loss 0.080729\n",
      "batch 1141: loss 0.172278\n",
      "batch 1142: loss 0.157070\n",
      "batch 1143: loss 0.198970\n",
      "batch 1144: loss 0.120654\n",
      "batch 1145: loss 0.124255\n",
      "batch 1146: loss 0.218447\n",
      "batch 1147: loss 0.107913\n",
      "batch 1148: loss 0.085905\n",
      "batch 1149: loss 0.164480\n",
      "batch 1150: loss 0.063316\n",
      "batch 1151: loss 0.080392\n",
      "batch 1152: loss 0.101469\n",
      "batch 1153: loss 0.054993\n",
      "batch 1154: loss 0.072015\n",
      "batch 1155: loss 0.122522\n",
      "batch 1156: loss 0.100560\n",
      "batch 1157: loss 0.116955\n",
      "batch 1158: loss 0.194332\n",
      "batch 1159: loss 0.102047\n",
      "batch 1160: loss 0.113710\n",
      "batch 1161: loss 0.114145\n",
      "batch 1162: loss 0.102684\n",
      "batch 1163: loss 0.069941\n",
      "batch 1164: loss 0.185934\n",
      "batch 1165: loss 0.103090\n",
      "batch 1166: loss 0.053924\n",
      "batch 1167: loss 0.068038\n",
      "batch 1168: loss 0.187188\n",
      "batch 1169: loss 0.084386\n",
      "batch 1170: loss 0.178416\n",
      "batch 1171: loss 0.111213\n",
      "batch 1172: loss 0.127251\n",
      "batch 1173: loss 0.164236\n",
      "batch 1174: loss 0.118077\n",
      "batch 1175: loss 0.102244\n",
      "batch 1176: loss 0.099866\n",
      "batch 1177: loss 0.157526\n",
      "batch 1178: loss 0.186770\n",
      "batch 1179: loss 0.118552\n",
      "batch 1180: loss 0.084732\n",
      "batch 1181: loss 0.091339\n",
      "batch 1182: loss 0.104820\n",
      "batch 1183: loss 0.114324\n",
      "batch 1184: loss 0.082832\n",
      "batch 1185: loss 0.123465\n",
      "batch 1186: loss 0.120137\n",
      "batch 1187: loss 0.120187\n",
      "batch 1188: loss 0.285315\n",
      "batch 1189: loss 0.125624\n",
      "batch 1190: loss 0.127515\n",
      "batch 1191: loss 0.151860\n",
      "batch 1192: loss 0.074398\n",
      "batch 1193: loss 0.068020\n",
      "batch 1194: loss 0.141029\n",
      "batch 1195: loss 0.101475\n",
      "batch 1196: loss 0.079423\n",
      "batch 1197: loss 0.088202\n",
      "batch 1198: loss 0.125960\n",
      "batch 1199: loss 0.107771\n",
      "batch 1200: loss 0.169749\n",
      "batch 1201: loss 0.113927\n",
      "batch 1202: loss 0.173878\n",
      "batch 1203: loss 0.114400\n",
      "batch 1204: loss 0.113735\n",
      "batch 1205: loss 0.089593\n",
      "batch 1206: loss 0.148840\n",
      "batch 1207: loss 0.073599\n",
      "batch 1208: loss 0.146229\n",
      "batch 1209: loss 0.104459\n",
      "batch 1210: loss 0.181340\n",
      "batch 1211: loss 0.110241\n",
      "batch 1212: loss 0.111777\n",
      "batch 1213: loss 0.088614\n",
      "batch 1214: loss 0.143956\n",
      "batch 1215: loss 0.108155\n",
      "batch 1216: loss 0.087210\n",
      "batch 1217: loss 0.069338\n",
      "batch 1218: loss 0.097489\n",
      "batch 1219: loss 0.138065\n",
      "batch 1220: loss 0.112295\n",
      "batch 1221: loss 0.137787\n",
      "batch 1222: loss 0.099045\n",
      "batch 1223: loss 0.091277\n",
      "batch 1224: loss 0.112361\n",
      "batch 1225: loss 0.057269\n",
      "batch 1226: loss 0.196891\n",
      "batch 1227: loss 0.234544\n",
      "batch 1228: loss 0.096788\n",
      "batch 1229: loss 0.189365\n",
      "batch 1230: loss 0.102166\n",
      "batch 1231: loss 0.090411\n",
      "batch 1232: loss 0.128688\n",
      "batch 1233: loss 0.100281\n",
      "batch 1234: loss 0.178665\n",
      "batch 1235: loss 0.091129\n",
      "batch 1236: loss 0.103952\n",
      "batch 1237: loss 0.052425\n",
      "batch 1238: loss 0.185671\n",
      "batch 1239: loss 0.142774\n",
      "batch 1240: loss 0.099961\n",
      "batch 1241: loss 0.068298\n",
      "batch 1242: loss 0.240782\n",
      "batch 1243: loss 0.102316\n",
      "batch 1244: loss 0.121021\n",
      "batch 1245: loss 0.184996\n",
      "batch 1246: loss 0.123781\n",
      "batch 1247: loss 0.128298\n",
      "batch 1248: loss 0.194036\n",
      "batch 1249: loss 0.085388\n",
      "batch 1250: loss 0.124923\n",
      "batch 1251: loss 0.104505\n",
      "batch 1252: loss 0.113030\n",
      "batch 1253: loss 0.122737\n",
      "batch 1254: loss 0.166848\n",
      "batch 1255: loss 0.102385\n",
      "batch 1256: loss 0.045969\n",
      "batch 1257: loss 0.184152\n",
      "batch 1258: loss 0.090371\n",
      "batch 1259: loss 0.070630\n",
      "batch 1260: loss 0.181711\n",
      "batch 1261: loss 0.097767\n",
      "batch 1262: loss 0.116076\n",
      "batch 1263: loss 0.104622\n",
      "batch 1264: loss 0.191101\n",
      "batch 1265: loss 0.154094\n",
      "batch 1266: loss 0.133740\n",
      "batch 1267: loss 0.158654\n",
      "batch 1268: loss 0.122607\n",
      "batch 1269: loss 0.162330\n",
      "batch 1270: loss 0.143911\n",
      "batch 1271: loss 0.142793\n",
      "batch 1272: loss 0.169315\n",
      "batch 1273: loss 0.049643\n",
      "batch 1274: loss 0.075501\n",
      "batch 1275: loss 0.159059\n",
      "batch 1276: loss 0.183315\n",
      "batch 1277: loss 0.177764\n",
      "batch 1278: loss 0.180829\n",
      "batch 1279: loss 0.079027\n",
      "batch 1280: loss 0.112383\n",
      "batch 1281: loss 0.092641\n",
      "batch 1282: loss 0.238701\n",
      "batch 1283: loss 0.108091\n",
      "batch 1284: loss 0.117424\n",
      "batch 1285: loss 0.089419\n",
      "batch 1286: loss 0.142874\n",
      "batch 1287: loss 0.122672\n",
      "batch 1288: loss 0.115974\n",
      "batch 1289: loss 0.078529\n",
      "batch 1290: loss 0.089401\n",
      "batch 1291: loss 0.124354\n",
      "batch 1292: loss 0.069410\n",
      "batch 1293: loss 0.062107\n",
      "batch 1294: loss 0.212741\n",
      "batch 1295: loss 0.098553\n",
      "batch 1296: loss 0.152952\n",
      "batch 1297: loss 0.111251\n",
      "batch 1298: loss 0.156866\n",
      "batch 1299: loss 0.154196\n",
      "batch 1300: loss 0.063927\n",
      "batch 1301: loss 0.087144\n",
      "batch 1302: loss 0.147985\n",
      "batch 1303: loss 0.079680\n",
      "batch 1304: loss 0.115072\n",
      "batch 1305: loss 0.071182\n",
      "batch 1306: loss 0.107574\n",
      "batch 1307: loss 0.201477\n",
      "batch 1308: loss 0.124124\n",
      "batch 1309: loss 0.073736\n",
      "batch 1310: loss 0.152725\n",
      "batch 1311: loss 0.255782\n",
      "batch 1312: loss 0.131162\n",
      "batch 1313: loss 0.143693\n",
      "batch 1314: loss 0.062384\n",
      "batch 1315: loss 0.144452\n",
      "batch 1316: loss 0.160727\n",
      "batch 1317: loss 0.190509\n",
      "batch 1318: loss 0.058623\n",
      "batch 1319: loss 0.159726\n",
      "batch 1320: loss 0.108063\n",
      "batch 1321: loss 0.070461\n",
      "batch 1322: loss 0.143046\n",
      "batch 1323: loss 0.063495\n",
      "batch 1324: loss 0.093205\n",
      "batch 1325: loss 0.065796\n",
      "batch 1326: loss 0.080776\n",
      "batch 1327: loss 0.159591\n",
      "batch 1328: loss 0.094747\n",
      "batch 1329: loss 0.216456\n",
      "batch 1330: loss 0.098248\n",
      "batch 1331: loss 0.084397\n",
      "batch 1332: loss 0.073783\n",
      "batch 1333: loss 0.171152\n",
      "batch 1334: loss 0.213915\n",
      "batch 1335: loss 0.102837\n",
      "batch 1336: loss 0.191854\n",
      "batch 1337: loss 0.075814\n",
      "batch 1338: loss 0.178403\n",
      "batch 1339: loss 0.104536\n",
      "batch 1340: loss 0.171007\n",
      "batch 1341: loss 0.062428\n",
      "batch 1342: loss 0.050799\n",
      "batch 1343: loss 0.105321\n",
      "batch 1344: loss 0.084668\n",
      "batch 1345: loss 0.093017\n",
      "batch 1346: loss 0.133531\n",
      "batch 1347: loss 0.105647\n",
      "batch 1348: loss 0.074491\n",
      "batch 1349: loss 0.109553\n",
      "batch 1350: loss 0.159123\n",
      "batch 1351: loss 0.164242\n",
      "batch 1352: loss 0.099772\n",
      "batch 1353: loss 0.134606\n",
      "batch 1354: loss 0.172353\n",
      "batch 1355: loss 0.050198\n",
      "batch 1356: loss 0.072341\n",
      "batch 1357: loss 0.139048\n",
      "batch 1358: loss 0.139576\n",
      "batch 1359: loss 0.118062\n",
      "batch 1360: loss 0.135389\n",
      "batch 1361: loss 0.136805\n",
      "batch 1362: loss 0.121857\n",
      "batch 1363: loss 0.200441\n",
      "batch 1364: loss 0.136714\n",
      "batch 1365: loss 0.072014\n",
      "batch 1366: loss 0.116619\n",
      "batch 1367: loss 0.113512\n",
      "batch 1368: loss 0.059231\n",
      "batch 1369: loss 0.134460\n",
      "batch 1370: loss 0.058832\n",
      "batch 1371: loss 0.156924\n",
      "batch 1372: loss 0.210145\n",
      "batch 1373: loss 0.063434\n",
      "batch 1374: loss 0.083494\n",
      "batch 1375: loss 0.072738\n",
      "batch 1376: loss 0.117917\n",
      "batch 1377: loss 0.098348\n",
      "batch 1378: loss 0.073752\n",
      "batch 1379: loss 0.116541\n",
      "batch 1380: loss 0.093064\n",
      "batch 1381: loss 0.198427\n",
      "batch 1382: loss 0.141599\n",
      "batch 1383: loss 0.091192\n",
      "batch 1384: loss 0.091342\n",
      "batch 1385: loss 0.052877\n",
      "batch 1386: loss 0.165645\n",
      "batch 1387: loss 0.077113\n",
      "batch 1388: loss 0.187177\n",
      "batch 1389: loss 0.168215\n",
      "batch 1390: loss 0.094910\n",
      "batch 1391: loss 0.043202\n",
      "batch 1392: loss 0.152779\n",
      "batch 1393: loss 0.107052\n",
      "batch 1394: loss 0.115956\n",
      "batch 1395: loss 0.139968\n",
      "batch 1396: loss 0.047444\n",
      "batch 1397: loss 0.120624\n",
      "batch 1398: loss 0.082283\n",
      "batch 1399: loss 0.115924\n",
      "batch 1400: loss 0.142080\n",
      "batch 1401: loss 0.081398\n",
      "batch 1402: loss 0.144838\n",
      "batch 1403: loss 0.105514\n",
      "batch 1404: loss 0.195544\n",
      "batch 1405: loss 0.090465\n",
      "batch 1406: loss 0.083287\n",
      "batch 1407: loss 0.254630\n",
      "batch 1408: loss 0.057145\n",
      "batch 1409: loss 0.134153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1410: loss 0.120405\n",
      "batch 1411: loss 0.132794\n",
      "batch 1412: loss 0.057551\n",
      "batch 1413: loss 0.101053\n",
      "batch 1414: loss 0.157777\n",
      "batch 1415: loss 0.095585\n",
      "batch 1416: loss 0.134260\n",
      "batch 1417: loss 0.149012\n",
      "batch 1418: loss 0.075579\n",
      "batch 1419: loss 0.139558\n",
      "batch 1420: loss 0.113521\n",
      "batch 1421: loss 0.039231\n",
      "batch 1422: loss 0.096476\n",
      "batch 1423: loss 0.159399\n",
      "batch 1424: loss 0.077456\n",
      "batch 1425: loss 0.066192\n",
      "batch 1426: loss 0.146494\n",
      "batch 1427: loss 0.109288\n",
      "batch 1428: loss 0.135529\n",
      "batch 1429: loss 0.058691\n",
      "batch 1430: loss 0.062791\n",
      "batch 1431: loss 0.147238\n",
      "batch 1432: loss 0.176786\n",
      "batch 1433: loss 0.167249\n",
      "batch 1434: loss 0.076077\n",
      "batch 1435: loss 0.281355\n",
      "batch 1436: loss 0.084928\n",
      "batch 1437: loss 0.193813\n",
      "batch 1438: loss 0.143316\n",
      "batch 1439: loss 0.097211\n",
      "batch 1440: loss 0.109280\n",
      "batch 1441: loss 0.116248\n",
      "batch 1442: loss 0.110352\n",
      "batch 1443: loss 0.094117\n",
      "batch 1444: loss 0.070617\n",
      "batch 1445: loss 0.105272\n",
      "batch 1446: loss 0.061085\n",
      "batch 1447: loss 0.097491\n",
      "batch 1448: loss 0.081232\n",
      "batch 1449: loss 0.115598\n",
      "batch 1450: loss 0.150305\n",
      "batch 1451: loss 0.091485\n",
      "batch 1452: loss 0.101111\n",
      "batch 1453: loss 0.144677\n",
      "batch 1454: loss 0.098434\n",
      "batch 1455: loss 0.105682\n",
      "batch 1456: loss 0.180971\n",
      "batch 1457: loss 0.125205\n",
      "batch 1458: loss 0.115634\n",
      "batch 1459: loss 0.054610\n",
      "batch 1460: loss 0.147909\n",
      "batch 1461: loss 0.075084\n",
      "batch 1462: loss 0.188828\n",
      "batch 1463: loss 0.107290\n",
      "batch 1464: loss 0.163960\n",
      "batch 1465: loss 0.063791\n",
      "batch 1466: loss 0.047019\n",
      "batch 1467: loss 0.155187\n",
      "batch 1468: loss 0.094900\n",
      "batch 1469: loss 0.115902\n",
      "batch 1470: loss 0.087541\n",
      "batch 1471: loss 0.151591\n",
      "batch 1472: loss 0.113869\n",
      "batch 1473: loss 0.090994\n",
      "batch 1474: loss 0.173841\n",
      "batch 1475: loss 0.086713\n",
      "batch 1476: loss 0.158520\n",
      "batch 1477: loss 0.115916\n",
      "batch 1478: loss 0.087563\n",
      "batch 1479: loss 0.129398\n",
      "batch 1480: loss 0.088569\n",
      "batch 1481: loss 0.068067\n",
      "batch 1482: loss 0.080673\n",
      "batch 1483: loss 0.096774\n",
      "batch 1484: loss 0.095451\n",
      "batch 1485: loss 0.173663\n",
      "batch 1486: loss 0.077110\n",
      "batch 1487: loss 0.102915\n",
      "batch 1488: loss 0.071136\n",
      "batch 1489: loss 0.082880\n",
      "batch 1490: loss 0.146954\n",
      "batch 1491: loss 0.156876\n",
      "batch 1492: loss 0.127124\n",
      "batch 1493: loss 0.155334\n",
      "batch 1494: loss 0.091618\n",
      "batch 1495: loss 0.079305\n",
      "batch 1496: loss 0.071563\n",
      "batch 1497: loss 0.111972\n",
      "batch 1498: loss 0.130142\n",
      "batch 1499: loss 0.107869\n",
      "batch 1500: loss 0.179969\n",
      "batch 1501: loss 0.118526\n",
      "batch 1502: loss 0.214068\n",
      "batch 1503: loss 0.039981\n",
      "batch 1504: loss 0.037934\n",
      "batch 1505: loss 0.053660\n",
      "batch 1506: loss 0.083011\n",
      "batch 1507: loss 0.085849\n",
      "batch 1508: loss 0.070213\n",
      "batch 1509: loss 0.114919\n",
      "batch 1510: loss 0.123456\n",
      "batch 1511: loss 0.097573\n",
      "batch 1512: loss 0.097148\n",
      "batch 1513: loss 0.111296\n",
      "batch 1514: loss 0.082172\n",
      "batch 1515: loss 0.069870\n",
      "batch 1516: loss 0.213971\n",
      "batch 1517: loss 0.160193\n",
      "batch 1518: loss 0.131347\n",
      "batch 1519: loss 0.060194\n",
      "batch 1520: loss 0.123063\n",
      "batch 1521: loss 0.076947\n",
      "batch 1522: loss 0.110330\n",
      "batch 1523: loss 0.108408\n",
      "batch 1524: loss 0.070085\n",
      "batch 1525: loss 0.091719\n",
      "batch 1526: loss 0.100852\n",
      "batch 1527: loss 0.123126\n",
      "batch 1528: loss 0.157138\n",
      "batch 1529: loss 0.097960\n",
      "batch 1530: loss 0.091848\n",
      "batch 1531: loss 0.231772\n",
      "batch 1532: loss 0.089608\n",
      "batch 1533: loss 0.159884\n",
      "batch 1534: loss 0.175070\n",
      "batch 1535: loss 0.204805\n",
      "batch 1536: loss 0.075266\n",
      "batch 1537: loss 0.089200\n",
      "batch 1538: loss 0.138540\n",
      "batch 1539: loss 0.059747\n",
      "batch 1540: loss 0.087326\n",
      "batch 1541: loss 0.147399\n",
      "batch 1542: loss 0.109651\n",
      "batch 1543: loss 0.124282\n",
      "batch 1544: loss 0.057407\n",
      "batch 1545: loss 0.081298\n",
      "batch 1546: loss 0.114338\n",
      "batch 1547: loss 0.192833\n",
      "batch 1548: loss 0.156760\n",
      "batch 1549: loss 0.131280\n",
      "batch 1550: loss 0.137481\n",
      "batch 1551: loss 0.178967\n",
      "batch 1552: loss 0.134057\n",
      "batch 1553: loss 0.105663\n",
      "batch 1554: loss 0.102503\n",
      "batch 1555: loss 0.095249\n",
      "batch 1556: loss 0.147447\n",
      "batch 1557: loss 0.182469\n",
      "batch 1558: loss 0.096871\n",
      "batch 1559: loss 0.181794\n",
      "batch 1560: loss 0.070545\n",
      "batch 1561: loss 0.076908\n",
      "batch 1562: loss 0.087020\n",
      "batch 1563: loss 0.075636\n",
      "batch 1564: loss 0.057630\n",
      "batch 1565: loss 0.146674\n",
      "batch 1566: loss 0.134958\n",
      "batch 1567: loss 0.039016\n",
      "batch 1568: loss 0.104393\n",
      "batch 1569: loss 0.030057\n",
      "batch 1570: loss 0.141533\n",
      "batch 1571: loss 0.081953\n",
      "batch 1572: loss 0.098144\n",
      "batch 1573: loss 0.059920\n",
      "batch 1574: loss 0.083545\n",
      "batch 1575: loss 0.079589\n",
      "batch 1576: loss 0.043223\n",
      "batch 1577: loss 0.080245\n",
      "batch 1578: loss 0.101274\n",
      "batch 1579: loss 0.147649\n",
      "batch 1580: loss 0.056685\n",
      "batch 1581: loss 0.098669\n",
      "batch 1582: loss 0.037595\n",
      "batch 1583: loss 0.119289\n",
      "batch 1584: loss 0.151873\n",
      "batch 1585: loss 0.075223\n",
      "batch 1586: loss 0.113132\n",
      "batch 1587: loss 0.119984\n",
      "batch 1588: loss 0.102912\n",
      "batch 1589: loss 0.068295\n",
      "batch 1590: loss 0.128905\n",
      "batch 1591: loss 0.079387\n",
      "batch 1592: loss 0.072389\n",
      "batch 1593: loss 0.078969\n",
      "batch 1594: loss 0.057766\n",
      "batch 1595: loss 0.114329\n",
      "batch 1596: loss 0.025258\n",
      "batch 1597: loss 0.094077\n",
      "batch 1598: loss 0.085521\n",
      "batch 1599: loss 0.149480\n",
      "batch 1600: loss 0.123058\n",
      "batch 1601: loss 0.126178\n",
      "batch 1602: loss 0.113746\n",
      "batch 1603: loss 0.112739\n",
      "batch 1604: loss 0.062303\n",
      "batch 1605: loss 0.062859\n",
      "batch 1606: loss 0.160796\n",
      "batch 1607: loss 0.045571\n",
      "batch 1608: loss 0.090116\n",
      "batch 1609: loss 0.062392\n",
      "batch 1610: loss 0.067332\n",
      "batch 1611: loss 0.115818\n",
      "batch 1612: loss 0.168517\n",
      "batch 1613: loss 0.090639\n",
      "batch 1614: loss 0.128086\n",
      "batch 1615: loss 0.057980\n",
      "batch 1616: loss 0.133247\n",
      "batch 1617: loss 0.130800\n",
      "batch 1618: loss 0.081043\n",
      "batch 1619: loss 0.111186\n",
      "batch 1620: loss 0.133573\n",
      "batch 1621: loss 0.079985\n",
      "batch 1622: loss 0.109163\n",
      "batch 1623: loss 0.086878\n",
      "batch 1624: loss 0.098267\n",
      "batch 1625: loss 0.091181\n",
      "batch 1626: loss 0.064559\n",
      "batch 1627: loss 0.078413\n",
      "batch 1628: loss 0.065707\n",
      "batch 1629: loss 0.157545\n",
      "batch 1630: loss 0.057567\n",
      "batch 1631: loss 0.088768\n",
      "batch 1632: loss 0.173765\n",
      "batch 1633: loss 0.131548\n",
      "batch 1634: loss 0.135938\n",
      "batch 1635: loss 0.092710\n",
      "batch 1636: loss 0.090085\n",
      "batch 1637: loss 0.116749\n",
      "batch 1638: loss 0.084665\n",
      "batch 1639: loss 0.088029\n",
      "batch 1640: loss 0.137653\n",
      "batch 1641: loss 0.096744\n",
      "batch 1642: loss 0.137561\n",
      "batch 1643: loss 0.054352\n",
      "batch 1644: loss 0.042346\n",
      "batch 1645: loss 0.238336\n",
      "batch 1646: loss 0.105639\n",
      "batch 1647: loss 0.110131\n",
      "batch 1648: loss 0.079359\n",
      "batch 1649: loss 0.090037\n",
      "batch 1650: loss 0.130682\n",
      "batch 1651: loss 0.073689\n",
      "batch 1652: loss 0.123077\n",
      "batch 1653: loss 0.099536\n",
      "batch 1654: loss 0.092322\n",
      "batch 1655: loss 0.155666\n",
      "batch 1656: loss 0.221139\n",
      "batch 1657: loss 0.110358\n",
      "batch 1658: loss 0.165256\n",
      "batch 1659: loss 0.105522\n",
      "batch 1660: loss 0.082314\n",
      "batch 1661: loss 0.191788\n",
      "batch 1662: loss 0.160833\n",
      "batch 1663: loss 0.056454\n",
      "batch 1664: loss 0.060512\n",
      "batch 1665: loss 0.171226\n",
      "batch 1666: loss 0.161240\n",
      "batch 1667: loss 0.110060\n",
      "batch 1668: loss 0.086211\n",
      "batch 1669: loss 0.093672\n",
      "batch 1670: loss 0.100129\n",
      "batch 1671: loss 0.138313\n",
      "batch 1672: loss 0.177497\n",
      "batch 1673: loss 0.182232\n",
      "batch 1674: loss 0.073563\n",
      "batch 1675: loss 0.050418\n",
      "batch 1676: loss 0.098622\n",
      "batch 1677: loss 0.117093\n",
      "batch 1678: loss 0.109200\n",
      "batch 1679: loss 0.173667\n",
      "batch 1680: loss 0.034564\n",
      "batch 1681: loss 0.120802\n",
      "batch 1682: loss 0.182992\n",
      "batch 1683: loss 0.112586\n",
      "batch 1684: loss 0.112847\n",
      "batch 1685: loss 0.100235\n",
      "batch 1686: loss 0.143206\n",
      "batch 1687: loss 0.107262\n",
      "batch 1688: loss 0.220548\n",
      "batch 1689: loss 0.112576\n",
      "batch 1690: loss 0.105045\n",
      "batch 1691: loss 0.163122\n",
      "batch 1692: loss 0.111180\n",
      "batch 1693: loss 0.044246\n",
      "batch 1694: loss 0.146443\n",
      "batch 1695: loss 0.072580\n",
      "batch 1696: loss 0.082834\n",
      "batch 1697: loss 0.053781\n",
      "batch 1698: loss 0.107912\n",
      "batch 1699: loss 0.194555\n",
      "batch 1700: loss 0.048017\n",
      "batch 1701: loss 0.107640\n",
      "batch 1702: loss 0.081679\n",
      "batch 1703: loss 0.076299\n",
      "batch 1704: loss 0.038365\n",
      "batch 1705: loss 0.080243\n",
      "batch 1706: loss 0.103124\n",
      "batch 1707: loss 0.103234\n",
      "batch 1708: loss 0.105557\n",
      "batch 1709: loss 0.058322\n",
      "batch 1710: loss 0.073261\n",
      "batch 1711: loss 0.127556\n",
      "batch 1712: loss 0.071827\n",
      "batch 1713: loss 0.057360\n",
      "batch 1714: loss 0.073759\n",
      "batch 1715: loss 0.123096\n",
      "batch 1716: loss 0.060606\n",
      "batch 1717: loss 0.105633\n",
      "batch 1718: loss 0.087518\n",
      "batch 1719: loss 0.079308\n",
      "batch 1720: loss 0.217440\n",
      "batch 1721: loss 0.077548\n",
      "batch 1722: loss 0.118672\n",
      "batch 1723: loss 0.052573\n",
      "batch 1724: loss 0.138708\n",
      "batch 1725: loss 0.097365\n",
      "batch 1726: loss 0.114270\n",
      "batch 1727: loss 0.095167\n",
      "batch 1728: loss 0.095035\n",
      "batch 1729: loss 0.062815\n",
      "batch 1730: loss 0.109088\n",
      "batch 1731: loss 0.064338\n",
      "batch 1732: loss 0.046063\n",
      "batch 1733: loss 0.123676\n",
      "batch 1734: loss 0.203815\n",
      "batch 1735: loss 0.050850\n",
      "batch 1736: loss 0.071649\n",
      "batch 1737: loss 0.071235\n",
      "batch 1738: loss 0.109456\n",
      "batch 1739: loss 0.151766\n",
      "batch 1740: loss 0.097420\n",
      "batch 1741: loss 0.132479\n",
      "batch 1742: loss 0.151884\n",
      "batch 1743: loss 0.064408\n",
      "batch 1744: loss 0.236478\n",
      "batch 1745: loss 0.086065\n",
      "batch 1746: loss 0.086254\n",
      "batch 1747: loss 0.136415\n",
      "batch 1748: loss 0.082522\n",
      "batch 1749: loss 0.061710\n",
      "batch 1750: loss 0.140411\n",
      "batch 1751: loss 0.103064\n",
      "batch 1752: loss 0.088521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1753: loss 0.104259\n",
      "batch 1754: loss 0.097941\n",
      "batch 1755: loss 0.035824\n",
      "batch 1756: loss 0.120498\n",
      "batch 1757: loss 0.115086\n",
      "batch 1758: loss 0.066428\n",
      "batch 1759: loss 0.055466\n",
      "batch 1760: loss 0.084437\n",
      "batch 1761: loss 0.061315\n",
      "batch 1762: loss 0.093948\n",
      "batch 1763: loss 0.053131\n",
      "batch 1764: loss 0.071983\n",
      "batch 1765: loss 0.076783\n",
      "batch 1766: loss 0.049113\n",
      "batch 1767: loss 0.096790\n",
      "batch 1768: loss 0.098795\n",
      "batch 1769: loss 0.109611\n",
      "batch 1770: loss 0.076075\n",
      "batch 1771: loss 0.085645\n",
      "batch 1772: loss 0.058530\n",
      "batch 1773: loss 0.117812\n",
      "batch 1774: loss 0.086189\n",
      "batch 1775: loss 0.062260\n",
      "batch 1776: loss 0.132159\n",
      "batch 1777: loss 0.082786\n",
      "batch 1778: loss 0.095504\n",
      "batch 1779: loss 0.064015\n",
      "batch 1780: loss 0.071428\n",
      "batch 1781: loss 0.080032\n",
      "batch 1782: loss 0.080693\n",
      "batch 1783: loss 0.074219\n",
      "batch 1784: loss 0.088571\n",
      "batch 1785: loss 0.072496\n",
      "batch 1786: loss 0.067427\n",
      "batch 1787: loss 0.103544\n",
      "batch 1788: loss 0.064993\n",
      "batch 1789: loss 0.056026\n",
      "batch 1790: loss 0.050375\n",
      "batch 1791: loss 0.059961\n",
      "batch 1792: loss 0.119250\n",
      "batch 1793: loss 0.043585\n",
      "batch 1794: loss 0.034689\n",
      "batch 1795: loss 0.109910\n",
      "batch 1796: loss 0.101327\n",
      "batch 1797: loss 0.077298\n",
      "batch 1798: loss 0.059081\n",
      "batch 1799: loss 0.074494\n",
      "batch 1800: loss 0.114081\n",
      "batch 1801: loss 0.077421\n",
      "batch 1802: loss 0.109453\n",
      "batch 1803: loss 0.087235\n",
      "batch 1804: loss 0.054519\n",
      "batch 1805: loss 0.076154\n",
      "batch 1806: loss 0.118986\n",
      "batch 1807: loss 0.047803\n",
      "batch 1808: loss 0.058197\n",
      "batch 1809: loss 0.122351\n",
      "batch 1810: loss 0.080635\n",
      "batch 1811: loss 0.090627\n",
      "batch 1812: loss 0.121913\n",
      "batch 1813: loss 0.073696\n",
      "batch 1814: loss 0.055303\n",
      "batch 1815: loss 0.132588\n",
      "batch 1816: loss 0.050104\n",
      "batch 1817: loss 0.121706\n",
      "batch 1818: loss 0.074406\n",
      "batch 1819: loss 0.061693\n",
      "batch 1820: loss 0.093747\n",
      "batch 1821: loss 0.178041\n",
      "batch 1822: loss 0.100049\n",
      "batch 1823: loss 0.026168\n",
      "batch 1824: loss 0.109464\n",
      "batch 1825: loss 0.049268\n",
      "batch 1826: loss 0.090516\n",
      "batch 1827: loss 0.089983\n",
      "batch 1828: loss 0.214427\n",
      "batch 1829: loss 0.135227\n",
      "batch 1830: loss 0.170688\n",
      "batch 1831: loss 0.100541\n",
      "batch 1832: loss 0.169483\n",
      "batch 1833: loss 0.026838\n",
      "batch 1834: loss 0.135913\n",
      "batch 1835: loss 0.146187\n",
      "batch 1836: loss 0.078919\n",
      "batch 1837: loss 0.087832\n",
      "batch 1838: loss 0.086370\n",
      "batch 1839: loss 0.040533\n",
      "batch 1840: loss 0.091989\n",
      "batch 1841: loss 0.045964\n",
      "batch 1842: loss 0.085926\n",
      "batch 1843: loss 0.117027\n",
      "batch 1844: loss 0.196069\n",
      "batch 1845: loss 0.073318\n",
      "batch 1846: loss 0.053881\n",
      "batch 1847: loss 0.083645\n",
      "batch 1848: loss 0.064908\n",
      "batch 1849: loss 0.041611\n",
      "batch 1850: loss 0.094764\n",
      "batch 1851: loss 0.074413\n",
      "batch 1852: loss 0.081200\n",
      "batch 1853: loss 0.109863\n",
      "batch 1854: loss 0.121176\n",
      "batch 1855: loss 0.130187\n",
      "batch 1856: loss 0.136822\n",
      "batch 1857: loss 0.154339\n",
      "batch 1858: loss 0.073148\n",
      "batch 1859: loss 0.054651\n",
      "batch 1860: loss 0.041921\n",
      "batch 1861: loss 0.119736\n",
      "batch 1862: loss 0.047701\n",
      "batch 1863: loss 0.058100\n",
      "batch 1864: loss 0.066825\n",
      "batch 1865: loss 0.175706\n",
      "batch 1866: loss 0.038083\n",
      "batch 1867: loss 0.049516\n",
      "batch 1868: loss 0.076573\n",
      "batch 1869: loss 0.073190\n",
      "batch 1870: loss 0.116126\n",
      "batch 1871: loss 0.103698\n",
      "batch 1872: loss 0.075914\n",
      "batch 1873: loss 0.109843\n",
      "batch 1874: loss 0.064525\n",
      "batch 1875: loss 0.082276\n",
      "batch 1876: loss 0.065055\n",
      "batch 1877: loss 0.059887\n",
      "batch 1878: loss 0.067198\n",
      "batch 1879: loss 0.046554\n",
      "batch 1880: loss 0.073522\n",
      "batch 1881: loss 0.048624\n",
      "batch 1882: loss 0.046097\n",
      "batch 1883: loss 0.119804\n",
      "batch 1884: loss 0.098072\n",
      "batch 1885: loss 0.078472\n",
      "batch 1886: loss 0.068456\n",
      "batch 1887: loss 0.171102\n",
      "batch 1888: loss 0.257440\n",
      "batch 1889: loss 0.089676\n",
      "batch 1890: loss 0.085784\n",
      "batch 1891: loss 0.065502\n",
      "batch 1892: loss 0.033200\n",
      "batch 1893: loss 0.118647\n",
      "batch 1894: loss 0.052309\n",
      "batch 1895: loss 0.077488\n",
      "batch 1896: loss 0.096008\n",
      "batch 1897: loss 0.043759\n",
      "batch 1898: loss 0.134049\n",
      "batch 1899: loss 0.102235\n",
      "batch 1900: loss 0.100450\n",
      "batch 1901: loss 0.095242\n",
      "batch 1902: loss 0.065200\n",
      "batch 1903: loss 0.090595\n",
      "batch 1904: loss 0.102116\n",
      "batch 1905: loss 0.088300\n",
      "batch 1906: loss 0.035749\n",
      "batch 1907: loss 0.043989\n",
      "batch 1908: loss 0.060732\n",
      "batch 1909: loss 0.089717\n",
      "batch 1910: loss 0.047440\n",
      "batch 1911: loss 0.066878\n",
      "batch 1912: loss 0.100713\n",
      "batch 1913: loss 0.050753\n",
      "batch 1914: loss 0.096268\n",
      "batch 1915: loss 0.053298\n",
      "batch 1916: loss 0.063989\n",
      "batch 1917: loss 0.052677\n",
      "batch 1918: loss 0.209077\n",
      "batch 1919: loss 0.182195\n",
      "batch 1920: loss 0.065571\n",
      "batch 1921: loss 0.086749\n",
      "batch 1922: loss 0.159919\n",
      "batch 1923: loss 0.166417\n",
      "batch 1924: loss 0.141371\n",
      "batch 1925: loss 0.063366\n",
      "batch 1926: loss 0.061955\n",
      "batch 1927: loss 0.098436\n",
      "batch 1928: loss 0.046039\n",
      "batch 1929: loss 0.055936\n",
      "batch 1930: loss 0.062981\n",
      "batch 1931: loss 0.123170\n",
      "batch 1932: loss 0.119912\n",
      "batch 1933: loss 0.087111\n",
      "batch 1934: loss 0.144060\n",
      "batch 1935: loss 0.085838\n",
      "batch 1936: loss 0.075060\n",
      "batch 1937: loss 0.046770\n",
      "batch 1938: loss 0.159575\n",
      "batch 1939: loss 0.115248\n",
      "batch 1940: loss 0.063709\n",
      "batch 1941: loss 0.067307\n",
      "batch 1942: loss 0.073278\n",
      "batch 1943: loss 0.081033\n",
      "batch 1944: loss 0.083552\n",
      "batch 1945: loss 0.060535\n",
      "batch 1946: loss 0.098844\n",
      "batch 1947: loss 0.079304\n",
      "batch 1948: loss 0.066656\n",
      "batch 1949: loss 0.160517\n",
      "batch 1950: loss 0.135169\n",
      "batch 1951: loss 0.066482\n",
      "batch 1952: loss 0.074475\n",
      "batch 1953: loss 0.127682\n",
      "batch 1954: loss 0.126148\n",
      "batch 1955: loss 0.150361\n",
      "batch 1956: loss 0.077524\n",
      "batch 1957: loss 0.117538\n",
      "batch 1958: loss 0.132949\n",
      "batch 1959: loss 0.089429\n",
      "batch 1960: loss 0.087994\n",
      "batch 1961: loss 0.052388\n",
      "batch 1962: loss 0.049277\n",
      "batch 1963: loss 0.175398\n",
      "batch 1964: loss 0.150729\n",
      "batch 1965: loss 0.024429\n",
      "batch 1966: loss 0.083849\n",
      "batch 1967: loss 0.033753\n",
      "batch 1968: loss 0.069473\n",
      "batch 1969: loss 0.104076\n",
      "batch 1970: loss 0.063870\n",
      "batch 1971: loss 0.085405\n",
      "batch 1972: loss 0.094193\n",
      "batch 1973: loss 0.090216\n",
      "batch 1974: loss 0.042864\n",
      "batch 1975: loss 0.136167\n",
      "batch 1976: loss 0.090161\n",
      "batch 1977: loss 0.128079\n",
      "batch 1978: loss 0.064214\n",
      "batch 1979: loss 0.074788\n",
      "batch 1980: loss 0.092454\n",
      "batch 1981: loss 0.034044\n",
      "batch 1982: loss 0.083640\n",
      "batch 1983: loss 0.162233\n",
      "batch 1984: loss 0.107630\n",
      "batch 1985: loss 0.047494\n",
      "batch 1986: loss 0.111569\n",
      "batch 1987: loss 0.062334\n",
      "batch 1988: loss 0.072511\n",
      "batch 1989: loss 0.084083\n",
      "batch 1990: loss 0.113064\n",
      "batch 1991: loss 0.052988\n",
      "batch 1992: loss 0.069069\n",
      "batch 1993: loss 0.044030\n",
      "batch 1994: loss 0.146043\n",
      "batch 1995: loss 0.080730\n",
      "batch 1996: loss 0.085460\n",
      "batch 1997: loss 0.053832\n",
      "batch 1998: loss 0.108818\n",
      "batch 1999: loss 0.037826\n",
      "batch 2000: loss 0.112112\n",
      "batch 2001: loss 0.104035\n",
      "batch 2002: loss 0.163023\n",
      "batch 2003: loss 0.088835\n",
      "batch 2004: loss 0.058022\n",
      "batch 2005: loss 0.058843\n",
      "batch 2006: loss 0.121424\n",
      "batch 2007: loss 0.117678\n",
      "batch 2008: loss 0.114453\n",
      "batch 2009: loss 0.115587\n",
      "batch 2010: loss 0.099022\n",
      "batch 2011: loss 0.054929\n",
      "batch 2012: loss 0.098064\n",
      "batch 2013: loss 0.081154\n",
      "batch 2014: loss 0.087603\n",
      "batch 2015: loss 0.075565\n",
      "batch 2016: loss 0.056434\n",
      "batch 2017: loss 0.110549\n",
      "batch 2018: loss 0.046503\n",
      "batch 2019: loss 0.074578\n",
      "batch 2020: loss 0.120259\n",
      "batch 2021: loss 0.084189\n",
      "batch 2022: loss 0.060129\n",
      "batch 2023: loss 0.055748\n",
      "batch 2024: loss 0.051512\n",
      "batch 2025: loss 0.144110\n",
      "batch 2026: loss 0.078795\n",
      "batch 2027: loss 0.112696\n",
      "batch 2028: loss 0.053509\n",
      "batch 2029: loss 0.085475\n",
      "batch 2030: loss 0.094952\n",
      "batch 2031: loss 0.092088\n",
      "batch 2032: loss 0.092185\n",
      "batch 2033: loss 0.066847\n",
      "batch 2034: loss 0.085467\n",
      "batch 2035: loss 0.058038\n",
      "batch 2036: loss 0.050315\n",
      "batch 2037: loss 0.101978\n",
      "batch 2038: loss 0.064920\n",
      "batch 2039: loss 0.034488\n",
      "batch 2040: loss 0.088687\n",
      "batch 2041: loss 0.046606\n",
      "batch 2042: loss 0.123587\n",
      "batch 2043: loss 0.069637\n",
      "batch 2044: loss 0.103218\n",
      "batch 2045: loss 0.061562\n",
      "batch 2046: loss 0.025252\n",
      "batch 2047: loss 0.053336\n",
      "batch 2048: loss 0.125635\n",
      "batch 2049: loss 0.110964\n",
      "batch 2050: loss 0.029469\n",
      "batch 2051: loss 0.102540\n",
      "batch 2052: loss 0.099119\n",
      "batch 2053: loss 0.045851\n",
      "batch 2054: loss 0.023517\n",
      "batch 2055: loss 0.104459\n",
      "batch 2056: loss 0.067564\n",
      "batch 2057: loss 0.061514\n",
      "batch 2058: loss 0.075482\n",
      "batch 2059: loss 0.134507\n",
      "batch 2060: loss 0.044333\n",
      "batch 2061: loss 0.057427\n",
      "batch 2062: loss 0.092610\n",
      "batch 2063: loss 0.047317\n",
      "batch 2064: loss 0.073135\n",
      "batch 2065: loss 0.100858\n",
      "batch 2066: loss 0.062721\n",
      "batch 2067: loss 0.128486\n",
      "batch 2068: loss 0.066799\n",
      "batch 2069: loss 0.096321\n",
      "batch 2070: loss 0.152077\n",
      "batch 2071: loss 0.164094\n",
      "batch 2072: loss 0.096774\n",
      "batch 2073: loss 0.041792\n",
      "batch 2074: loss 0.042350\n",
      "batch 2075: loss 0.073257\n",
      "batch 2076: loss 0.100010\n",
      "batch 2077: loss 0.097939\n",
      "batch 2078: loss 0.106794\n",
      "batch 2079: loss 0.031727\n",
      "batch 2080: loss 0.087046\n",
      "batch 2081: loss 0.101703\n",
      "batch 2082: loss 0.083990\n",
      "batch 2083: loss 0.098028\n",
      "batch 2084: loss 0.073470\n",
      "batch 2085: loss 0.046239\n",
      "batch 2086: loss 0.035065\n",
      "batch 2087: loss 0.107444\n",
      "batch 2088: loss 0.100304\n",
      "batch 2089: loss 0.027239\n",
      "batch 2090: loss 0.039966\n",
      "batch 2091: loss 0.039857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2092: loss 0.071801\n",
      "batch 2093: loss 0.038141\n",
      "batch 2094: loss 0.084948\n",
      "batch 2095: loss 0.036102\n",
      "batch 2096: loss 0.038373\n",
      "batch 2097: loss 0.050959\n",
      "batch 2098: loss 0.068948\n",
      "batch 2099: loss 0.119486\n",
      "batch 2100: loss 0.129419\n",
      "batch 2101: loss 0.153248\n",
      "batch 2102: loss 0.138223\n",
      "batch 2103: loss 0.098205\n",
      "batch 2104: loss 0.036412\n",
      "batch 2105: loss 0.069374\n",
      "batch 2106: loss 0.106664\n",
      "batch 2107: loss 0.087354\n",
      "batch 2108: loss 0.060671\n",
      "batch 2109: loss 0.105987\n",
      "batch 2110: loss 0.144447\n",
      "batch 2111: loss 0.092417\n",
      "batch 2112: loss 0.046164\n",
      "batch 2113: loss 0.126687\n",
      "batch 2114: loss 0.035823\n",
      "batch 2115: loss 0.051897\n",
      "batch 2116: loss 0.101288\n",
      "batch 2117: loss 0.111377\n",
      "batch 2118: loss 0.094861\n",
      "batch 2119: loss 0.057322\n",
      "batch 2120: loss 0.061432\n",
      "batch 2121: loss 0.032926\n",
      "batch 2122: loss 0.061255\n",
      "batch 2123: loss 0.080209\n",
      "batch 2124: loss 0.071556\n",
      "batch 2125: loss 0.049604\n",
      "batch 2126: loss 0.065093\n",
      "batch 2127: loss 0.018859\n",
      "batch 2128: loss 0.085539\n",
      "batch 2129: loss 0.102866\n",
      "batch 2130: loss 0.074872\n",
      "batch 2131: loss 0.054237\n",
      "batch 2132: loss 0.063963\n",
      "batch 2133: loss 0.081097\n",
      "batch 2134: loss 0.047520\n",
      "batch 2135: loss 0.107631\n",
      "batch 2136: loss 0.087947\n",
      "batch 2137: loss 0.033853\n",
      "batch 2138: loss 0.047964\n",
      "batch 2139: loss 0.048732\n",
      "batch 2140: loss 0.059773\n",
      "batch 2141: loss 0.066707\n",
      "batch 2142: loss 0.078602\n",
      "batch 2143: loss 0.078950\n",
      "batch 2144: loss 0.084409\n",
      "batch 2145: loss 0.050248\n",
      "batch 2146: loss 0.086354\n",
      "batch 2147: loss 0.100550\n",
      "batch 2148: loss 0.063881\n",
      "batch 2149: loss 0.069507\n",
      "batch 2150: loss 0.082158\n",
      "batch 2151: loss 0.088284\n",
      "batch 2152: loss 0.105057\n",
      "batch 2153: loss 0.070042\n",
      "batch 2154: loss 0.123165\n",
      "batch 2155: loss 0.086648\n",
      "batch 2156: loss 0.116663\n",
      "batch 2157: loss 0.064245\n",
      "batch 2158: loss 0.149799\n",
      "batch 2159: loss 0.041429\n",
      "batch 2160: loss 0.097497\n",
      "batch 2161: loss 0.073552\n",
      "batch 2162: loss 0.039133\n",
      "batch 2163: loss 0.102612\n",
      "batch 2164: loss 0.116035\n",
      "batch 2165: loss 0.097684\n",
      "batch 2166: loss 0.119728\n",
      "batch 2167: loss 0.059538\n",
      "batch 2168: loss 0.034869\n",
      "batch 2169: loss 0.086906\n",
      "batch 2170: loss 0.062662\n",
      "batch 2171: loss 0.033899\n",
      "batch 2172: loss 0.086363\n",
      "batch 2173: loss 0.116838\n",
      "batch 2174: loss 0.177598\n",
      "batch 2175: loss 0.101382\n",
      "batch 2176: loss 0.046398\n",
      "batch 2177: loss 0.042715\n",
      "batch 2178: loss 0.116726\n",
      "batch 2179: loss 0.098768\n",
      "batch 2180: loss 0.106393\n",
      "batch 2181: loss 0.042303\n",
      "batch 2182: loss 0.049793\n",
      "batch 2183: loss 0.141335\n",
      "batch 2184: loss 0.071565\n",
      "batch 2185: loss 0.152899\n",
      "batch 2186: loss 0.097272\n",
      "batch 2187: loss 0.055480\n",
      "batch 2188: loss 0.102642\n",
      "batch 2189: loss 0.064335\n",
      "batch 2190: loss 0.018630\n",
      "batch 2191: loss 0.097163\n",
      "batch 2192: loss 0.046585\n",
      "batch 2193: loss 0.062798\n",
      "batch 2194: loss 0.093287\n",
      "batch 2195: loss 0.040453\n",
      "batch 2196: loss 0.061378\n",
      "batch 2197: loss 0.050680\n",
      "batch 2198: loss 0.082458\n",
      "batch 2199: loss 0.062814\n",
      "batch 2200: loss 0.101001\n",
      "batch 2201: loss 0.042403\n",
      "batch 2202: loss 0.044411\n",
      "batch 2203: loss 0.047054\n",
      "batch 2204: loss 0.130734\n",
      "batch 2205: loss 0.112898\n",
      "batch 2206: loss 0.180644\n",
      "batch 2207: loss 0.071358\n",
      "batch 2208: loss 0.051682\n",
      "batch 2209: loss 0.157339\n",
      "batch 2210: loss 0.099707\n",
      "batch 2211: loss 0.092954\n",
      "batch 2212: loss 0.178261\n",
      "batch 2213: loss 0.080486\n",
      "batch 2214: loss 0.051466\n",
      "batch 2215: loss 0.042267\n",
      "batch 2216: loss 0.062937\n",
      "batch 2217: loss 0.121437\n",
      "batch 2218: loss 0.210532\n",
      "batch 2219: loss 0.127915\n",
      "batch 2220: loss 0.064968\n",
      "batch 2221: loss 0.112273\n",
      "batch 2222: loss 0.095730\n",
      "batch 2223: loss 0.114703\n",
      "batch 2224: loss 0.062181\n",
      "batch 2225: loss 0.106842\n",
      "batch 2226: loss 0.069661\n",
      "batch 2227: loss 0.037252\n",
      "batch 2228: loss 0.083056\n",
      "batch 2229: loss 0.046660\n",
      "batch 2230: loss 0.139795\n",
      "batch 2231: loss 0.060203\n",
      "batch 2232: loss 0.060576\n",
      "batch 2233: loss 0.069323\n",
      "batch 2234: loss 0.040028\n",
      "batch 2235: loss 0.059275\n",
      "batch 2236: loss 0.111093\n",
      "batch 2237: loss 0.076420\n",
      "batch 2238: loss 0.092197\n",
      "batch 2239: loss 0.059946\n",
      "batch 2240: loss 0.143409\n",
      "batch 2241: loss 0.065078\n",
      "batch 2242: loss 0.057255\n",
      "batch 2243: loss 0.039853\n",
      "batch 2244: loss 0.091618\n",
      "batch 2245: loss 0.063521\n",
      "batch 2246: loss 0.056331\n",
      "batch 2247: loss 0.046089\n",
      "batch 2248: loss 0.038567\n",
      "batch 2249: loss 0.066532\n",
      "batch 2250: loss 0.072019\n",
      "batch 2251: loss 0.066961\n",
      "batch 2252: loss 0.086724\n",
      "batch 2253: loss 0.097708\n",
      "batch 2254: loss 0.080371\n",
      "batch 2255: loss 0.068596\n",
      "batch 2256: loss 0.029529\n",
      "batch 2257: loss 0.040285\n",
      "batch 2258: loss 0.067217\n",
      "batch 2259: loss 0.059737\n",
      "batch 2260: loss 0.074945\n",
      "batch 2261: loss 0.067973\n",
      "batch 2262: loss 0.076088\n",
      "batch 2263: loss 0.090344\n",
      "batch 2264: loss 0.077578\n",
      "batch 2265: loss 0.040254\n",
      "batch 2266: loss 0.026250\n",
      "batch 2267: loss 0.057914\n",
      "batch 2268: loss 0.109520\n",
      "batch 2269: loss 0.033621\n",
      "batch 2270: loss 0.041351\n",
      "batch 2271: loss 0.059456\n",
      "batch 2272: loss 0.090115\n",
      "batch 2273: loss 0.098095\n",
      "batch 2274: loss 0.084454\n",
      "batch 2275: loss 0.075476\n",
      "batch 2276: loss 0.073923\n",
      "batch 2277: loss 0.048451\n",
      "batch 2278: loss 0.096259\n",
      "batch 2279: loss 0.059713\n",
      "batch 2280: loss 0.043618\n",
      "batch 2281: loss 0.042980\n",
      "batch 2282: loss 0.043294\n",
      "batch 2283: loss 0.040400\n",
      "batch 2284: loss 0.069985\n",
      "batch 2285: loss 0.034895\n",
      "batch 2286: loss 0.068116\n",
      "batch 2287: loss 0.088620\n",
      "batch 2288: loss 0.054078\n",
      "batch 2289: loss 0.084307\n",
      "batch 2290: loss 0.031139\n",
      "batch 2291: loss 0.085825\n",
      "batch 2292: loss 0.075619\n",
      "batch 2293: loss 0.086368\n",
      "batch 2294: loss 0.136962\n",
      "batch 2295: loss 0.130661\n",
      "batch 2296: loss 0.066619\n",
      "batch 2297: loss 0.096638\n",
      "batch 2298: loss 0.076122\n",
      "batch 2299: loss 0.028905\n",
      "batch 2300: loss 0.112546\n",
      "batch 2301: loss 0.098699\n",
      "batch 2302: loss 0.033742\n",
      "batch 2303: loss 0.104450\n",
      "batch 2304: loss 0.132609\n",
      "batch 2305: loss 0.063545\n",
      "batch 2306: loss 0.103957\n",
      "batch 2307: loss 0.102518\n",
      "batch 2308: loss 0.115884\n",
      "batch 2309: loss 0.052439\n",
      "batch 2310: loss 0.073230\n",
      "batch 2311: loss 0.098473\n",
      "batch 2312: loss 0.062764\n",
      "batch 2313: loss 0.094338\n",
      "batch 2314: loss 0.067938\n",
      "batch 2315: loss 0.203251\n",
      "batch 2316: loss 0.081369\n",
      "batch 2317: loss 0.127107\n",
      "batch 2318: loss 0.041881\n",
      "batch 2319: loss 0.058990\n",
      "batch 2320: loss 0.069202\n",
      "batch 2321: loss 0.041194\n",
      "batch 2322: loss 0.076058\n",
      "batch 2323: loss 0.118824\n",
      "batch 2324: loss 0.071293\n",
      "batch 2325: loss 0.189942\n",
      "batch 2326: loss 0.047856\n",
      "batch 2327: loss 0.101356\n",
      "batch 2328: loss 0.061764\n",
      "batch 2329: loss 0.064913\n",
      "batch 2330: loss 0.082616\n",
      "batch 2331: loss 0.105230\n",
      "batch 2332: loss 0.085196\n",
      "batch 2333: loss 0.032116\n",
      "batch 2334: loss 0.118536\n",
      "batch 2335: loss 0.126683\n",
      "batch 2336: loss 0.082208\n",
      "batch 2337: loss 0.150347\n",
      "batch 2338: loss 0.081771\n",
      "batch 2339: loss 0.059017\n",
      "test accuracy: 0.971054\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    \n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 調整學習率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.368104\n",
      "batch 1: loss 7.012143\n",
      "batch 2: loss 5.887691\n",
      "batch 3: loss 4.617076\n",
      "batch 4: loss 3.871229\n",
      "batch 5: loss 2.760186\n",
      "batch 6: loss 2.907932\n",
      "batch 7: loss 1.875108\n",
      "batch 8: loss 1.109124\n",
      "batch 9: loss 1.068275\n",
      "batch 10: loss 1.228499\n",
      "batch 11: loss 0.862108\n",
      "batch 12: loss 1.013499\n",
      "batch 13: loss 0.850300\n",
      "batch 14: loss 0.779070\n",
      "batch 15: loss 0.818109\n",
      "batch 16: loss 0.650978\n",
      "batch 17: loss 0.977911\n",
      "batch 18: loss 0.715714\n",
      "batch 19: loss 0.892761\n",
      "batch 20: loss 0.444570\n",
      "batch 21: loss 0.776119\n",
      "batch 22: loss 0.646434\n",
      "batch 23: loss 0.703823\n",
      "batch 24: loss 0.490481\n",
      "batch 25: loss 0.545381\n",
      "batch 26: loss 0.431673\n",
      "batch 27: loss 0.785677\n",
      "batch 28: loss 0.684091\n",
      "batch 29: loss 0.565225\n",
      "batch 30: loss 0.431843\n",
      "batch 31: loss 0.585828\n",
      "batch 32: loss 0.509517\n",
      "batch 33: loss 0.243271\n",
      "batch 34: loss 0.439721\n",
      "batch 35: loss 0.506194\n",
      "batch 36: loss 0.431115\n",
      "batch 37: loss 0.367034\n",
      "batch 38: loss 0.383039\n",
      "batch 39: loss 0.268904\n",
      "batch 40: loss 0.566544\n",
      "batch 41: loss 0.326631\n",
      "batch 42: loss 0.293959\n",
      "batch 43: loss 0.434313\n",
      "batch 44: loss 0.404810\n",
      "batch 45: loss 0.347143\n",
      "batch 46: loss 0.246807\n",
      "batch 47: loss 0.336320\n",
      "batch 48: loss 0.531433\n",
      "batch 49: loss 0.393964\n",
      "batch 50: loss 0.349748\n",
      "batch 51: loss 0.633102\n",
      "batch 52: loss 0.377831\n",
      "batch 53: loss 0.295863\n",
      "batch 54: loss 0.354045\n",
      "batch 55: loss 0.491812\n",
      "batch 56: loss 0.344435\n",
      "batch 57: loss 0.262906\n",
      "batch 58: loss 0.413625\n",
      "batch 59: loss 0.439102\n",
      "batch 60: loss 0.318731\n",
      "batch 61: loss 0.481333\n",
      "batch 62: loss 0.314929\n",
      "batch 63: loss 0.285180\n",
      "batch 64: loss 0.417133\n",
      "batch 65: loss 0.355291\n",
      "batch 66: loss 0.221395\n",
      "batch 67: loss 0.311935\n",
      "batch 68: loss 0.259937\n",
      "batch 69: loss 0.275680\n",
      "batch 70: loss 0.315458\n",
      "batch 71: loss 0.423700\n",
      "batch 72: loss 0.396270\n",
      "batch 73: loss 0.418594\n",
      "batch 74: loss 0.412453\n",
      "batch 75: loss 0.241488\n",
      "batch 76: loss 0.437954\n",
      "batch 77: loss 0.331735\n",
      "batch 78: loss 0.293821\n",
      "batch 79: loss 0.232783\n",
      "batch 80: loss 0.220496\n",
      "batch 81: loss 0.447743\n",
      "batch 82: loss 0.499985\n",
      "batch 83: loss 0.297485\n",
      "batch 84: loss 0.317755\n",
      "batch 85: loss 0.325884\n",
      "batch 86: loss 0.250507\n",
      "batch 87: loss 0.351362\n",
      "batch 88: loss 0.363512\n",
      "batch 89: loss 0.528149\n",
      "batch 90: loss 0.250789\n",
      "batch 91: loss 0.399532\n",
      "batch 92: loss 0.395762\n",
      "batch 93: loss 0.234893\n",
      "batch 94: loss 0.270538\n",
      "batch 95: loss 0.379922\n",
      "batch 96: loss 0.406139\n",
      "batch 97: loss 0.162479\n",
      "batch 98: loss 0.338537\n",
      "batch 99: loss 0.143375\n",
      "batch 100: loss 0.243107\n",
      "batch 101: loss 0.329913\n",
      "batch 102: loss 0.199345\n",
      "batch 103: loss 0.314150\n",
      "batch 104: loss 0.316903\n",
      "batch 105: loss 0.404740\n",
      "batch 106: loss 0.410152\n",
      "batch 107: loss 0.182183\n",
      "batch 108: loss 0.359803\n",
      "batch 109: loss 0.279180\n",
      "batch 110: loss 0.385085\n",
      "batch 111: loss 0.197134\n",
      "batch 112: loss 0.249287\n",
      "batch 113: loss 0.267488\n",
      "batch 114: loss 0.236644\n",
      "batch 115: loss 0.232870\n",
      "batch 116: loss 0.212367\n",
      "batch 117: loss 0.276447\n",
      "batch 118: loss 0.375378\n",
      "batch 119: loss 0.196232\n",
      "batch 120: loss 0.221548\n",
      "batch 121: loss 0.255355\n",
      "batch 122: loss 0.447329\n",
      "batch 123: loss 0.336965\n",
      "batch 124: loss 0.437209\n",
      "batch 125: loss 0.208981\n",
      "batch 126: loss 0.320825\n",
      "batch 127: loss 0.321236\n",
      "batch 128: loss 0.315939\n",
      "batch 129: loss 0.420956\n",
      "batch 130: loss 0.395143\n",
      "batch 131: loss 0.377153\n",
      "batch 132: loss 0.290091\n",
      "batch 133: loss 0.215373\n",
      "batch 134: loss 0.384747\n",
      "batch 135: loss 0.268009\n",
      "batch 136: loss 0.329575\n",
      "batch 137: loss 0.287456\n",
      "batch 138: loss 0.300226\n",
      "batch 139: loss 0.360556\n",
      "batch 140: loss 0.292157\n",
      "batch 141: loss 0.348018\n",
      "batch 142: loss 0.241831\n",
      "batch 143: loss 0.308091\n",
      "batch 144: loss 0.252019\n",
      "batch 145: loss 0.221166\n",
      "batch 146: loss 0.212441\n",
      "batch 147: loss 0.443845\n",
      "batch 148: loss 0.245372\n",
      "batch 149: loss 0.322355\n",
      "batch 150: loss 0.356069\n",
      "batch 151: loss 0.165124\n",
      "batch 152: loss 0.333833\n",
      "batch 153: loss 0.295307\n",
      "batch 154: loss 0.311339\n",
      "batch 155: loss 0.307038\n",
      "batch 156: loss 0.370113\n",
      "batch 157: loss 0.286486\n",
      "batch 158: loss 0.299306\n",
      "batch 159: loss 0.401904\n",
      "batch 160: loss 0.239207\n",
      "batch 161: loss 0.216993\n",
      "batch 162: loss 0.187145\n",
      "batch 163: loss 0.328547\n",
      "batch 164: loss 0.350832\n",
      "batch 165: loss 0.259589\n",
      "batch 166: loss 0.157533\n",
      "batch 167: loss 0.178448\n",
      "batch 168: loss 0.311584\n",
      "batch 169: loss 0.386339\n",
      "batch 170: loss 0.326899\n",
      "batch 171: loss 0.182522\n",
      "batch 172: loss 0.185651\n",
      "batch 173: loss 0.442518\n",
      "batch 174: loss 0.384593\n",
      "batch 175: loss 0.245918\n",
      "batch 176: loss 0.334434\n",
      "batch 177: loss 0.286113\n",
      "batch 178: loss 0.278881\n",
      "batch 179: loss 0.330878\n",
      "batch 180: loss 0.351151\n",
      "batch 181: loss 0.454377\n",
      "batch 182: loss 0.452947\n",
      "batch 183: loss 0.277326\n",
      "batch 184: loss 0.327627\n",
      "batch 185: loss 0.217716\n",
      "batch 186: loss 0.243777\n",
      "batch 187: loss 0.218997\n",
      "batch 188: loss 0.174951\n",
      "batch 189: loss 0.307166\n",
      "batch 190: loss 0.287712\n",
      "batch 191: loss 0.311157\n",
      "batch 192: loss 0.327625\n",
      "batch 193: loss 0.239421\n",
      "batch 194: loss 0.286180\n",
      "batch 195: loss 0.179141\n",
      "batch 196: loss 0.109771\n",
      "batch 197: loss 0.278760\n",
      "batch 198: loss 0.357629\n",
      "batch 199: loss 0.242208\n",
      "batch 200: loss 0.391282\n",
      "batch 201: loss 0.431215\n",
      "batch 202: loss 0.287579\n",
      "batch 203: loss 0.284851\n",
      "batch 204: loss 0.304896\n",
      "batch 205: loss 0.220992\n",
      "batch 206: loss 0.240644\n",
      "batch 207: loss 0.458234\n",
      "batch 208: loss 0.215097\n",
      "batch 209: loss 0.282720\n",
      "batch 210: loss 0.303517\n",
      "batch 211: loss 0.281628\n",
      "batch 212: loss 0.398354\n",
      "batch 213: loss 0.212147\n",
      "batch 214: loss 0.237816\n",
      "batch 215: loss 0.228179\n",
      "batch 216: loss 0.196105\n",
      "batch 217: loss 0.216917\n",
      "batch 218: loss 0.252749\n",
      "batch 219: loss 0.254453\n",
      "batch 220: loss 0.238182\n",
      "batch 221: loss 0.347979\n",
      "batch 222: loss 0.313064\n",
      "batch 223: loss 0.361165\n",
      "batch 224: loss 0.224572\n",
      "batch 225: loss 0.219632\n",
      "batch 226: loss 0.172173\n",
      "batch 227: loss 0.253995\n",
      "batch 228: loss 0.216584\n",
      "batch 229: loss 0.336647\n",
      "batch 230: loss 0.255683\n",
      "batch 231: loss 0.207985\n",
      "batch 232: loss 0.323192\n",
      "batch 233: loss 0.302184\n",
      "batch 234: loss 0.198608\n",
      "batch 235: loss 0.253395\n",
      "batch 236: loss 0.320636\n",
      "batch 237: loss 0.145223\n",
      "batch 238: loss 0.324242\n",
      "batch 239: loss 0.230937\n",
      "batch 240: loss 0.301052\n",
      "batch 241: loss 0.198742\n",
      "batch 242: loss 0.510204\n",
      "batch 243: loss 0.220936\n",
      "batch 244: loss 0.218330\n",
      "batch 245: loss 0.370184\n",
      "batch 246: loss 0.246395\n",
      "batch 247: loss 0.342022\n",
      "batch 248: loss 0.246035\n",
      "batch 249: loss 0.267041\n",
      "batch 250: loss 0.255109\n",
      "batch 251: loss 0.377604\n",
      "batch 252: loss 0.341248\n",
      "batch 253: loss 0.388824\n",
      "batch 254: loss 0.405839\n",
      "batch 255: loss 0.309570\n",
      "batch 256: loss 0.239312\n",
      "batch 257: loss 0.252935\n",
      "batch 258: loss 0.175599\n",
      "batch 259: loss 0.191919\n",
      "batch 260: loss 0.208220\n",
      "batch 261: loss 0.371185\n",
      "batch 262: loss 0.216585\n",
      "batch 263: loss 0.425634\n",
      "batch 264: loss 0.264936\n",
      "batch 265: loss 0.161161\n",
      "batch 266: loss 0.176224\n",
      "batch 267: loss 0.223308\n",
      "batch 268: loss 0.309720\n",
      "batch 269: loss 0.235544\n",
      "batch 270: loss 0.275222\n",
      "batch 271: loss 0.227239\n",
      "batch 272: loss 0.355978\n",
      "batch 273: loss 0.287974\n",
      "batch 274: loss 0.233025\n",
      "batch 275: loss 0.297178\n",
      "batch 276: loss 0.151328\n",
      "batch 277: loss 0.276620\n",
      "batch 278: loss 0.310162\n",
      "batch 279: loss 0.161141\n",
      "batch 280: loss 0.223956\n",
      "batch 281: loss 0.129381\n",
      "batch 282: loss 0.155012\n",
      "batch 283: loss 0.220508\n",
      "batch 284: loss 0.360409\n",
      "batch 285: loss 0.315329\n",
      "batch 286: loss 0.305038\n",
      "batch 287: loss 0.193275\n",
      "batch 288: loss 0.342033\n",
      "batch 289: loss 0.399427\n",
      "batch 290: loss 0.321892\n",
      "batch 291: loss 0.319904\n",
      "batch 292: loss 0.303727\n",
      "batch 293: loss 0.060024\n",
      "batch 294: loss 0.352328\n",
      "batch 295: loss 0.257453\n",
      "batch 296: loss 0.370612\n",
      "batch 297: loss 0.254749\n",
      "batch 298: loss 0.088772\n",
      "batch 299: loss 0.271139\n",
      "batch 300: loss 0.287885\n",
      "batch 301: loss 0.325310\n",
      "batch 302: loss 0.410639\n",
      "batch 303: loss 0.178230\n",
      "batch 304: loss 0.294225\n",
      "batch 305: loss 0.363225\n",
      "batch 306: loss 0.272000\n",
      "batch 307: loss 0.158005\n",
      "batch 308: loss 0.310828\n",
      "batch 309: loss 0.522643\n",
      "batch 310: loss 0.172857\n",
      "batch 311: loss 0.198776\n",
      "batch 312: loss 0.237724\n",
      "batch 313: loss 0.271517\n",
      "batch 314: loss 0.280756\n",
      "batch 315: loss 0.427748\n",
      "batch 316: loss 0.539890\n",
      "batch 317: loss 0.106459\n",
      "batch 318: loss 0.401394\n",
      "batch 319: loss 0.183367\n",
      "batch 320: loss 0.184915\n",
      "batch 321: loss 0.240981\n",
      "batch 322: loss 0.394026\n",
      "batch 323: loss 0.240091\n",
      "batch 324: loss 0.192899\n",
      "batch 325: loss 0.318947\n",
      "batch 326: loss 0.244232\n",
      "batch 327: loss 0.349433\n",
      "batch 328: loss 0.317056\n",
      "batch 329: loss 0.398003\n",
      "batch 330: loss 0.229537\n",
      "batch 331: loss 0.161760\n",
      "batch 332: loss 0.319750\n",
      "batch 333: loss 0.366973\n",
      "batch 334: loss 0.146534\n",
      "batch 335: loss 0.438589\n",
      "batch 336: loss 0.255405\n",
      "batch 337: loss 0.327603\n",
      "batch 338: loss 0.311882\n",
      "batch 339: loss 0.394530\n",
      "batch 340: loss 0.223265\n",
      "batch 341: loss 0.202978\n",
      "batch 342: loss 0.253939\n",
      "batch 343: loss 0.346078\n",
      "batch 344: loss 0.209402\n",
      "batch 345: loss 0.271293\n",
      "batch 346: loss 0.162953\n",
      "batch 347: loss 0.152974\n",
      "batch 348: loss 0.311886\n",
      "batch 349: loss 0.295006\n",
      "batch 350: loss 0.177189\n",
      "batch 351: loss 0.329759\n",
      "batch 352: loss 0.448650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 353: loss 0.163416\n",
      "batch 354: loss 0.354142\n",
      "batch 355: loss 0.268097\n",
      "batch 356: loss 0.198360\n",
      "batch 357: loss 0.255766\n",
      "batch 358: loss 0.218026\n",
      "batch 359: loss 0.275436\n",
      "batch 360: loss 0.284540\n",
      "batch 361: loss 0.361612\n",
      "batch 362: loss 0.400093\n",
      "batch 363: loss 0.138810\n",
      "batch 364: loss 0.133173\n",
      "batch 365: loss 0.188071\n",
      "batch 366: loss 0.289412\n",
      "batch 367: loss 0.198910\n",
      "batch 368: loss 0.445451\n",
      "batch 369: loss 0.233058\n",
      "batch 370: loss 0.402177\n",
      "batch 371: loss 0.210588\n",
      "batch 372: loss 0.218471\n",
      "batch 373: loss 0.293794\n",
      "batch 374: loss 0.107501\n",
      "batch 375: loss 0.450904\n",
      "batch 376: loss 0.157169\n",
      "batch 377: loss 0.458624\n",
      "batch 378: loss 0.326552\n",
      "batch 379: loss 0.326953\n",
      "batch 380: loss 0.881475\n",
      "batch 381: loss 0.250807\n",
      "batch 382: loss 0.193594\n",
      "batch 383: loss 0.156702\n",
      "batch 384: loss 0.310083\n",
      "batch 385: loss 0.324211\n",
      "batch 386: loss 0.300216\n",
      "batch 387: loss 0.339295\n",
      "batch 388: loss 0.319143\n",
      "batch 389: loss 0.348919\n",
      "batch 390: loss 0.496542\n",
      "batch 391: loss 0.270403\n",
      "batch 392: loss 0.257846\n",
      "batch 393: loss 0.322457\n",
      "batch 394: loss 0.275018\n",
      "batch 395: loss 0.139340\n",
      "batch 396: loss 0.430986\n",
      "batch 397: loss 0.277470\n",
      "batch 398: loss 0.240928\n",
      "batch 399: loss 0.324373\n",
      "batch 400: loss 0.282859\n",
      "batch 401: loss 0.193807\n",
      "batch 402: loss 0.189391\n",
      "batch 403: loss 0.192395\n",
      "batch 404: loss 0.324818\n",
      "batch 405: loss 0.274420\n",
      "batch 406: loss 0.223793\n",
      "batch 407: loss 0.261345\n",
      "batch 408: loss 0.296822\n",
      "batch 409: loss 0.321794\n",
      "batch 410: loss 0.377839\n",
      "batch 411: loss 0.210163\n",
      "batch 412: loss 0.537164\n",
      "batch 413: loss 0.310571\n",
      "batch 414: loss 0.185691\n",
      "batch 415: loss 0.356331\n",
      "batch 416: loss 0.286440\n",
      "batch 417: loss 0.349394\n",
      "batch 418: loss 0.312255\n",
      "batch 419: loss 0.183783\n",
      "batch 420: loss 0.128259\n",
      "batch 421: loss 0.247837\n",
      "batch 422: loss 0.211478\n",
      "batch 423: loss 0.306878\n",
      "batch 424: loss 0.280483\n",
      "batch 425: loss 0.096320\n",
      "batch 426: loss 0.232169\n",
      "batch 427: loss 0.300596\n",
      "batch 428: loss 0.221507\n",
      "batch 429: loss 0.186240\n",
      "batch 430: loss 0.124681\n",
      "batch 431: loss 0.232176\n",
      "batch 432: loss 0.193610\n",
      "batch 433: loss 0.187080\n",
      "batch 434: loss 0.215720\n",
      "batch 435: loss 0.207407\n",
      "batch 436: loss 0.310507\n",
      "batch 437: loss 0.236328\n",
      "batch 438: loss 0.220996\n",
      "batch 439: loss 0.184970\n",
      "batch 440: loss 0.302888\n",
      "batch 441: loss 0.306198\n",
      "batch 442: loss 0.159896\n",
      "batch 443: loss 0.216567\n",
      "batch 444: loss 0.278022\n",
      "batch 445: loss 0.177732\n",
      "batch 446: loss 0.173194\n",
      "batch 447: loss 0.263053\n",
      "batch 448: loss 0.268181\n",
      "batch 449: loss 0.318802\n",
      "batch 450: loss 0.184682\n",
      "batch 451: loss 0.158753\n",
      "batch 452: loss 0.202472\n",
      "batch 453: loss 0.477305\n",
      "batch 454: loss 0.273053\n",
      "batch 455: loss 0.343954\n",
      "batch 456: loss 0.404001\n",
      "batch 457: loss 0.493130\n",
      "batch 458: loss 0.228532\n",
      "batch 459: loss 0.108230\n",
      "batch 460: loss 0.307792\n",
      "batch 461: loss 0.206356\n",
      "batch 462: loss 0.331687\n",
      "batch 463: loss 0.262811\n",
      "batch 464: loss 0.220151\n",
      "batch 465: loss 0.243385\n",
      "batch 466: loss 0.347360\n",
      "batch 467: loss 0.186584\n",
      "batch 468: loss 0.214169\n",
      "batch 469: loss 0.422005\n",
      "batch 470: loss 0.090032\n",
      "batch 471: loss 0.259993\n",
      "batch 472: loss 0.307955\n",
      "batch 473: loss 0.249750\n",
      "batch 474: loss 0.254746\n",
      "batch 475: loss 0.153580\n",
      "batch 476: loss 0.262832\n",
      "batch 477: loss 0.235896\n",
      "batch 478: loss 0.396198\n",
      "batch 479: loss 0.195655\n",
      "batch 480: loss 0.202614\n",
      "batch 481: loss 0.145694\n",
      "batch 482: loss 0.194462\n",
      "batch 483: loss 0.247268\n",
      "batch 484: loss 0.226646\n",
      "batch 485: loss 0.211767\n",
      "batch 486: loss 0.227244\n",
      "batch 487: loss 0.331897\n",
      "batch 488: loss 0.159674\n",
      "batch 489: loss 0.265534\n",
      "batch 490: loss 0.167096\n",
      "batch 491: loss 0.182356\n",
      "batch 492: loss 0.121246\n",
      "batch 493: loss 0.305137\n",
      "batch 494: loss 0.232456\n",
      "batch 495: loss 0.104454\n",
      "batch 496: loss 0.315163\n",
      "batch 497: loss 0.333984\n",
      "batch 498: loss 0.156477\n",
      "batch 499: loss 0.212111\n",
      "batch 500: loss 0.166120\n",
      "batch 501: loss 0.531995\n",
      "batch 502: loss 0.230662\n",
      "batch 503: loss 0.315820\n",
      "batch 504: loss 0.329866\n",
      "batch 505: loss 0.274938\n",
      "batch 506: loss 0.139382\n",
      "batch 507: loss 0.447748\n",
      "batch 508: loss 0.306660\n",
      "batch 509: loss 0.152746\n",
      "batch 510: loss 0.366678\n",
      "batch 511: loss 0.159720\n",
      "batch 512: loss 0.170714\n",
      "batch 513: loss 0.175199\n",
      "batch 514: loss 0.379511\n",
      "batch 515: loss 0.262011\n",
      "batch 516: loss 0.403186\n",
      "batch 517: loss 0.150921\n",
      "batch 518: loss 0.292461\n",
      "batch 519: loss 0.409177\n",
      "batch 520: loss 0.138056\n",
      "batch 521: loss 0.493478\n",
      "batch 522: loss 0.277257\n",
      "batch 523: loss 0.210796\n",
      "batch 524: loss 0.227361\n",
      "batch 525: loss 0.364948\n",
      "batch 526: loss 0.260474\n",
      "batch 527: loss 0.331413\n",
      "batch 528: loss 0.279698\n",
      "batch 529: loss 0.315464\n",
      "batch 530: loss 0.279930\n",
      "batch 531: loss 0.338358\n",
      "batch 532: loss 0.249886\n",
      "batch 533: loss 0.252565\n",
      "batch 534: loss 0.234239\n",
      "batch 535: loss 0.426270\n",
      "batch 536: loss 0.309051\n",
      "batch 537: loss 0.308012\n",
      "batch 538: loss 0.322371\n",
      "batch 539: loss 0.312700\n",
      "batch 540: loss 0.220997\n",
      "batch 541: loss 0.176624\n",
      "batch 542: loss 0.496580\n",
      "batch 543: loss 0.346039\n",
      "batch 544: loss 0.249240\n",
      "batch 545: loss 0.217781\n",
      "batch 546: loss 0.164890\n",
      "batch 547: loss 0.279975\n",
      "batch 548: loss 0.248828\n",
      "batch 549: loss 0.233795\n",
      "batch 550: loss 0.407624\n",
      "batch 551: loss 0.251256\n",
      "batch 552: loss 0.279775\n",
      "batch 553: loss 0.254368\n",
      "batch 554: loss 0.269256\n",
      "batch 555: loss 0.343294\n",
      "batch 556: loss 0.208062\n",
      "batch 557: loss 0.287653\n",
      "batch 558: loss 0.216005\n",
      "batch 559: loss 0.154128\n",
      "batch 560: loss 0.456449\n",
      "batch 561: loss 0.229456\n",
      "batch 562: loss 0.402690\n",
      "batch 563: loss 0.356144\n",
      "batch 564: loss 0.343823\n",
      "batch 565: loss 0.266346\n",
      "batch 566: loss 0.115685\n",
      "batch 567: loss 0.181967\n",
      "batch 568: loss 0.545052\n",
      "batch 569: loss 0.375750\n",
      "batch 570: loss 0.345776\n",
      "batch 571: loss 0.228180\n",
      "batch 572: loss 0.243851\n",
      "batch 573: loss 0.190961\n",
      "batch 574: loss 0.148788\n",
      "batch 575: loss 0.407103\n",
      "batch 576: loss 0.248146\n",
      "batch 577: loss 0.178174\n",
      "batch 578: loss 0.181425\n",
      "batch 579: loss 0.274751\n",
      "batch 580: loss 0.314142\n",
      "batch 581: loss 0.208744\n",
      "batch 582: loss 0.383042\n",
      "batch 583: loss 0.131994\n",
      "batch 584: loss 0.405177\n",
      "batch 585: loss 0.279817\n",
      "batch 586: loss 0.236248\n",
      "batch 587: loss 0.410344\n",
      "batch 588: loss 0.506965\n",
      "batch 589: loss 0.276988\n",
      "batch 590: loss 0.175089\n",
      "batch 591: loss 0.282271\n",
      "batch 592: loss 0.172944\n",
      "batch 593: loss 0.327401\n",
      "batch 594: loss 0.244533\n",
      "batch 595: loss 0.259791\n",
      "batch 596: loss 0.447441\n",
      "batch 597: loss 0.431607\n",
      "batch 598: loss 0.139002\n",
      "batch 599: loss 0.170932\n",
      "batch 600: loss 0.375643\n",
      "batch 601: loss 0.313508\n",
      "batch 602: loss 0.300564\n",
      "batch 603: loss 0.266754\n",
      "batch 604: loss 0.175512\n",
      "batch 605: loss 0.136729\n",
      "batch 606: loss 0.267808\n",
      "batch 607: loss 0.197692\n",
      "batch 608: loss 0.244594\n",
      "batch 609: loss 0.138580\n",
      "batch 610: loss 0.304405\n",
      "batch 611: loss 0.241068\n",
      "batch 612: loss 0.276640\n",
      "batch 613: loss 0.275034\n",
      "batch 614: loss 0.285243\n",
      "batch 615: loss 0.210758\n",
      "batch 616: loss 0.300870\n",
      "batch 617: loss 0.121514\n",
      "batch 618: loss 0.264780\n",
      "batch 619: loss 0.197529\n",
      "batch 620: loss 0.396899\n",
      "batch 621: loss 0.183347\n",
      "batch 622: loss 0.197384\n",
      "batch 623: loss 0.579426\n",
      "batch 624: loss 0.237793\n",
      "batch 625: loss 0.281827\n",
      "batch 626: loss 0.187698\n",
      "batch 627: loss 0.289320\n",
      "batch 628: loss 0.317038\n",
      "batch 629: loss 0.217721\n",
      "batch 630: loss 0.276376\n",
      "batch 631: loss 0.248581\n",
      "batch 632: loss 0.147127\n",
      "batch 633: loss 0.261306\n",
      "batch 634: loss 0.349901\n",
      "batch 635: loss 0.142176\n",
      "batch 636: loss 0.142574\n",
      "batch 637: loss 0.235411\n",
      "batch 638: loss 0.335361\n",
      "batch 639: loss 0.375272\n",
      "batch 640: loss 0.362252\n",
      "batch 641: loss 0.354465\n",
      "batch 642: loss 0.215092\n",
      "batch 643: loss 0.224210\n",
      "batch 644: loss 0.330803\n",
      "batch 645: loss 0.154925\n",
      "batch 646: loss 0.269301\n",
      "batch 647: loss 0.334084\n",
      "batch 648: loss 0.058421\n",
      "batch 649: loss 0.139605\n",
      "batch 650: loss 0.298470\n",
      "batch 651: loss 0.128613\n",
      "batch 652: loss 0.551351\n",
      "batch 653: loss 0.123031\n",
      "batch 654: loss 0.177742\n",
      "batch 655: loss 0.354235\n",
      "batch 656: loss 0.255178\n",
      "batch 657: loss 0.182312\n",
      "batch 658: loss 0.238024\n",
      "batch 659: loss 0.189774\n",
      "batch 660: loss 0.241261\n",
      "batch 661: loss 0.173904\n",
      "batch 662: loss 0.250804\n",
      "batch 663: loss 0.193339\n",
      "batch 664: loss 0.269430\n",
      "batch 665: loss 0.213317\n",
      "batch 666: loss 0.299315\n",
      "batch 667: loss 0.104959\n",
      "batch 668: loss 0.262914\n",
      "batch 669: loss 0.332523\n",
      "batch 670: loss 0.182675\n",
      "batch 671: loss 0.098022\n",
      "batch 672: loss 0.359966\n",
      "batch 673: loss 0.181504\n",
      "batch 674: loss 0.262325\n",
      "batch 675: loss 0.352609\n",
      "batch 676: loss 0.233054\n",
      "batch 677: loss 0.274194\n",
      "batch 678: loss 0.225049\n",
      "batch 679: loss 0.352358\n",
      "batch 680: loss 0.155009\n",
      "batch 681: loss 0.177165\n",
      "batch 682: loss 0.146589\n",
      "batch 683: loss 0.398190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 684: loss 0.310706\n",
      "batch 685: loss 0.237497\n",
      "batch 686: loss 0.375533\n",
      "batch 687: loss 0.335631\n",
      "batch 688: loss 0.274833\n",
      "batch 689: loss 0.465101\n",
      "batch 690: loss 0.241185\n",
      "batch 691: loss 0.222943\n",
      "batch 692: loss 0.121247\n",
      "batch 693: loss 0.122871\n",
      "batch 694: loss 0.186677\n",
      "batch 695: loss 0.307108\n",
      "batch 696: loss 0.246741\n",
      "batch 697: loss 0.518447\n",
      "batch 698: loss 0.288144\n",
      "batch 699: loss 0.180171\n",
      "batch 700: loss 0.500083\n",
      "batch 701: loss 0.365927\n",
      "batch 702: loss 0.480527\n",
      "batch 703: loss 0.187013\n",
      "batch 704: loss 0.323829\n",
      "batch 705: loss 0.397115\n",
      "batch 706: loss 0.286706\n",
      "batch 707: loss 0.222229\n",
      "batch 708: loss 0.248538\n",
      "batch 709: loss 0.156586\n",
      "batch 710: loss 0.211095\n",
      "batch 711: loss 0.411186\n",
      "batch 712: loss 0.227208\n",
      "batch 713: loss 0.306504\n",
      "batch 714: loss 0.276108\n",
      "batch 715: loss 0.328496\n",
      "batch 716: loss 0.445611\n",
      "batch 717: loss 0.342279\n",
      "batch 718: loss 0.356238\n",
      "batch 719: loss 0.271200\n",
      "batch 720: loss 0.406238\n",
      "batch 721: loss 0.273367\n",
      "batch 722: loss 0.377769\n",
      "batch 723: loss 0.155629\n",
      "batch 724: loss 0.277864\n",
      "batch 725: loss 0.185251\n",
      "batch 726: loss 0.314360\n",
      "batch 727: loss 0.263133\n",
      "batch 728: loss 0.556748\n",
      "batch 729: loss 0.117099\n",
      "batch 730: loss 0.481885\n",
      "batch 731: loss 0.420921\n",
      "batch 732: loss 0.341029\n",
      "batch 733: loss 0.304919\n",
      "batch 734: loss 0.264280\n",
      "batch 735: loss 0.212536\n",
      "batch 736: loss 0.228712\n",
      "batch 737: loss 0.369649\n",
      "batch 738: loss 0.303761\n",
      "batch 739: loss 0.519927\n",
      "batch 740: loss 0.300704\n",
      "batch 741: loss 0.169983\n",
      "batch 742: loss 0.278621\n",
      "batch 743: loss 0.135887\n",
      "batch 744: loss 0.249602\n",
      "batch 745: loss 0.277172\n",
      "batch 746: loss 0.260699\n",
      "batch 747: loss 0.136270\n",
      "batch 748: loss 0.410898\n",
      "batch 749: loss 0.372842\n",
      "batch 750: loss 0.218240\n",
      "batch 751: loss 0.177000\n",
      "batch 752: loss 0.451574\n",
      "batch 753: loss 0.358419\n",
      "batch 754: loss 0.416390\n",
      "batch 755: loss 0.276536\n",
      "batch 756: loss 0.208161\n",
      "batch 757: loss 0.232268\n",
      "batch 758: loss 0.184388\n",
      "batch 759: loss 0.291819\n",
      "batch 760: loss 0.208039\n",
      "batch 761: loss 0.184430\n",
      "batch 762: loss 0.085053\n",
      "batch 763: loss 0.358033\n",
      "batch 764: loss 0.263350\n",
      "batch 765: loss 0.306018\n",
      "batch 766: loss 0.397103\n",
      "batch 767: loss 0.530824\n",
      "batch 768: loss 0.219399\n",
      "batch 769: loss 0.226397\n",
      "batch 770: loss 0.319045\n",
      "batch 771: loss 0.205676\n",
      "batch 772: loss 0.366462\n",
      "batch 773: loss 0.296583\n",
      "batch 774: loss 0.097963\n",
      "batch 775: loss 0.216838\n",
      "batch 776: loss 0.342320\n",
      "batch 777: loss 0.400164\n",
      "batch 778: loss 0.412868\n",
      "batch 779: loss 0.320225\n",
      "batch 780: loss 0.206202\n",
      "batch 781: loss 0.568125\n",
      "batch 782: loss 0.345138\n",
      "batch 783: loss 0.270492\n",
      "batch 784: loss 0.304731\n",
      "batch 785: loss 0.297206\n",
      "batch 786: loss 0.278338\n",
      "batch 787: loss 0.225554\n",
      "batch 788: loss 0.305580\n",
      "batch 789: loss 0.413130\n",
      "batch 790: loss 0.377706\n",
      "batch 791: loss 0.180863\n",
      "batch 792: loss 0.234719\n",
      "batch 793: loss 0.271653\n",
      "batch 794: loss 0.309067\n",
      "batch 795: loss 0.357972\n",
      "batch 796: loss 0.236611\n",
      "batch 797: loss 0.304064\n",
      "batch 798: loss 0.335572\n",
      "batch 799: loss 0.344894\n",
      "batch 800: loss 0.121899\n",
      "batch 801: loss 0.287799\n",
      "batch 802: loss 0.222651\n",
      "batch 803: loss 0.237616\n",
      "batch 804: loss 0.250715\n",
      "batch 805: loss 0.377499\n",
      "batch 806: loss 0.272705\n",
      "batch 807: loss 0.088806\n",
      "batch 808: loss 0.469052\n",
      "batch 809: loss 0.356789\n",
      "batch 810: loss 0.336210\n",
      "batch 811: loss 0.406082\n",
      "batch 812: loss 0.250076\n",
      "batch 813: loss 0.216186\n",
      "batch 814: loss 0.321293\n",
      "batch 815: loss 0.263890\n",
      "batch 816: loss 0.136985\n",
      "batch 817: loss 0.360429\n",
      "batch 818: loss 0.102482\n",
      "batch 819: loss 0.399868\n",
      "batch 820: loss 0.249266\n",
      "batch 821: loss 0.269657\n",
      "batch 822: loss 0.169890\n",
      "batch 823: loss 0.146682\n",
      "batch 824: loss 0.183952\n",
      "batch 825: loss 0.247355\n",
      "batch 826: loss 0.268445\n",
      "batch 827: loss 0.239572\n",
      "batch 828: loss 0.201650\n",
      "batch 829: loss 0.172166\n",
      "batch 830: loss 0.159698\n",
      "batch 831: loss 0.321941\n",
      "batch 832: loss 0.324179\n",
      "batch 833: loss 0.345159\n",
      "batch 834: loss 0.351367\n",
      "batch 835: loss 0.162202\n",
      "batch 836: loss 0.152719\n",
      "batch 837: loss 0.223446\n",
      "batch 838: loss 0.444252\n",
      "batch 839: loss 0.238046\n",
      "batch 840: loss 0.264289\n",
      "batch 841: loss 0.192680\n",
      "batch 842: loss 0.472575\n",
      "batch 843: loss 0.362746\n",
      "batch 844: loss 0.334457\n",
      "batch 845: loss 0.418928\n",
      "batch 846: loss 0.292746\n",
      "batch 847: loss 0.190048\n",
      "batch 848: loss 0.133538\n",
      "batch 849: loss 0.310067\n",
      "batch 850: loss 0.233149\n",
      "batch 851: loss 0.212673\n",
      "batch 852: loss 0.331826\n",
      "batch 853: loss 0.250813\n",
      "batch 854: loss 0.199371\n",
      "batch 855: loss 0.317679\n",
      "batch 856: loss 0.178747\n",
      "batch 857: loss 0.285944\n",
      "batch 858: loss 0.282658\n",
      "batch 859: loss 0.375437\n",
      "batch 860: loss 0.291056\n",
      "batch 861: loss 0.201931\n",
      "batch 862: loss 0.215118\n",
      "batch 863: loss 0.252111\n",
      "batch 864: loss 0.266353\n",
      "batch 865: loss 0.297872\n",
      "batch 866: loss 0.279871\n",
      "batch 867: loss 0.156686\n",
      "batch 868: loss 0.358533\n",
      "batch 869: loss 0.338376\n",
      "batch 870: loss 0.266588\n",
      "batch 871: loss 0.200952\n",
      "batch 872: loss 0.269972\n",
      "batch 873: loss 0.125514\n",
      "batch 874: loss 0.121317\n",
      "batch 875: loss 0.170824\n",
      "batch 876: loss 0.337344\n",
      "batch 877: loss 0.312017\n",
      "batch 878: loss 0.178476\n",
      "batch 879: loss 0.116866\n",
      "batch 880: loss 0.297135\n",
      "batch 881: loss 0.256632\n",
      "batch 882: loss 0.280903\n",
      "batch 883: loss 0.269212\n",
      "batch 884: loss 0.249755\n",
      "batch 885: loss 0.199083\n",
      "batch 886: loss 0.067824\n",
      "batch 887: loss 0.219638\n",
      "batch 888: loss 0.298764\n",
      "batch 889: loss 0.428259\n",
      "batch 890: loss 0.236920\n",
      "batch 891: loss 0.285715\n",
      "batch 892: loss 0.172513\n",
      "batch 893: loss 0.630071\n",
      "batch 894: loss 0.158172\n",
      "batch 895: loss 0.255728\n",
      "batch 896: loss 0.206917\n",
      "batch 897: loss 0.302407\n",
      "batch 898: loss 0.522281\n",
      "batch 899: loss 0.046745\n",
      "batch 900: loss 0.214191\n",
      "batch 901: loss 0.477079\n",
      "batch 902: loss 0.227121\n",
      "batch 903: loss 0.235318\n",
      "batch 904: loss 0.116211\n",
      "batch 905: loss 0.411433\n",
      "batch 906: loss 0.378106\n",
      "batch 907: loss 0.225135\n",
      "batch 908: loss 0.446614\n",
      "batch 909: loss 0.417180\n",
      "batch 910: loss 0.180419\n",
      "batch 911: loss 0.366489\n",
      "batch 912: loss 0.371432\n",
      "batch 913: loss 0.309454\n",
      "batch 914: loss 0.232152\n",
      "batch 915: loss 0.147327\n",
      "batch 916: loss 0.170288\n",
      "batch 917: loss 0.290372\n",
      "batch 918: loss 0.352171\n",
      "batch 919: loss 0.212418\n",
      "batch 920: loss 0.308675\n",
      "batch 921: loss 0.385437\n",
      "batch 922: loss 0.584253\n",
      "batch 923: loss 0.203820\n",
      "batch 924: loss 0.146308\n",
      "batch 925: loss 0.245768\n",
      "batch 926: loss 0.185348\n",
      "batch 927: loss 0.475594\n",
      "batch 928: loss 0.355910\n",
      "batch 929: loss 0.236342\n",
      "batch 930: loss 0.344622\n",
      "batch 931: loss 0.315473\n",
      "batch 932: loss 0.368015\n",
      "batch 933: loss 0.343816\n",
      "batch 934: loss 0.281422\n",
      "batch 935: loss 0.213239\n",
      "batch 936: loss 0.246067\n",
      "batch 937: loss 0.334944\n",
      "batch 938: loss 0.330432\n",
      "batch 939: loss 0.415529\n",
      "batch 940: loss 0.235223\n",
      "batch 941: loss 0.439918\n",
      "batch 942: loss 0.347051\n",
      "batch 943: loss 0.377505\n",
      "batch 944: loss 0.398259\n",
      "batch 945: loss 0.395355\n",
      "batch 946: loss 0.315882\n",
      "batch 947: loss 0.091289\n",
      "batch 948: loss 0.158574\n",
      "batch 949: loss 0.140430\n",
      "batch 950: loss 0.346590\n",
      "batch 951: loss 0.265302\n",
      "batch 952: loss 0.266602\n",
      "batch 953: loss 0.285213\n",
      "batch 954: loss 0.229188\n",
      "batch 955: loss 0.307455\n",
      "batch 956: loss 0.227970\n",
      "batch 957: loss 0.154110\n",
      "batch 958: loss 0.124980\n",
      "batch 959: loss 0.225802\n",
      "batch 960: loss 0.141137\n",
      "batch 961: loss 0.207887\n",
      "batch 962: loss 0.162456\n",
      "batch 963: loss 0.217469\n",
      "batch 964: loss 0.200986\n",
      "batch 965: loss 0.453593\n",
      "batch 966: loss 0.371828\n",
      "batch 967: loss 0.213784\n",
      "batch 968: loss 0.251668\n",
      "batch 969: loss 0.286249\n",
      "batch 970: loss 0.314705\n",
      "batch 971: loss 0.183117\n",
      "batch 972: loss 0.386835\n",
      "batch 973: loss 0.330931\n",
      "batch 974: loss 0.242763\n",
      "batch 975: loss 0.202625\n",
      "batch 976: loss 0.152702\n",
      "batch 977: loss 0.311069\n",
      "batch 978: loss 0.216335\n",
      "batch 979: loss 0.221391\n",
      "batch 980: loss 0.322417\n",
      "batch 981: loss 0.385781\n",
      "batch 982: loss 0.090134\n",
      "batch 983: loss 0.315187\n",
      "batch 984: loss 0.425336\n",
      "batch 985: loss 0.273384\n",
      "batch 986: loss 0.224438\n",
      "batch 987: loss 0.084287\n",
      "batch 988: loss 0.180332\n",
      "batch 989: loss 0.321338\n",
      "batch 990: loss 0.201087\n",
      "batch 991: loss 0.165334\n",
      "batch 992: loss 0.095657\n",
      "batch 993: loss 0.307184\n",
      "batch 994: loss 0.385283\n",
      "batch 995: loss 0.192288\n",
      "batch 996: loss 0.321247\n",
      "batch 997: loss 0.210668\n",
      "batch 998: loss 0.225740\n",
      "batch 999: loss 0.073895\n",
      "batch 1000: loss 0.274435\n",
      "batch 1001: loss 0.247658\n",
      "batch 1002: loss 0.457188\n",
      "batch 1003: loss 0.300634\n",
      "batch 1004: loss 0.398437\n",
      "batch 1005: loss 0.241985\n",
      "batch 1006: loss 0.109594\n",
      "batch 1007: loss 0.290186\n",
      "batch 1008: loss 0.216212\n",
      "batch 1009: loss 0.324460\n",
      "batch 1010: loss 0.175118\n",
      "batch 1011: loss 0.078368\n",
      "batch 1012: loss 0.340537\n",
      "batch 1013: loss 0.212468\n",
      "batch 1014: loss 0.315820\n",
      "batch 1015: loss 0.267164\n",
      "batch 1016: loss 0.124097\n",
      "batch 1017: loss 0.341516\n",
      "batch 1018: loss 0.208592\n",
      "batch 1019: loss 0.068875\n",
      "batch 1020: loss 0.211564\n",
      "batch 1021: loss 0.387333\n",
      "batch 1022: loss 0.181501\n",
      "batch 1023: loss 0.221773\n",
      "batch 1024: loss 0.219621\n",
      "batch 1025: loss 0.200744\n",
      "batch 1026: loss 0.395536\n",
      "batch 1027: loss 0.214169\n",
      "batch 1028: loss 0.253160\n",
      "batch 1029: loss 0.193595\n",
      "batch 1030: loss 0.282661\n",
      "batch 1031: loss 0.194339\n",
      "batch 1032: loss 0.311820\n",
      "batch 1033: loss 0.240332\n",
      "batch 1034: loss 0.236973\n",
      "batch 1035: loss 0.349596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1036: loss 0.390241\n",
      "batch 1037: loss 0.231809\n",
      "batch 1038: loss 0.406588\n",
      "batch 1039: loss 0.063009\n",
      "batch 1040: loss 0.209590\n",
      "batch 1041: loss 0.272164\n",
      "batch 1042: loss 0.306018\n",
      "batch 1043: loss 0.288138\n",
      "batch 1044: loss 0.272349\n",
      "batch 1045: loss 0.358806\n",
      "batch 1046: loss 0.102268\n",
      "batch 1047: loss 0.292734\n",
      "batch 1048: loss 0.305129\n",
      "batch 1049: loss 0.197297\n",
      "batch 1050: loss 0.348480\n",
      "batch 1051: loss 0.226302\n",
      "batch 1052: loss 0.598089\n",
      "batch 1053: loss 0.270981\n",
      "batch 1054: loss 0.206156\n",
      "batch 1055: loss 0.110184\n",
      "batch 1056: loss 0.280292\n",
      "batch 1057: loss 0.216930\n",
      "batch 1058: loss 0.294430\n",
      "batch 1059: loss 0.196380\n",
      "batch 1060: loss 0.199245\n",
      "batch 1061: loss 0.173690\n",
      "batch 1062: loss 0.176524\n",
      "batch 1063: loss 0.317253\n",
      "batch 1064: loss 0.130994\n",
      "batch 1065: loss 0.170756\n",
      "batch 1066: loss 0.143004\n",
      "batch 1067: loss 0.216916\n",
      "batch 1068: loss 0.261996\n",
      "batch 1069: loss 0.282499\n",
      "batch 1070: loss 0.287657\n",
      "batch 1071: loss 0.440900\n",
      "batch 1072: loss 0.127845\n",
      "batch 1073: loss 0.241863\n",
      "batch 1074: loss 0.124433\n",
      "batch 1075: loss 0.160431\n",
      "batch 1076: loss 0.371367\n",
      "batch 1077: loss 0.160141\n",
      "batch 1078: loss 0.548941\n",
      "batch 1079: loss 0.340438\n",
      "batch 1080: loss 0.222240\n",
      "batch 1081: loss 0.477129\n",
      "batch 1082: loss 0.158190\n",
      "batch 1083: loss 0.175693\n",
      "batch 1084: loss 0.151986\n",
      "batch 1085: loss 0.394454\n",
      "batch 1086: loss 0.570386\n",
      "batch 1087: loss 0.302666\n",
      "batch 1088: loss 0.204590\n",
      "batch 1089: loss 0.247827\n",
      "batch 1090: loss 0.278765\n",
      "batch 1091: loss 0.210507\n",
      "batch 1092: loss 0.331073\n",
      "batch 1093: loss 0.368436\n",
      "batch 1094: loss 0.353446\n",
      "batch 1095: loss 0.254498\n",
      "batch 1096: loss 0.243148\n",
      "batch 1097: loss 0.273168\n",
      "batch 1098: loss 0.440090\n",
      "batch 1099: loss 0.107414\n",
      "batch 1100: loss 0.218870\n",
      "batch 1101: loss 0.162205\n",
      "batch 1102: loss 0.329920\n",
      "batch 1103: loss 0.172872\n",
      "batch 1104: loss 0.330529\n",
      "batch 1105: loss 0.284657\n",
      "batch 1106: loss 0.206650\n",
      "batch 1107: loss 0.356261\n",
      "batch 1108: loss 0.254596\n",
      "batch 1109: loss 0.163635\n",
      "batch 1110: loss 0.333876\n",
      "batch 1111: loss 0.332738\n",
      "batch 1112: loss 0.408771\n",
      "batch 1113: loss 0.364398\n",
      "batch 1114: loss 0.087749\n",
      "batch 1115: loss 0.264853\n",
      "batch 1116: loss 0.120382\n",
      "batch 1117: loss 0.193805\n",
      "batch 1118: loss 0.532924\n",
      "batch 1119: loss 0.173249\n",
      "batch 1120: loss 0.268068\n",
      "batch 1121: loss 0.287586\n",
      "batch 1122: loss 0.502378\n",
      "batch 1123: loss 0.191605\n",
      "batch 1124: loss 0.175454\n",
      "batch 1125: loss 0.242904\n",
      "batch 1126: loss 0.258968\n",
      "batch 1127: loss 0.116024\n",
      "batch 1128: loss 0.273472\n",
      "batch 1129: loss 0.197373\n",
      "batch 1130: loss 0.231566\n",
      "batch 1131: loss 0.203260\n",
      "batch 1132: loss 0.205429\n",
      "batch 1133: loss 0.224239\n",
      "batch 1134: loss 0.407674\n",
      "batch 1135: loss 0.189321\n",
      "batch 1136: loss 0.058831\n",
      "batch 1137: loss 0.130764\n",
      "batch 1138: loss 0.288926\n",
      "batch 1139: loss 0.184118\n",
      "batch 1140: loss 0.253343\n",
      "batch 1141: loss 0.256102\n",
      "batch 1142: loss 0.382611\n",
      "batch 1143: loss 0.475151\n",
      "batch 1144: loss 0.173879\n",
      "batch 1145: loss 0.291640\n",
      "batch 1146: loss 0.155507\n",
      "batch 1147: loss 0.128330\n",
      "batch 1148: loss 0.132951\n",
      "batch 1149: loss 0.377527\n",
      "batch 1150: loss 0.234418\n",
      "batch 1151: loss 0.262323\n",
      "batch 1152: loss 0.201790\n",
      "batch 1153: loss 0.200665\n",
      "batch 1154: loss 0.499858\n",
      "batch 1155: loss 0.405308\n",
      "batch 1156: loss 0.323374\n",
      "batch 1157: loss 0.213060\n",
      "batch 1158: loss 0.273501\n",
      "batch 1159: loss 0.206416\n",
      "batch 1160: loss 0.339376\n",
      "batch 1161: loss 0.175015\n",
      "batch 1162: loss 0.196411\n",
      "batch 1163: loss 0.221917\n",
      "batch 1164: loss 0.261935\n",
      "batch 1165: loss 0.316790\n",
      "batch 1166: loss 0.330840\n",
      "batch 1167: loss 0.206075\n",
      "batch 1168: loss 0.183650\n",
      "batch 1169: loss 0.202283\n",
      "batch 1170: loss 0.284844\n",
      "batch 1171: loss 0.232105\n",
      "batch 1172: loss 0.200704\n",
      "batch 1173: loss 0.158780\n",
      "batch 1174: loss 0.111143\n",
      "batch 1175: loss 0.116842\n",
      "batch 1176: loss 0.563291\n",
      "batch 1177: loss 0.309824\n",
      "batch 1178: loss 0.225403\n",
      "batch 1179: loss 0.343694\n",
      "batch 1180: loss 0.102897\n",
      "batch 1181: loss 0.203690\n",
      "batch 1182: loss 0.173321\n",
      "batch 1183: loss 0.244204\n",
      "batch 1184: loss 0.223530\n",
      "batch 1185: loss 0.260911\n",
      "batch 1186: loss 0.202949\n",
      "batch 1187: loss 0.277902\n",
      "batch 1188: loss 0.206487\n",
      "batch 1189: loss 0.458818\n",
      "batch 1190: loss 0.229665\n",
      "batch 1191: loss 0.200850\n",
      "batch 1192: loss 0.325549\n",
      "batch 1193: loss 0.153012\n",
      "batch 1194: loss 0.303679\n",
      "batch 1195: loss 0.162430\n",
      "batch 1196: loss 0.134067\n",
      "batch 1197: loss 0.208081\n",
      "batch 1198: loss 0.398883\n",
      "batch 1199: loss 0.249134\n",
      "batch 1200: loss 0.293598\n",
      "batch 1201: loss 0.379046\n",
      "batch 1202: loss 0.337939\n",
      "batch 1203: loss 0.184599\n",
      "batch 1204: loss 0.118313\n",
      "batch 1205: loss 0.187992\n",
      "batch 1206: loss 0.192889\n",
      "batch 1207: loss 0.438926\n",
      "batch 1208: loss 0.238515\n",
      "batch 1209: loss 0.425974\n",
      "batch 1210: loss 0.247420\n",
      "batch 1211: loss 0.198695\n",
      "batch 1212: loss 0.252975\n",
      "batch 1213: loss 0.360147\n",
      "batch 1214: loss 0.374607\n",
      "batch 1215: loss 0.273090\n",
      "batch 1216: loss 0.173802\n",
      "batch 1217: loss 0.211161\n",
      "batch 1218: loss 0.341905\n",
      "batch 1219: loss 0.200810\n",
      "batch 1220: loss 0.426010\n",
      "batch 1221: loss 0.256541\n",
      "batch 1222: loss 0.116071\n",
      "batch 1223: loss 0.215405\n",
      "batch 1224: loss 0.337179\n",
      "batch 1225: loss 0.193398\n",
      "batch 1226: loss 0.251717\n",
      "batch 1227: loss 0.088679\n",
      "batch 1228: loss 0.205813\n",
      "batch 1229: loss 0.347037\n",
      "batch 1230: loss 0.246210\n",
      "batch 1231: loss 0.241369\n",
      "batch 1232: loss 0.381004\n",
      "batch 1233: loss 0.204149\n",
      "batch 1234: loss 0.176490\n",
      "batch 1235: loss 0.184206\n",
      "batch 1236: loss 0.244821\n",
      "batch 1237: loss 0.267803\n",
      "batch 1238: loss 0.217548\n",
      "batch 1239: loss 0.115357\n",
      "batch 1240: loss 0.174278\n",
      "batch 1241: loss 0.171668\n",
      "batch 1242: loss 0.318595\n",
      "batch 1243: loss 0.724845\n",
      "batch 1244: loss 0.279493\n",
      "batch 1245: loss 0.287031\n",
      "batch 1246: loss 0.199479\n",
      "batch 1247: loss 0.595746\n",
      "batch 1248: loss 0.125766\n",
      "batch 1249: loss 0.212540\n",
      "batch 1250: loss 0.210591\n",
      "batch 1251: loss 0.121041\n",
      "batch 1252: loss 0.234395\n",
      "batch 1253: loss 0.392339\n",
      "batch 1254: loss 0.345707\n",
      "batch 1255: loss 0.169883\n",
      "batch 1256: loss 0.194631\n",
      "batch 1257: loss 0.164755\n",
      "batch 1258: loss 0.278009\n",
      "batch 1259: loss 0.312726\n",
      "batch 1260: loss 0.352372\n",
      "batch 1261: loss 0.242761\n",
      "batch 1262: loss 0.138442\n",
      "batch 1263: loss 0.311649\n",
      "batch 1264: loss 0.155863\n",
      "batch 1265: loss 0.268719\n",
      "batch 1266: loss 0.441470\n",
      "batch 1267: loss 0.108340\n",
      "batch 1268: loss 0.247010\n",
      "batch 1269: loss 0.263119\n",
      "batch 1270: loss 0.308957\n",
      "batch 1271: loss 0.234067\n",
      "batch 1272: loss 0.142408\n",
      "batch 1273: loss 0.237555\n",
      "batch 1274: loss 0.241464\n",
      "batch 1275: loss 0.292428\n",
      "batch 1276: loss 0.436809\n",
      "batch 1277: loss 0.141691\n",
      "batch 1278: loss 0.163331\n",
      "batch 1279: loss 0.200120\n",
      "batch 1280: loss 0.178222\n",
      "batch 1281: loss 0.197157\n",
      "batch 1282: loss 0.320162\n",
      "batch 1283: loss 0.175770\n",
      "batch 1284: loss 0.318597\n",
      "batch 1285: loss 0.154785\n",
      "batch 1286: loss 0.116331\n",
      "batch 1287: loss 0.305029\n",
      "batch 1288: loss 0.121452\n",
      "batch 1289: loss 0.173312\n",
      "batch 1290: loss 0.170065\n",
      "batch 1291: loss 0.246006\n",
      "batch 1292: loss 0.410710\n",
      "batch 1293: loss 0.117075\n",
      "batch 1294: loss 0.122154\n",
      "batch 1295: loss 0.167578\n",
      "batch 1296: loss 0.102569\n",
      "batch 1297: loss 0.223092\n",
      "batch 1298: loss 0.359444\n",
      "batch 1299: loss 0.210355\n",
      "batch 1300: loss 0.320239\n",
      "batch 1301: loss 0.339993\n",
      "batch 1302: loss 0.176058\n",
      "batch 1303: loss 0.118188\n",
      "batch 1304: loss 0.254389\n",
      "batch 1305: loss 0.317063\n",
      "batch 1306: loss 0.213293\n",
      "batch 1307: loss 0.146757\n",
      "batch 1308: loss 0.145468\n",
      "batch 1309: loss 0.219839\n",
      "batch 1310: loss 0.379555\n",
      "batch 1311: loss 0.305768\n",
      "batch 1312: loss 0.302399\n",
      "batch 1313: loss 0.148147\n",
      "batch 1314: loss 0.181851\n",
      "batch 1315: loss 0.155822\n",
      "batch 1316: loss 0.325735\n",
      "batch 1317: loss 0.061962\n",
      "batch 1318: loss 0.203394\n",
      "batch 1319: loss 0.267588\n",
      "batch 1320: loss 0.202529\n",
      "batch 1321: loss 0.436998\n",
      "batch 1322: loss 0.302509\n",
      "batch 1323: loss 0.287590\n",
      "batch 1324: loss 0.296648\n",
      "batch 1325: loss 0.480762\n",
      "batch 1326: loss 0.258650\n",
      "batch 1327: loss 0.222407\n",
      "batch 1328: loss 0.209525\n",
      "batch 1329: loss 0.219764\n",
      "batch 1330: loss 0.338117\n",
      "batch 1331: loss 0.200613\n",
      "batch 1332: loss 0.260917\n",
      "batch 1333: loss 0.286841\n",
      "batch 1334: loss 0.405043\n",
      "batch 1335: loss 0.147745\n",
      "batch 1336: loss 0.209825\n",
      "batch 1337: loss 0.194724\n",
      "batch 1338: loss 0.251771\n",
      "batch 1339: loss 0.369515\n",
      "batch 1340: loss 0.196413\n",
      "batch 1341: loss 0.233508\n",
      "batch 1342: loss 0.212158\n",
      "batch 1343: loss 0.266283\n",
      "batch 1344: loss 0.225846\n",
      "batch 1345: loss 0.105054\n",
      "batch 1346: loss 0.190720\n",
      "batch 1347: loss 0.410549\n",
      "batch 1348: loss 0.294132\n",
      "batch 1349: loss 0.285767\n",
      "batch 1350: loss 0.374148\n",
      "batch 1351: loss 0.166396\n",
      "batch 1352: loss 0.273052\n",
      "batch 1353: loss 0.064042\n",
      "batch 1354: loss 0.121635\n",
      "batch 1355: loss 0.139028\n",
      "batch 1356: loss 0.288746\n",
      "batch 1357: loss 0.379386\n",
      "batch 1358: loss 0.253081\n",
      "batch 1359: loss 0.231339\n",
      "batch 1360: loss 0.230838\n",
      "batch 1361: loss 0.185826\n",
      "batch 1362: loss 0.426827\n",
      "batch 1363: loss 0.225951\n",
      "batch 1364: loss 0.155811\n",
      "batch 1365: loss 0.293578\n",
      "batch 1366: loss 0.379285\n",
      "batch 1367: loss 0.131683\n",
      "batch 1368: loss 0.321040\n",
      "batch 1369: loss 0.165875\n",
      "batch 1370: loss 0.399067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1371: loss 0.249179\n",
      "batch 1372: loss 0.211262\n",
      "batch 1373: loss 0.255046\n",
      "batch 1374: loss 0.175734\n",
      "batch 1375: loss 0.138676\n",
      "batch 1376: loss 0.120086\n",
      "batch 1377: loss 0.227010\n",
      "batch 1378: loss 0.378131\n",
      "batch 1379: loss 0.147962\n",
      "batch 1380: loss 0.205489\n",
      "batch 1381: loss 0.283904\n",
      "batch 1382: loss 0.313327\n",
      "batch 1383: loss 0.150976\n",
      "batch 1384: loss 0.152689\n",
      "batch 1385: loss 0.178672\n",
      "batch 1386: loss 0.298602\n",
      "batch 1387: loss 0.290850\n",
      "batch 1388: loss 0.239560\n",
      "batch 1389: loss 0.215846\n",
      "batch 1390: loss 0.197701\n",
      "batch 1391: loss 0.336196\n",
      "batch 1392: loss 0.109281\n",
      "batch 1393: loss 0.299877\n",
      "batch 1394: loss 0.228903\n",
      "batch 1395: loss 0.306355\n",
      "batch 1396: loss 0.140917\n",
      "batch 1397: loss 0.319106\n",
      "batch 1398: loss 0.303777\n",
      "batch 1399: loss 0.352069\n",
      "batch 1400: loss 0.264017\n",
      "batch 1401: loss 0.328543\n",
      "batch 1402: loss 0.208645\n",
      "batch 1403: loss 0.387271\n",
      "batch 1404: loss 0.254098\n",
      "batch 1405: loss 0.104444\n",
      "batch 1406: loss 0.246910\n",
      "batch 1407: loss 0.137139\n",
      "batch 1408: loss 0.215752\n",
      "batch 1409: loss 0.355229\n",
      "batch 1410: loss 0.220375\n",
      "batch 1411: loss 0.311355\n",
      "batch 1412: loss 0.457052\n",
      "batch 1413: loss 0.208019\n",
      "batch 1414: loss 0.176385\n",
      "batch 1415: loss 0.292087\n",
      "batch 1416: loss 0.200845\n",
      "batch 1417: loss 0.151731\n",
      "batch 1418: loss 0.188150\n",
      "batch 1419: loss 0.094215\n",
      "batch 1420: loss 0.199941\n",
      "batch 1421: loss 0.507062\n",
      "batch 1422: loss 0.179475\n",
      "batch 1423: loss 0.291392\n",
      "batch 1424: loss 0.160425\n",
      "batch 1425: loss 0.434732\n",
      "batch 1426: loss 0.104283\n",
      "batch 1427: loss 0.206705\n",
      "batch 1428: loss 0.284167\n",
      "batch 1429: loss 0.368621\n",
      "batch 1430: loss 0.200678\n",
      "batch 1431: loss 0.207666\n",
      "batch 1432: loss 0.230728\n",
      "batch 1433: loss 0.223796\n",
      "batch 1434: loss 0.277510\n",
      "batch 1435: loss 0.238710\n",
      "batch 1436: loss 0.219125\n",
      "batch 1437: loss 0.175078\n",
      "batch 1438: loss 0.142772\n",
      "batch 1439: loss 0.469332\n",
      "batch 1440: loss 0.148380\n",
      "batch 1441: loss 0.508858\n",
      "batch 1442: loss 0.633729\n",
      "batch 1443: loss 0.213968\n",
      "batch 1444: loss 0.208552\n",
      "batch 1445: loss 0.172094\n",
      "batch 1446: loss 0.233185\n",
      "batch 1447: loss 0.256117\n",
      "batch 1448: loss 0.242506\n",
      "batch 1449: loss 0.139636\n",
      "batch 1450: loss 0.279000\n",
      "batch 1451: loss 0.198361\n",
      "batch 1452: loss 0.127649\n",
      "batch 1453: loss 0.114687\n",
      "batch 1454: loss 0.185568\n",
      "batch 1455: loss 0.208788\n",
      "batch 1456: loss 0.422897\n",
      "batch 1457: loss 0.474284\n",
      "batch 1458: loss 0.239823\n",
      "batch 1459: loss 0.321395\n",
      "batch 1460: loss 0.184937\n",
      "batch 1461: loss 0.372381\n",
      "batch 1462: loss 0.205010\n",
      "batch 1463: loss 0.213440\n",
      "batch 1464: loss 0.255484\n",
      "batch 1465: loss 0.296667\n",
      "batch 1466: loss 0.312415\n",
      "batch 1467: loss 0.366326\n",
      "batch 1468: loss 0.216500\n",
      "batch 1469: loss 0.319342\n",
      "batch 1470: loss 0.222963\n",
      "batch 1471: loss 0.217473\n",
      "batch 1472: loss 0.113774\n",
      "batch 1473: loss 0.099346\n",
      "batch 1474: loss 0.230381\n",
      "batch 1475: loss 0.139352\n",
      "batch 1476: loss 0.231961\n",
      "batch 1477: loss 0.235199\n",
      "batch 1478: loss 0.292622\n",
      "batch 1479: loss 0.095180\n",
      "batch 1480: loss 0.096330\n",
      "batch 1481: loss 0.176140\n",
      "batch 1482: loss 0.337946\n",
      "batch 1483: loss 0.199593\n",
      "batch 1484: loss 0.407403\n",
      "batch 1485: loss 0.111739\n",
      "batch 1486: loss 0.138005\n",
      "batch 1487: loss 0.271051\n",
      "batch 1488: loss 0.582155\n",
      "batch 1489: loss 0.175975\n",
      "batch 1490: loss 0.161837\n",
      "batch 1491: loss 0.176757\n",
      "batch 1492: loss 0.293483\n",
      "batch 1493: loss 0.160556\n",
      "batch 1494: loss 0.287470\n",
      "batch 1495: loss 0.223695\n",
      "batch 1496: loss 0.292515\n",
      "batch 1497: loss 0.291851\n",
      "batch 1498: loss 0.150463\n",
      "batch 1499: loss 0.398987\n",
      "batch 1500: loss 0.356412\n",
      "batch 1501: loss 0.158489\n",
      "batch 1502: loss 0.145043\n",
      "batch 1503: loss 0.314347\n",
      "batch 1504: loss 0.279498\n",
      "batch 1505: loss 0.172609\n",
      "batch 1506: loss 0.192540\n",
      "batch 1507: loss 0.241388\n",
      "batch 1508: loss 0.253270\n",
      "batch 1509: loss 0.123879\n",
      "batch 1510: loss 0.185374\n",
      "batch 1511: loss 0.121462\n",
      "batch 1512: loss 0.152772\n",
      "batch 1513: loss 0.193780\n",
      "batch 1514: loss 0.317492\n",
      "batch 1515: loss 0.077427\n",
      "batch 1516: loss 0.221891\n",
      "batch 1517: loss 0.287742\n",
      "batch 1518: loss 0.487156\n",
      "batch 1519: loss 0.206772\n",
      "batch 1520: loss 0.113104\n",
      "batch 1521: loss 0.335098\n",
      "batch 1522: loss 0.274827\n",
      "batch 1523: loss 0.126673\n",
      "batch 1524: loss 0.225945\n",
      "batch 1525: loss 0.122661\n",
      "batch 1526: loss 0.208894\n",
      "batch 1527: loss 0.170969\n",
      "batch 1528: loss 0.225892\n",
      "batch 1529: loss 0.303927\n",
      "batch 1530: loss 0.190537\n",
      "batch 1531: loss 0.477727\n",
      "batch 1532: loss 0.205246\n",
      "batch 1533: loss 0.198745\n",
      "batch 1534: loss 0.093532\n",
      "batch 1535: loss 0.350434\n",
      "batch 1536: loss 0.462271\n",
      "batch 1537: loss 0.276705\n",
      "batch 1538: loss 0.257536\n",
      "batch 1539: loss 0.237122\n",
      "batch 1540: loss 0.243556\n",
      "batch 1541: loss 0.199639\n",
      "batch 1542: loss 0.238207\n",
      "batch 1543: loss 0.199491\n",
      "batch 1544: loss 0.339451\n",
      "batch 1545: loss 0.303783\n",
      "batch 1546: loss 0.108299\n",
      "batch 1547: loss 0.433086\n",
      "batch 1548: loss 0.118861\n",
      "batch 1549: loss 0.212627\n",
      "batch 1550: loss 0.218894\n",
      "batch 1551: loss 0.366858\n",
      "batch 1552: loss 0.268283\n",
      "batch 1553: loss 0.458283\n",
      "batch 1554: loss 0.153522\n",
      "batch 1555: loss 0.319259\n",
      "batch 1556: loss 0.406328\n",
      "batch 1557: loss 0.215657\n",
      "batch 1558: loss 0.274771\n",
      "batch 1559: loss 0.231019\n",
      "batch 1560: loss 0.239000\n",
      "batch 1561: loss 0.313076\n",
      "batch 1562: loss 0.230922\n",
      "batch 1563: loss 0.236241\n",
      "batch 1564: loss 0.215734\n",
      "batch 1565: loss 0.276041\n",
      "batch 1566: loss 0.253143\n",
      "batch 1567: loss 0.147099\n",
      "batch 1568: loss 0.317827\n",
      "batch 1569: loss 0.350454\n",
      "batch 1570: loss 0.158265\n",
      "batch 1571: loss 0.221913\n",
      "batch 1572: loss 0.357729\n",
      "batch 1573: loss 0.345039\n",
      "batch 1574: loss 0.084503\n",
      "batch 1575: loss 0.139722\n",
      "batch 1576: loss 0.158600\n",
      "batch 1577: loss 0.288245\n",
      "batch 1578: loss 0.207290\n",
      "batch 1579: loss 0.182069\n",
      "batch 1580: loss 0.333273\n",
      "batch 1581: loss 0.261002\n",
      "batch 1582: loss 0.323250\n",
      "batch 1583: loss 0.468169\n",
      "batch 1584: loss 0.319446\n",
      "batch 1585: loss 0.333712\n",
      "batch 1586: loss 0.191669\n",
      "batch 1587: loss 0.102737\n",
      "batch 1588: loss 0.312082\n",
      "batch 1589: loss 0.355099\n",
      "batch 1590: loss 0.196197\n",
      "batch 1591: loss 0.292651\n",
      "batch 1592: loss 0.370105\n",
      "batch 1593: loss 0.168397\n",
      "batch 1594: loss 0.378895\n",
      "batch 1595: loss 0.293494\n",
      "batch 1596: loss 0.302294\n",
      "batch 1597: loss 0.196525\n",
      "batch 1598: loss 0.335536\n",
      "batch 1599: loss 0.408293\n",
      "batch 1600: loss 0.191285\n",
      "batch 1601: loss 0.194026\n",
      "batch 1602: loss 0.185317\n",
      "batch 1603: loss 0.406951\n",
      "batch 1604: loss 0.223643\n",
      "batch 1605: loss 0.212545\n",
      "batch 1606: loss 0.348828\n",
      "batch 1607: loss 0.107365\n",
      "batch 1608: loss 0.315054\n",
      "batch 1609: loss 0.330755\n",
      "batch 1610: loss 0.281044\n",
      "batch 1611: loss 0.265915\n",
      "batch 1612: loss 0.288539\n",
      "batch 1613: loss 0.331714\n",
      "batch 1614: loss 0.313542\n",
      "batch 1615: loss 0.190772\n",
      "batch 1616: loss 0.178958\n",
      "batch 1617: loss 0.138475\n",
      "batch 1618: loss 0.222062\n",
      "batch 1619: loss 0.264283\n",
      "batch 1620: loss 0.182471\n",
      "batch 1621: loss 0.223580\n",
      "batch 1622: loss 0.248687\n",
      "batch 1623: loss 0.239404\n",
      "batch 1624: loss 0.225283\n",
      "batch 1625: loss 0.281236\n",
      "batch 1626: loss 0.120118\n",
      "batch 1627: loss 0.213460\n",
      "batch 1628: loss 0.189791\n",
      "batch 1629: loss 0.253978\n",
      "batch 1630: loss 0.133837\n",
      "batch 1631: loss 0.251112\n",
      "batch 1632: loss 0.153748\n",
      "batch 1633: loss 0.161900\n",
      "batch 1634: loss 0.062976\n",
      "batch 1635: loss 0.062463\n",
      "batch 1636: loss 0.186653\n",
      "batch 1637: loss 0.205099\n",
      "batch 1638: loss 0.236740\n",
      "batch 1639: loss 0.194274\n",
      "batch 1640: loss 0.213943\n",
      "batch 1641: loss 0.199807\n",
      "batch 1642: loss 0.382103\n",
      "batch 1643: loss 0.129258\n",
      "batch 1644: loss 0.147488\n",
      "batch 1645: loss 0.282706\n",
      "batch 1646: loss 0.267601\n",
      "batch 1647: loss 0.372095\n",
      "batch 1648: loss 0.194513\n",
      "batch 1649: loss 0.259777\n",
      "batch 1650: loss 0.250694\n",
      "batch 1651: loss 0.104859\n",
      "batch 1652: loss 0.241091\n",
      "batch 1653: loss 0.421393\n",
      "batch 1654: loss 0.197839\n",
      "batch 1655: loss 0.356026\n",
      "batch 1656: loss 0.208670\n",
      "batch 1657: loss 0.331526\n",
      "batch 1658: loss 0.101125\n",
      "batch 1659: loss 0.256326\n",
      "batch 1660: loss 0.128498\n",
      "batch 1661: loss 0.256834\n",
      "batch 1662: loss 0.241662\n",
      "batch 1663: loss 0.137309\n",
      "batch 1664: loss 0.221760\n",
      "batch 1665: loss 0.141062\n",
      "batch 1666: loss 0.465586\n",
      "batch 1667: loss 0.234044\n",
      "batch 1668: loss 0.244845\n",
      "batch 1669: loss 0.297577\n",
      "batch 1670: loss 0.222809\n",
      "batch 1671: loss 0.245500\n",
      "batch 1672: loss 0.172058\n",
      "batch 1673: loss 0.398437\n",
      "batch 1674: loss 0.072961\n",
      "batch 1675: loss 0.245680\n",
      "batch 1676: loss 0.315489\n",
      "batch 1677: loss 0.167163\n",
      "batch 1678: loss 0.269667\n",
      "batch 1679: loss 0.383693\n",
      "batch 1680: loss 0.373437\n",
      "batch 1681: loss 0.143389\n",
      "batch 1682: loss 0.149980\n",
      "batch 1683: loss 0.265358\n",
      "batch 1684: loss 0.306381\n",
      "batch 1685: loss 0.263287\n",
      "batch 1686: loss 0.200581\n",
      "batch 1687: loss 0.193626\n",
      "batch 1688: loss 0.105475\n",
      "batch 1689: loss 0.374384\n",
      "batch 1690: loss 0.235992\n",
      "batch 1691: loss 0.164830\n",
      "batch 1692: loss 0.320941\n",
      "batch 1693: loss 0.285888\n",
      "batch 1694: loss 0.503134\n",
      "batch 1695: loss 0.242619\n",
      "batch 1696: loss 0.214420\n",
      "batch 1697: loss 0.319774\n",
      "batch 1698: loss 0.149182\n",
      "batch 1699: loss 0.142409\n",
      "batch 1700: loss 0.204590\n",
      "batch 1701: loss 0.327331\n",
      "batch 1702: loss 0.165350\n",
      "batch 1703: loss 0.205351\n",
      "batch 1704: loss 0.203151\n",
      "batch 1705: loss 0.430404\n",
      "batch 1706: loss 0.282686\n",
      "batch 1707: loss 0.415363\n",
      "batch 1708: loss 0.213879\n",
      "batch 1709: loss 0.381610\n",
      "batch 1710: loss 0.340218\n",
      "batch 1711: loss 0.136013\n",
      "batch 1712: loss 0.193085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1713: loss 0.157199\n",
      "batch 1714: loss 0.317990\n",
      "batch 1715: loss 0.248403\n",
      "batch 1716: loss 0.127350\n",
      "batch 1717: loss 0.170427\n",
      "batch 1718: loss 0.265037\n",
      "batch 1719: loss 0.393107\n",
      "batch 1720: loss 0.253501\n",
      "batch 1721: loss 0.337176\n",
      "batch 1722: loss 0.187661\n",
      "batch 1723: loss 0.386856\n",
      "batch 1724: loss 0.500823\n",
      "batch 1725: loss 0.158758\n",
      "batch 1726: loss 0.166711\n",
      "batch 1727: loss 0.347810\n",
      "batch 1728: loss 0.123764\n",
      "batch 1729: loss 0.349734\n",
      "batch 1730: loss 0.355006\n",
      "batch 1731: loss 0.312955\n",
      "batch 1732: loss 0.234521\n",
      "batch 1733: loss 0.072064\n",
      "batch 1734: loss 0.290351\n",
      "batch 1735: loss 0.310926\n",
      "batch 1736: loss 0.098792\n",
      "batch 1737: loss 0.150428\n",
      "batch 1738: loss 0.363316\n",
      "batch 1739: loss 0.254108\n",
      "batch 1740: loss 0.234642\n",
      "batch 1741: loss 0.472869\n",
      "batch 1742: loss 0.426793\n",
      "batch 1743: loss 0.109080\n",
      "batch 1744: loss 0.202664\n",
      "batch 1745: loss 0.184892\n",
      "batch 1746: loss 0.232077\n",
      "batch 1747: loss 0.214689\n",
      "batch 1748: loss 0.530332\n",
      "batch 1749: loss 0.248673\n",
      "batch 1750: loss 0.227267\n",
      "batch 1751: loss 0.067207\n",
      "batch 1752: loss 0.111454\n",
      "batch 1753: loss 0.194206\n",
      "batch 1754: loss 0.174167\n",
      "batch 1755: loss 0.284229\n",
      "batch 1756: loss 0.446775\n",
      "batch 1757: loss 0.315394\n",
      "batch 1758: loss 0.391233\n",
      "batch 1759: loss 0.093838\n",
      "batch 1760: loss 0.252664\n",
      "batch 1761: loss 0.366734\n",
      "batch 1762: loss 0.266998\n",
      "batch 1763: loss 0.261089\n",
      "batch 1764: loss 0.280165\n",
      "batch 1765: loss 0.109272\n",
      "batch 1766: loss 0.402676\n",
      "batch 1767: loss 0.140180\n",
      "batch 1768: loss 0.083605\n",
      "batch 1769: loss 0.288421\n",
      "batch 1770: loss 0.492845\n",
      "batch 1771: loss 0.282389\n",
      "batch 1772: loss 0.381231\n",
      "batch 1773: loss 0.128281\n",
      "batch 1774: loss 0.071175\n",
      "batch 1775: loss 0.298317\n",
      "batch 1776: loss 0.411331\n",
      "batch 1777: loss 0.392302\n",
      "batch 1778: loss 0.167986\n",
      "batch 1779: loss 0.113854\n",
      "batch 1780: loss 0.200065\n",
      "batch 1781: loss 0.120749\n",
      "batch 1782: loss 0.267459\n",
      "batch 1783: loss 0.172951\n",
      "batch 1784: loss 0.263120\n",
      "batch 1785: loss 0.191511\n",
      "batch 1786: loss 0.169362\n",
      "batch 1787: loss 0.183331\n",
      "batch 1788: loss 0.209085\n",
      "batch 1789: loss 0.189919\n",
      "batch 1790: loss 0.174735\n",
      "batch 1791: loss 0.289415\n",
      "batch 1792: loss 0.164724\n",
      "batch 1793: loss 0.177586\n",
      "batch 1794: loss 0.144588\n",
      "batch 1795: loss 0.434291\n",
      "batch 1796: loss 0.295856\n",
      "batch 1797: loss 0.277726\n",
      "batch 1798: loss 0.199947\n",
      "batch 1799: loss 0.171929\n",
      "batch 1800: loss 0.447139\n",
      "batch 1801: loss 0.229052\n",
      "batch 1802: loss 0.266385\n",
      "batch 1803: loss 0.208819\n",
      "batch 1804: loss 0.136514\n",
      "batch 1805: loss 0.207133\n",
      "batch 1806: loss 0.228950\n",
      "batch 1807: loss 0.203478\n",
      "batch 1808: loss 0.114603\n",
      "batch 1809: loss 0.339227\n",
      "batch 1810: loss 0.310607\n",
      "batch 1811: loss 0.224322\n",
      "batch 1812: loss 0.303286\n",
      "batch 1813: loss 0.106889\n",
      "batch 1814: loss 0.156227\n",
      "batch 1815: loss 0.386521\n",
      "batch 1816: loss 0.250268\n",
      "batch 1817: loss 0.183703\n",
      "batch 1818: loss 0.317154\n",
      "batch 1819: loss 0.436974\n",
      "batch 1820: loss 0.211254\n",
      "batch 1821: loss 0.271535\n",
      "batch 1822: loss 0.075673\n",
      "batch 1823: loss 0.184539\n",
      "batch 1824: loss 0.301909\n",
      "batch 1825: loss 0.368865\n",
      "batch 1826: loss 0.338914\n",
      "batch 1827: loss 0.095822\n",
      "batch 1828: loss 0.264754\n",
      "batch 1829: loss 0.331045\n",
      "batch 1830: loss 0.247258\n",
      "batch 1831: loss 0.130342\n",
      "batch 1832: loss 0.463464\n",
      "batch 1833: loss 0.217064\n",
      "batch 1834: loss 0.301322\n",
      "batch 1835: loss 0.489974\n",
      "batch 1836: loss 0.383475\n",
      "batch 1837: loss 0.184273\n",
      "batch 1838: loss 0.313659\n",
      "batch 1839: loss 0.323144\n",
      "batch 1840: loss 0.294193\n",
      "batch 1841: loss 0.858122\n",
      "batch 1842: loss 0.141526\n",
      "batch 1843: loss 0.441359\n",
      "batch 1844: loss 0.175210\n",
      "batch 1845: loss 0.183052\n",
      "batch 1846: loss 0.220749\n",
      "batch 1847: loss 0.184434\n",
      "batch 1848: loss 0.196029\n",
      "batch 1849: loss 0.273208\n",
      "batch 1850: loss 0.137296\n",
      "batch 1851: loss 0.393652\n",
      "batch 1852: loss 0.177155\n",
      "batch 1853: loss 0.246918\n",
      "batch 1854: loss 0.203474\n",
      "batch 1855: loss 0.334789\n",
      "batch 1856: loss 0.136494\n",
      "batch 1857: loss 0.387922\n",
      "batch 1858: loss 0.186304\n",
      "batch 1859: loss 0.436220\n",
      "batch 1860: loss 0.133250\n",
      "batch 1861: loss 0.283730\n",
      "batch 1862: loss 0.208486\n",
      "batch 1863: loss 0.099837\n",
      "batch 1864: loss 0.253836\n",
      "batch 1865: loss 0.203194\n",
      "batch 1866: loss 0.335665\n",
      "batch 1867: loss 0.398437\n",
      "batch 1868: loss 0.519543\n",
      "batch 1869: loss 0.247823\n",
      "batch 1870: loss 0.174868\n",
      "batch 1871: loss 0.274587\n",
      "batch 1872: loss 0.209336\n",
      "batch 1873: loss 0.145214\n",
      "batch 1874: loss 0.235735\n",
      "batch 1875: loss 0.186869\n",
      "batch 1876: loss 0.268316\n",
      "batch 1877: loss 0.194913\n",
      "batch 1878: loss 0.494087\n",
      "batch 1879: loss 0.340397\n",
      "batch 1880: loss 0.283603\n",
      "batch 1881: loss 0.179632\n",
      "batch 1882: loss 0.182328\n",
      "batch 1883: loss 0.127535\n",
      "batch 1884: loss 0.229043\n",
      "batch 1885: loss 0.133355\n",
      "batch 1886: loss 0.128708\n",
      "batch 1887: loss 0.216051\n",
      "batch 1888: loss 0.322429\n",
      "batch 1889: loss 0.293467\n",
      "batch 1890: loss 0.460566\n",
      "batch 1891: loss 0.238825\n",
      "batch 1892: loss 0.384452\n",
      "batch 1893: loss 0.297624\n",
      "batch 1894: loss 0.229678\n",
      "batch 1895: loss 0.256569\n",
      "batch 1896: loss 0.324846\n",
      "batch 1897: loss 0.227526\n",
      "batch 1898: loss 0.238131\n",
      "batch 1899: loss 0.159843\n",
      "batch 1900: loss 0.150627\n",
      "batch 1901: loss 0.240030\n",
      "batch 1902: loss 0.280278\n",
      "batch 1903: loss 0.229941\n",
      "batch 1904: loss 0.322252\n",
      "batch 1905: loss 0.374022\n",
      "batch 1906: loss 0.257389\n",
      "batch 1907: loss 0.218613\n",
      "batch 1908: loss 0.362412\n",
      "batch 1909: loss 0.140305\n",
      "batch 1910: loss 0.238179\n",
      "batch 1911: loss 0.211041\n",
      "batch 1912: loss 0.318884\n",
      "batch 1913: loss 0.191834\n",
      "batch 1914: loss 0.109220\n",
      "batch 1915: loss 0.246261\n",
      "batch 1916: loss 0.213814\n",
      "batch 1917: loss 0.482195\n",
      "batch 1918: loss 0.358497\n",
      "batch 1919: loss 0.168380\n",
      "batch 1920: loss 0.156640\n",
      "batch 1921: loss 0.369982\n",
      "batch 1922: loss 0.397027\n",
      "batch 1923: loss 0.256236\n",
      "batch 1924: loss 0.183615\n",
      "batch 1925: loss 0.207530\n",
      "batch 1926: loss 0.176676\n",
      "batch 1927: loss 0.278031\n",
      "batch 1928: loss 0.230119\n",
      "batch 1929: loss 0.371797\n",
      "batch 1930: loss 0.078296\n",
      "batch 1931: loss 0.133826\n",
      "batch 1932: loss 0.249879\n",
      "batch 1933: loss 0.253962\n",
      "batch 1934: loss 0.250087\n",
      "batch 1935: loss 0.425741\n",
      "batch 1936: loss 0.300207\n",
      "batch 1937: loss 0.208899\n",
      "batch 1938: loss 0.354964\n",
      "batch 1939: loss 0.278462\n",
      "batch 1940: loss 0.139814\n",
      "batch 1941: loss 0.260702\n",
      "batch 1942: loss 0.315309\n",
      "batch 1943: loss 0.113833\n",
      "batch 1944: loss 0.274158\n",
      "batch 1945: loss 0.293107\n",
      "batch 1946: loss 0.362997\n",
      "batch 1947: loss 0.491853\n",
      "batch 1948: loss 0.214877\n",
      "batch 1949: loss 0.244952\n",
      "batch 1950: loss 0.219756\n",
      "batch 1951: loss 0.319322\n",
      "batch 1952: loss 0.249347\n",
      "batch 1953: loss 0.283525\n",
      "batch 1954: loss 0.382484\n",
      "batch 1955: loss 0.402699\n",
      "batch 1956: loss 0.285668\n",
      "batch 1957: loss 0.144456\n",
      "batch 1958: loss 0.221690\n",
      "batch 1959: loss 0.211295\n",
      "batch 1960: loss 0.225134\n",
      "batch 1961: loss 0.242664\n",
      "batch 1962: loss 0.097611\n",
      "batch 1963: loss 0.337215\n",
      "batch 1964: loss 0.193989\n",
      "batch 1965: loss 0.349725\n",
      "batch 1966: loss 0.225454\n",
      "batch 1967: loss 0.399541\n",
      "batch 1968: loss 0.238912\n",
      "batch 1969: loss 0.441108\n",
      "batch 1970: loss 0.391865\n",
      "batch 1971: loss 0.314430\n",
      "batch 1972: loss 0.531513\n",
      "batch 1973: loss 0.193608\n",
      "batch 1974: loss 0.080404\n",
      "batch 1975: loss 0.206187\n",
      "batch 1976: loss 0.248161\n",
      "batch 1977: loss 0.087336\n",
      "batch 1978: loss 0.274850\n",
      "batch 1979: loss 0.411210\n",
      "batch 1980: loss 0.102353\n",
      "batch 1981: loss 0.187664\n",
      "batch 1982: loss 0.222502\n",
      "batch 1983: loss 0.147010\n",
      "batch 1984: loss 0.301685\n",
      "batch 1985: loss 0.123268\n",
      "batch 1986: loss 0.495360\n",
      "batch 1987: loss 0.274306\n",
      "batch 1988: loss 0.295024\n",
      "batch 1989: loss 0.226361\n",
      "batch 1990: loss 0.297566\n",
      "batch 1991: loss 0.326812\n",
      "batch 1992: loss 0.328097\n",
      "batch 1993: loss 0.478010\n",
      "batch 1994: loss 0.255756\n",
      "batch 1995: loss 0.291140\n",
      "batch 1996: loss 0.364278\n",
      "batch 1997: loss 0.177119\n",
      "batch 1998: loss 0.228783\n",
      "batch 1999: loss 0.322580\n",
      "batch 2000: loss 0.503090\n",
      "batch 2001: loss 0.395216\n",
      "batch 2002: loss 0.444459\n",
      "batch 2003: loss 0.343729\n",
      "batch 2004: loss 0.364458\n",
      "batch 2005: loss 0.190589\n",
      "batch 2006: loss 0.267981\n",
      "batch 2007: loss 0.321527\n",
      "batch 2008: loss 0.273921\n",
      "batch 2009: loss 0.097448\n",
      "batch 2010: loss 0.217529\n",
      "batch 2011: loss 0.263584\n",
      "batch 2012: loss 0.207554\n",
      "batch 2013: loss 0.291777\n",
      "batch 2014: loss 0.345001\n",
      "batch 2015: loss 0.371262\n",
      "batch 2016: loss 0.458543\n",
      "batch 2017: loss 0.283188\n",
      "batch 2018: loss 0.255481\n",
      "batch 2019: loss 0.317364\n",
      "batch 2020: loss 0.327411\n",
      "batch 2021: loss 0.361287\n",
      "batch 2022: loss 0.578461\n",
      "batch 2023: loss 0.345339\n",
      "batch 2024: loss 0.136632\n",
      "batch 2025: loss 0.170855\n",
      "batch 2026: loss 0.647356\n",
      "batch 2027: loss 0.311076\n",
      "batch 2028: loss 0.208059\n",
      "batch 2029: loss 0.189500\n",
      "batch 2030: loss 0.275897\n",
      "batch 2031: loss 0.219537\n",
      "batch 2032: loss 0.151125\n",
      "batch 2033: loss 0.193119\n",
      "batch 2034: loss 0.336773\n",
      "batch 2035: loss 0.257926\n",
      "batch 2036: loss 0.291032\n",
      "batch 2037: loss 0.099337\n",
      "batch 2038: loss 0.366640\n",
      "batch 2039: loss 0.324230\n",
      "batch 2040: loss 0.414128\n",
      "batch 2041: loss 0.365001\n",
      "batch 2042: loss 0.327399\n",
      "batch 2043: loss 0.265630\n",
      "batch 2044: loss 0.300691\n",
      "batch 2045: loss 0.090300\n",
      "batch 2046: loss 0.335325\n",
      "batch 2047: loss 0.396306\n",
      "batch 2048: loss 0.382173\n",
      "batch 2049: loss 0.399784\n",
      "batch 2050: loss 0.325999\n",
      "batch 2051: loss 0.481618\n",
      "batch 2052: loss 0.221391\n",
      "batch 2053: loss 0.266361\n",
      "batch 2054: loss 0.234775\n",
      "batch 2055: loss 0.191113\n",
      "batch 2056: loss 0.231667\n",
      "batch 2057: loss 0.268629\n",
      "batch 2058: loss 0.280988\n",
      "batch 2059: loss 0.285547\n",
      "batch 2060: loss 0.386211\n",
      "batch 2061: loss 0.177826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2062: loss 0.326795\n",
      "batch 2063: loss 0.475196\n",
      "batch 2064: loss 0.202492\n",
      "batch 2065: loss 0.389668\n",
      "batch 2066: loss 0.354052\n",
      "batch 2067: loss 0.192541\n",
      "batch 2068: loss 0.233831\n",
      "batch 2069: loss 0.395531\n",
      "batch 2070: loss 0.248891\n",
      "batch 2071: loss 0.082014\n",
      "batch 2072: loss 0.421458\n",
      "batch 2073: loss 0.144124\n",
      "batch 2074: loss 0.371231\n",
      "batch 2075: loss 0.341306\n",
      "batch 2076: loss 0.241869\n",
      "batch 2077: loss 0.270147\n",
      "batch 2078: loss 0.306185\n",
      "batch 2079: loss 0.141123\n",
      "batch 2080: loss 0.273538\n",
      "batch 2081: loss 0.198972\n",
      "batch 2082: loss 0.225990\n",
      "batch 2083: loss 0.164249\n",
      "batch 2084: loss 0.123127\n",
      "batch 2085: loss 0.188873\n",
      "batch 2086: loss 0.260434\n",
      "batch 2087: loss 0.238024\n",
      "batch 2088: loss 0.165532\n",
      "batch 2089: loss 0.440136\n",
      "batch 2090: loss 0.217023\n",
      "batch 2091: loss 0.326499\n",
      "batch 2092: loss 0.284284\n",
      "batch 2093: loss 0.262788\n",
      "batch 2094: loss 0.461276\n",
      "batch 2095: loss 0.638613\n",
      "batch 2096: loss 0.429470\n",
      "batch 2097: loss 0.461760\n",
      "batch 2098: loss 0.274795\n",
      "batch 2099: loss 0.061957\n",
      "batch 2100: loss 0.185056\n",
      "batch 2101: loss 0.149838\n",
      "batch 2102: loss 0.410977\n",
      "batch 2103: loss 0.224859\n",
      "batch 2104: loss 0.230050\n",
      "batch 2105: loss 0.344861\n",
      "batch 2106: loss 0.158238\n",
      "batch 2107: loss 0.291274\n",
      "batch 2108: loss 0.181232\n",
      "batch 2109: loss 0.187195\n",
      "batch 2110: loss 0.138415\n",
      "batch 2111: loss 0.081787\n",
      "batch 2112: loss 0.267412\n",
      "batch 2113: loss 0.220540\n",
      "batch 2114: loss 0.321788\n",
      "batch 2115: loss 0.201429\n",
      "batch 2116: loss 0.312323\n",
      "batch 2117: loss 0.186079\n",
      "batch 2118: loss 0.302268\n",
      "batch 2119: loss 0.267858\n",
      "batch 2120: loss 0.163514\n",
      "batch 2121: loss 0.157768\n",
      "batch 2122: loss 0.224299\n",
      "batch 2123: loss 0.250519\n",
      "batch 2124: loss 0.205448\n",
      "batch 2125: loss 0.257167\n",
      "batch 2126: loss 0.352236\n",
      "batch 2127: loss 0.351261\n",
      "batch 2128: loss 0.210290\n",
      "batch 2129: loss 0.098311\n",
      "batch 2130: loss 0.110447\n",
      "batch 2131: loss 0.431878\n",
      "batch 2132: loss 0.190184\n",
      "batch 2133: loss 0.227866\n",
      "batch 2134: loss 0.464019\n",
      "batch 2135: loss 0.134242\n",
      "batch 2136: loss 0.316247\n",
      "batch 2137: loss 0.598686\n",
      "batch 2138: loss 0.233645\n",
      "batch 2139: loss 0.373456\n",
      "batch 2140: loss 0.175525\n",
      "batch 2141: loss 0.261271\n",
      "batch 2142: loss 0.142561\n",
      "batch 2143: loss 0.221737\n",
      "batch 2144: loss 0.370519\n",
      "batch 2145: loss 0.233442\n",
      "batch 2146: loss 0.450673\n",
      "batch 2147: loss 0.206536\n",
      "batch 2148: loss 0.195758\n",
      "batch 2149: loss 0.559456\n",
      "batch 2150: loss 0.176727\n",
      "batch 2151: loss 0.227154\n",
      "batch 2152: loss 0.134599\n",
      "batch 2153: loss 0.273291\n",
      "batch 2154: loss 0.392011\n",
      "batch 2155: loss 0.297260\n",
      "batch 2156: loss 0.264966\n",
      "batch 2157: loss 0.422180\n",
      "batch 2158: loss 0.211462\n",
      "batch 2159: loss 0.167665\n",
      "batch 2160: loss 0.223955\n",
      "batch 2161: loss 0.253819\n",
      "batch 2162: loss 0.158866\n",
      "batch 2163: loss 0.135544\n",
      "batch 2164: loss 0.242268\n",
      "batch 2165: loss 0.313856\n",
      "batch 2166: loss 0.292975\n",
      "batch 2167: loss 0.221488\n",
      "batch 2168: loss 0.362508\n",
      "batch 2169: loss 0.400014\n",
      "batch 2170: loss 0.155731\n",
      "batch 2171: loss 0.113719\n",
      "batch 2172: loss 0.514393\n",
      "batch 2173: loss 0.222794\n",
      "batch 2174: loss 0.256813\n",
      "batch 2175: loss 0.306658\n",
      "batch 2176: loss 0.245847\n",
      "batch 2177: loss 0.149034\n",
      "batch 2178: loss 0.263313\n",
      "batch 2179: loss 0.370145\n",
      "batch 2180: loss 0.217367\n",
      "batch 2181: loss 0.076077\n",
      "batch 2182: loss 0.335916\n",
      "batch 2183: loss 0.361967\n",
      "batch 2184: loss 0.278475\n",
      "batch 2185: loss 0.148984\n",
      "batch 2186: loss 0.260930\n",
      "batch 2187: loss 0.333460\n",
      "batch 2188: loss 0.176114\n",
      "batch 2189: loss 0.367623\n",
      "batch 2190: loss 0.286764\n",
      "batch 2191: loss 0.192144\n",
      "batch 2192: loss 0.532408\n",
      "batch 2193: loss 0.373317\n",
      "batch 2194: loss 0.358040\n",
      "batch 2195: loss 0.419223\n",
      "batch 2196: loss 0.143398\n",
      "batch 2197: loss 0.486150\n",
      "batch 2198: loss 0.182405\n",
      "batch 2199: loss 0.205068\n",
      "batch 2200: loss 0.487635\n",
      "batch 2201: loss 0.159760\n",
      "batch 2202: loss 0.146991\n",
      "batch 2203: loss 0.162597\n",
      "batch 2204: loss 0.424346\n",
      "batch 2205: loss 0.181064\n",
      "batch 2206: loss 0.170571\n",
      "batch 2207: loss 0.553087\n",
      "batch 2208: loss 0.255622\n",
      "batch 2209: loss 0.200246\n",
      "batch 2210: loss 0.196476\n",
      "batch 2211: loss 0.454955\n",
      "batch 2212: loss 0.112753\n",
      "batch 2213: loss 0.175161\n",
      "batch 2214: loss 0.283772\n",
      "batch 2215: loss 0.200979\n",
      "batch 2216: loss 0.295005\n",
      "batch 2217: loss 0.166961\n",
      "batch 2218: loss 0.306526\n",
      "batch 2219: loss 0.276181\n",
      "batch 2220: loss 0.100429\n",
      "batch 2221: loss 0.275955\n",
      "batch 2222: loss 0.257291\n",
      "batch 2223: loss 0.206140\n",
      "batch 2224: loss 0.298567\n",
      "batch 2225: loss 0.477337\n",
      "batch 2226: loss 0.158598\n",
      "batch 2227: loss 0.124786\n",
      "batch 2228: loss 0.310266\n",
      "batch 2229: loss 0.379378\n",
      "batch 2230: loss 0.163840\n",
      "batch 2231: loss 0.269841\n",
      "batch 2232: loss 0.178530\n",
      "batch 2233: loss 0.211177\n",
      "batch 2234: loss 0.277019\n",
      "batch 2235: loss 0.334620\n",
      "batch 2236: loss 0.232963\n",
      "batch 2237: loss 0.130673\n",
      "batch 2238: loss 0.244711\n",
      "batch 2239: loss 0.253549\n",
      "batch 2240: loss 0.170127\n",
      "batch 2241: loss 0.388792\n",
      "batch 2242: loss 0.409103\n",
      "batch 2243: loss 0.099178\n",
      "batch 2244: loss 0.218593\n",
      "batch 2245: loss 0.253252\n",
      "batch 2246: loss 0.271782\n",
      "batch 2247: loss 0.234460\n",
      "batch 2248: loss 0.273203\n",
      "batch 2249: loss 0.378113\n",
      "batch 2250: loss 0.166438\n",
      "batch 2251: loss 0.227092\n",
      "batch 2252: loss 0.353413\n",
      "batch 2253: loss 0.140247\n",
      "batch 2254: loss 0.069084\n",
      "batch 2255: loss 0.266422\n",
      "batch 2256: loss 0.299650\n",
      "batch 2257: loss 0.348369\n",
      "batch 2258: loss 0.268932\n",
      "batch 2259: loss 0.323304\n",
      "batch 2260: loss 0.120710\n",
      "batch 2261: loss 0.171947\n",
      "batch 2262: loss 0.197694\n",
      "batch 2263: loss 0.330900\n",
      "batch 2264: loss 0.315904\n",
      "batch 2265: loss 0.325724\n",
      "batch 2266: loss 0.311717\n",
      "batch 2267: loss 0.306214\n",
      "batch 2268: loss 0.124768\n",
      "batch 2269: loss 0.307535\n",
      "batch 2270: loss 0.173203\n",
      "batch 2271: loss 0.359449\n",
      "batch 2272: loss 0.269872\n",
      "batch 2273: loss 0.269386\n",
      "batch 2274: loss 0.319006\n",
      "batch 2275: loss 0.147871\n",
      "batch 2276: loss 0.416278\n",
      "batch 2277: loss 0.174793\n",
      "batch 2278: loss 0.155925\n",
      "batch 2279: loss 0.093658\n",
      "batch 2280: loss 0.236153\n",
      "batch 2281: loss 0.201591\n",
      "batch 2282: loss 0.182533\n",
      "batch 2283: loss 0.191883\n",
      "batch 2284: loss 0.433960\n",
      "batch 2285: loss 0.313962\n",
      "batch 2286: loss 0.324851\n",
      "batch 2287: loss 0.390822\n",
      "batch 2288: loss 0.179112\n",
      "batch 2289: loss 0.411364\n",
      "batch 2290: loss 0.440024\n",
      "batch 2291: loss 0.167441\n",
      "batch 2292: loss 0.387463\n",
      "batch 2293: loss 0.124328\n",
      "batch 2294: loss 0.311261\n",
      "batch 2295: loss 0.153419\n",
      "batch 2296: loss 0.299867\n",
      "batch 2297: loss 0.494436\n",
      "batch 2298: loss 0.230384\n",
      "batch 2299: loss 0.321338\n",
      "batch 2300: loss 0.159333\n",
      "batch 2301: loss 0.302818\n",
      "batch 2302: loss 0.315786\n",
      "batch 2303: loss 0.276251\n",
      "batch 2304: loss 0.379853\n",
      "batch 2305: loss 0.320598\n",
      "batch 2306: loss 0.230085\n",
      "batch 2307: loss 0.189674\n",
      "batch 2308: loss 0.151298\n",
      "batch 2309: loss 0.168052\n",
      "batch 2310: loss 0.272965\n",
      "batch 2311: loss 0.195388\n",
      "batch 2312: loss 0.301697\n",
      "batch 2313: loss 0.254028\n",
      "batch 2314: loss 0.484762\n",
      "batch 2315: loss 0.127935\n",
      "batch 2316: loss 0.269680\n",
      "batch 2317: loss 0.312356\n",
      "batch 2318: loss 0.135226\n",
      "batch 2319: loss 0.227102\n",
      "batch 2320: loss 0.314521\n",
      "batch 2321: loss 0.206903\n",
      "batch 2322: loss 0.224366\n",
      "batch 2323: loss 0.380273\n",
      "batch 2324: loss 0.221496\n",
      "batch 2325: loss 0.283132\n",
      "batch 2326: loss 0.075198\n",
      "batch 2327: loss 0.301426\n",
      "batch 2328: loss 0.331260\n",
      "batch 2329: loss 0.204035\n",
      "batch 2330: loss 0.181952\n",
      "batch 2331: loss 0.177892\n",
      "batch 2332: loss 0.161749\n",
      "batch 2333: loss 0.169497\n",
      "batch 2334: loss 0.132957\n",
      "batch 2335: loss 0.205802\n",
      "batch 2336: loss 0.147934\n",
      "batch 2337: loss 0.179294\n",
      "batch 2338: loss 0.259297\n",
      "batch 2339: loss 0.222442\n",
      "test accuracy: 0.927885\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    \n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.05\n",
    "\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN神經網路訓練及驗證(keras版本)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)   (60000,)\n",
      "(10000, 784)   (10000,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])\n",
    "print(x_train.shape, ' ', y_train.shape)\n",
    "print(x_test.shape, ' ', y_test.shape)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#keras.optimizers.Adam(learning_rate=0.01)\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# provide labels as one_hot representation => tf.keras.losses.CategoricalCrossentropy\n",
    "# provide labels as integers => tf.keras.losses.SparseCategoricalCrossentropy \n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "165/165 - 0s - loss: 3.3766 - accuracy: 0.6972 - val_loss: 0.7349 - val_accuracy: 0.8313\n",
      "Epoch 2/100\n",
      "165/165 - 0s - loss: 0.5573 - accuracy: 0.8659 - val_loss: 0.4966 - val_accuracy: 0.8854\n",
      "Epoch 3/100\n",
      "165/165 - 0s - loss: 0.3762 - accuracy: 0.9019 - val_loss: 0.3991 - val_accuracy: 0.9014\n",
      "Epoch 4/100\n",
      "165/165 - 0s - loss: 0.2813 - accuracy: 0.9218 - val_loss: 0.3544 - val_accuracy: 0.9117\n",
      "Epoch 5/100\n",
      "165/165 - 0s - loss: 0.2303 - accuracy: 0.9356 - val_loss: 0.3218 - val_accuracy: 0.9231\n",
      "Epoch 6/100\n",
      "165/165 - 0s - loss: 0.2013 - accuracy: 0.9434 - val_loss: 0.2953 - val_accuracy: 0.9263\n",
      "Epoch 7/100\n",
      "165/165 - 0s - loss: 0.1712 - accuracy: 0.9509 - val_loss: 0.2887 - val_accuracy: 0.9304\n",
      "Epoch 8/100\n",
      "165/165 - 0s - loss: 0.1500 - accuracy: 0.9562 - val_loss: 0.2665 - val_accuracy: 0.9337\n",
      "Epoch 9/100\n",
      "165/165 - 0s - loss: 0.1359 - accuracy: 0.9587 - val_loss: 0.2755 - val_accuracy: 0.9358\n",
      "Epoch 10/100\n",
      "165/165 - 0s - loss: 0.1209 - accuracy: 0.9628 - val_loss: 0.2517 - val_accuracy: 0.9392\n",
      "Epoch 11/100\n",
      "165/165 - 0s - loss: 0.1106 - accuracy: 0.9660 - val_loss: 0.2661 - val_accuracy: 0.9387\n",
      "Epoch 12/100\n",
      "165/165 - 0s - loss: 0.1011 - accuracy: 0.9688 - val_loss: 0.2917 - val_accuracy: 0.9343\n",
      "Epoch 13/100\n",
      "165/165 - 0s - loss: 0.0910 - accuracy: 0.9716 - val_loss: 0.2564 - val_accuracy: 0.9406\n",
      "Epoch 14/100\n",
      "165/165 - 0s - loss: 0.0886 - accuracy: 0.9721 - val_loss: 0.2593 - val_accuracy: 0.9436\n",
      "Epoch 15/100\n",
      "165/165 - 0s - loss: 0.0919 - accuracy: 0.9710 - val_loss: 0.2541 - val_accuracy: 0.9453\n",
      "Epoch 16/100\n",
      "165/165 - 0s - loss: 0.0774 - accuracy: 0.9751 - val_loss: 0.2650 - val_accuracy: 0.9447\n",
      "Epoch 17/100\n",
      "165/165 - 0s - loss: 0.0739 - accuracy: 0.9764 - val_loss: 0.2543 - val_accuracy: 0.9448\n",
      "Epoch 18/100\n",
      "165/165 - 0s - loss: 0.0686 - accuracy: 0.9781 - val_loss: 0.2696 - val_accuracy: 0.9462\n",
      "Epoch 19/100\n",
      "165/165 - 0s - loss: 0.0697 - accuracy: 0.9781 - val_loss: 0.2905 - val_accuracy: 0.9407\n",
      "Epoch 20/100\n",
      "165/165 - 0s - loss: 0.0919 - accuracy: 0.9722 - val_loss: 0.2765 - val_accuracy: 0.9453\n",
      "Epoch 21/100\n",
      "165/165 - 0s - loss: 0.0577 - accuracy: 0.9811 - val_loss: 0.2703 - val_accuracy: 0.9506\n",
      "Epoch 22/100\n",
      "165/165 - 0s - loss: 0.0516 - accuracy: 0.9832 - val_loss: 0.2628 - val_accuracy: 0.9499\n",
      "Epoch 23/100\n",
      "165/165 - 0s - loss: 0.0775 - accuracy: 0.9760 - val_loss: 0.2741 - val_accuracy: 0.9513\n",
      "Epoch 24/100\n",
      "165/165 - 0s - loss: 0.0529 - accuracy: 0.9835 - val_loss: 0.2546 - val_accuracy: 0.9513\n",
      "Epoch 25/100\n",
      "165/165 - 0s - loss: 0.0528 - accuracy: 0.9835 - val_loss: 0.2680 - val_accuracy: 0.9509\n",
      "Epoch 26/100\n",
      "165/165 - 0s - loss: 0.0473 - accuracy: 0.9847 - val_loss: 0.2710 - val_accuracy: 0.9541\n",
      "Epoch 27/100\n",
      "165/165 - 0s - loss: 0.0546 - accuracy: 0.9830 - val_loss: 0.3161 - val_accuracy: 0.9457\n",
      "Epoch 28/100\n",
      "165/165 - 0s - loss: 0.0483 - accuracy: 0.9852 - val_loss: 0.2846 - val_accuracy: 0.9531\n",
      "Epoch 29/100\n",
      "165/165 - 0s - loss: 0.0473 - accuracy: 0.9854 - val_loss: 0.2885 - val_accuracy: 0.9533\n",
      "Epoch 30/100\n",
      "165/165 - 0s - loss: 0.0602 - accuracy: 0.9812 - val_loss: 0.3132 - val_accuracy: 0.9517\n",
      "Epoch 31/100\n",
      "165/165 - 0s - loss: 0.0495 - accuracy: 0.9848 - val_loss: 0.2883 - val_accuracy: 0.9526\n",
      "Epoch 32/100\n",
      "165/165 - 0s - loss: 0.0398 - accuracy: 0.9874 - val_loss: 0.2979 - val_accuracy: 0.9517\n",
      "Epoch 33/100\n",
      "165/165 - 0s - loss: 0.0435 - accuracy: 0.9865 - val_loss: 0.2809 - val_accuracy: 0.9567\n",
      "Epoch 34/100\n",
      "165/165 - 0s - loss: 0.0296 - accuracy: 0.9901 - val_loss: 0.2841 - val_accuracy: 0.9553\n",
      "Epoch 35/100\n",
      "165/165 - 0s - loss: 0.0273 - accuracy: 0.9906 - val_loss: 0.3017 - val_accuracy: 0.9569\n",
      "Epoch 36/100\n",
      "165/165 - 0s - loss: 0.0313 - accuracy: 0.9898 - val_loss: 0.2897 - val_accuracy: 0.9567\n",
      "Epoch 37/100\n",
      "165/165 - 0s - loss: 0.0341 - accuracy: 0.9894 - val_loss: 0.3081 - val_accuracy: 0.9546\n",
      "Epoch 38/100\n",
      "165/165 - 0s - loss: 0.0311 - accuracy: 0.9902 - val_loss: 0.3194 - val_accuracy: 0.9551\n",
      "Epoch 39/100\n",
      "165/165 - 0s - loss: 0.0384 - accuracy: 0.9877 - val_loss: 0.3460 - val_accuracy: 0.9516\n",
      "Epoch 40/100\n",
      "165/165 - 0s - loss: 0.0434 - accuracy: 0.9870 - val_loss: 0.3223 - val_accuracy: 0.9521\n",
      "Epoch 41/100\n",
      "165/165 - 0s - loss: 0.1025 - accuracy: 0.9756 - val_loss: 0.3126 - val_accuracy: 0.9522\n",
      "Epoch 42/100\n",
      "165/165 - 0s - loss: 0.0399 - accuracy: 0.9885 - val_loss: 0.3019 - val_accuracy: 0.9555\n",
      "Epoch 43/100\n",
      "165/165 - 0s - loss: 0.0238 - accuracy: 0.9921 - val_loss: 0.2830 - val_accuracy: 0.9593\n",
      "Epoch 44/100\n",
      "165/165 - 0s - loss: 0.0240 - accuracy: 0.9921 - val_loss: 0.3000 - val_accuracy: 0.9568\n",
      "Epoch 45/100\n",
      "165/165 - 0s - loss: 0.0199 - accuracy: 0.9937 - val_loss: 0.2995 - val_accuracy: 0.9586\n",
      "Epoch 46/100\n",
      "165/165 - 0s - loss: 0.0225 - accuracy: 0.9928 - val_loss: 0.2937 - val_accuracy: 0.9602\n",
      "Epoch 47/100\n",
      "165/165 - 0s - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.3207 - val_accuracy: 0.9573\n",
      "Epoch 48/100\n",
      "165/165 - 0s - loss: 0.0344 - accuracy: 0.9902 - val_loss: 0.3276 - val_accuracy: 0.9573\n",
      "Epoch 49/100\n",
      "165/165 - 0s - loss: 0.0320 - accuracy: 0.9908 - val_loss: 0.3553 - val_accuracy: 0.9554\n",
      "Epoch 50/100\n",
      "165/165 - 0s - loss: 0.0289 - accuracy: 0.9919 - val_loss: 0.3443 - val_accuracy: 0.9581\n",
      "Epoch 51/100\n",
      "165/165 - 0s - loss: 0.0291 - accuracy: 0.9915 - val_loss: 0.3385 - val_accuracy: 0.9569\n",
      "Epoch 52/100\n",
      "165/165 - 0s - loss: 0.0255 - accuracy: 0.9918 - val_loss: 0.3423 - val_accuracy: 0.9572\n",
      "Epoch 53/100\n",
      "165/165 - 0s - loss: 0.0272 - accuracy: 0.9912 - val_loss: 0.3409 - val_accuracy: 0.9596\n",
      "Epoch 54/100\n",
      "165/165 - 0s - loss: 0.0255 - accuracy: 0.9918 - val_loss: 0.3536 - val_accuracy: 0.9572\n",
      "Epoch 55/100\n",
      "165/165 - 0s - loss: 0.0268 - accuracy: 0.9926 - val_loss: 0.3336 - val_accuracy: 0.9589\n",
      "Epoch 56/100\n",
      "165/165 - 0s - loss: 0.0238 - accuracy: 0.9927 - val_loss: 0.3606 - val_accuracy: 0.9569\n",
      "Epoch 57/100\n",
      "165/165 - 0s - loss: 0.0186 - accuracy: 0.9941 - val_loss: 0.3869 - val_accuracy: 0.9546\n",
      "Epoch 58/100\n",
      "165/165 - 0s - loss: 0.0285 - accuracy: 0.9914 - val_loss: 0.3573 - val_accuracy: 0.9573\n",
      "Epoch 59/100\n",
      "165/165 - 0s - loss: 0.0674 - accuracy: 0.9849 - val_loss: 0.3403 - val_accuracy: 0.9573\n",
      "Epoch 60/100\n",
      "165/165 - 0s - loss: 0.0275 - accuracy: 0.9915 - val_loss: 0.3486 - val_accuracy: 0.9586\n",
      "Epoch 61/100\n",
      "165/165 - 0s - loss: 0.0186 - accuracy: 0.9944 - val_loss: 0.3231 - val_accuracy: 0.9597\n",
      "Epoch 62/100\n",
      "165/165 - 0s - loss: 0.0093 - accuracy: 0.9967 - val_loss: 0.3221 - val_accuracy: 0.9630\n",
      "Epoch 63/100\n",
      "165/165 - 0s - loss: 0.0135 - accuracy: 0.9957 - val_loss: 0.3428 - val_accuracy: 0.9600\n",
      "Epoch 64/100\n",
      "165/165 - 0s - loss: 0.0167 - accuracy: 0.9949 - val_loss: 0.3587 - val_accuracy: 0.9608\n",
      "Epoch 65/100\n",
      "165/165 - 0s - loss: 0.0235 - accuracy: 0.9943 - val_loss: 0.3333 - val_accuracy: 0.9613\n",
      "Epoch 66/100\n",
      "165/165 - 0s - loss: 0.0304 - accuracy: 0.9912 - val_loss: 0.3694 - val_accuracy: 0.9586\n",
      "Epoch 67/100\n",
      "165/165 - 0s - loss: 0.0186 - accuracy: 0.9947 - val_loss: 0.3396 - val_accuracy: 0.9624\n",
      "Epoch 68/100\n",
      "165/165 - 0s - loss: 0.0168 - accuracy: 0.9953 - val_loss: 0.3773 - val_accuracy: 0.9598\n",
      "Epoch 69/100\n",
      "165/165 - 0s - loss: 0.0211 - accuracy: 0.9936 - val_loss: 0.3917 - val_accuracy: 0.9562\n",
      "Epoch 70/100\n",
      "165/165 - 0s - loss: 0.0170 - accuracy: 0.9947 - val_loss: 0.3863 - val_accuracy: 0.9589\n",
      "Epoch 71/100\n",
      "165/165 - 0s - loss: 0.0233 - accuracy: 0.9936 - val_loss: 0.4256 - val_accuracy: 0.9542\n",
      "Epoch 72/100\n",
      "165/165 - 0s - loss: 0.0357 - accuracy: 0.9903 - val_loss: 0.3354 - val_accuracy: 0.9603\n",
      "Epoch 73/100\n",
      "165/165 - 0s - loss: 0.0167 - accuracy: 0.9950 - val_loss: 0.3747 - val_accuracy: 0.9598\n",
      "Epoch 74/100\n",
      "165/165 - 0s - loss: 0.0187 - accuracy: 0.9947 - val_loss: 0.3593 - val_accuracy: 0.9608\n",
      "Epoch 75/100\n",
      "165/165 - 0s - loss: 0.0133 - accuracy: 0.9959 - val_loss: 0.3583 - val_accuracy: 0.9611\n",
      "Epoch 76/100\n",
      "165/165 - 0s - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.3694 - val_accuracy: 0.9606\n",
      "Epoch 77/100\n",
      "165/165 - 0s - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.3344 - val_accuracy: 0.9641\n",
      "Epoch 78/100\n",
      "165/165 - 0s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.3900 - val_accuracy: 0.9619\n",
      "Epoch 79/100\n",
      "165/165 - 0s - loss: 0.0370 - accuracy: 0.9913 - val_loss: 0.4086 - val_accuracy: 0.9533\n",
      "Epoch 80/100\n",
      "165/165 - 0s - loss: 0.0358 - accuracy: 0.9906 - val_loss: 0.3736 - val_accuracy: 0.9594\n",
      "Epoch 81/100\n",
      "165/165 - 0s - loss: 0.0497 - accuracy: 0.9885 - val_loss: 0.3486 - val_accuracy: 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "165/165 - 0s - loss: 0.0167 - accuracy: 0.9949 - val_loss: 0.3238 - val_accuracy: 0.9644\n",
      "Epoch 83/100\n",
      "165/165 - 0s - loss: 0.0227 - accuracy: 0.9941 - val_loss: 0.3383 - val_accuracy: 0.9631\n",
      "Epoch 84/100\n",
      "165/165 - 0s - loss: 0.0164 - accuracy: 0.9949 - val_loss: 0.3440 - val_accuracy: 0.9625\n",
      "Epoch 85/100\n",
      "165/165 - 0s - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.3310 - val_accuracy: 0.9659\n",
      "Epoch 86/100\n",
      "165/165 - 0s - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.3515 - val_accuracy: 0.9656\n",
      "Epoch 87/100\n",
      "165/165 - 0s - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.3557 - val_accuracy: 0.9644\n",
      "Epoch 88/100\n",
      "165/165 - 0s - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.3730 - val_accuracy: 0.9614\n",
      "Epoch 89/100\n",
      "165/165 - 0s - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.3741 - val_accuracy: 0.9638\n",
      "Epoch 90/100\n",
      "165/165 - 0s - loss: 0.0272 - accuracy: 0.9934 - val_loss: 0.4132 - val_accuracy: 0.9566\n",
      "Epoch 91/100\n",
      "165/165 - 0s - loss: 0.0436 - accuracy: 0.9896 - val_loss: 0.4021 - val_accuracy: 0.9581\n",
      "Epoch 92/100\n",
      "165/165 - 0s - loss: 0.0220 - accuracy: 0.9937 - val_loss: 0.3770 - val_accuracy: 0.9604\n",
      "Epoch 93/100\n",
      "165/165 - 0s - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.3538 - val_accuracy: 0.9638\n",
      "Epoch 94/100\n",
      "165/165 - 0s - loss: 0.0104 - accuracy: 0.9970 - val_loss: 0.3610 - val_accuracy: 0.9646\n",
      "Epoch 95/100\n",
      "165/165 - 0s - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.3660 - val_accuracy: 0.9632\n",
      "Epoch 96/100\n",
      "165/165 - 0s - loss: 0.0114 - accuracy: 0.9967 - val_loss: 0.3867 - val_accuracy: 0.9631\n",
      "Epoch 97/100\n",
      "165/165 - 0s - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.3464 - val_accuracy: 0.9634\n",
      "Epoch 98/100\n",
      "165/165 - 0s - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.3610 - val_accuracy: 0.9650\n",
      "Epoch 99/100\n",
      "165/165 - 0s - loss: 0.0106 - accuracy: 0.9970 - val_loss: 0.4007 - val_accuracy: 0.9603\n",
      "Epoch 100/100\n",
      "165/165 - 0s - loss: 0.0273 - accuracy: 0.9933 - val_loss: 0.4027 - val_accuracy: 0.9606\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=256, epochs=100, validation_split=0.3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 644us/step - loss: 0.3482 - accuracy: 0.9643\n",
      "[0.34821102023124695, 0.9642999768257141]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型儲存  \n",
    "ref: https://blog.csdn.net/qq_27825451/article/details/105505033?fbclid=IwAR3Tzb3l9bI9qfvvsnkeg5MbpNKwNBqBKkbk2bBug_pPVEcgPGxizpdbb4I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)   (60000,)\n",
      "(10000, 784)   (10000,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "165/165 - 0s - loss: 3.2502 - accuracy: 0.7316 - val_loss: 0.8427 - val_accuracy: 0.8461\n",
      "Epoch 2/10\n",
      "165/165 - 0s - loss: 0.6186 - accuracy: 0.8730 - val_loss: 0.5537 - val_accuracy: 0.8846\n",
      "Epoch 3/10\n",
      "165/165 - 0s - loss: 0.3914 - accuracy: 0.9054 - val_loss: 0.4254 - val_accuracy: 0.9051\n",
      "Epoch 4/10\n",
      "165/165 - 0s - loss: 0.2792 - accuracy: 0.9257 - val_loss: 0.3818 - val_accuracy: 0.9133\n",
      "Epoch 5/10\n",
      "165/165 - 0s - loss: 0.2224 - accuracy: 0.9385 - val_loss: 0.3450 - val_accuracy: 0.9192\n",
      "Epoch 6/10\n",
      "165/165 - 0s - loss: 0.1846 - accuracy: 0.9465 - val_loss: 0.3155 - val_accuracy: 0.9284\n",
      "Epoch 7/10\n",
      "165/165 - 0s - loss: 0.1559 - accuracy: 0.9544 - val_loss: 0.3028 - val_accuracy: 0.9286\n",
      "Epoch 8/10\n",
      "165/165 - 0s - loss: 0.1413 - accuracy: 0.9576 - val_loss: 0.2932 - val_accuracy: 0.9321\n",
      "Epoch 9/10\n",
      "165/165 - 0s - loss: 0.1214 - accuracy: 0.9632 - val_loss: 0.2942 - val_accuracy: 0.9339\n",
      "Epoch 10/10\n",
      "165/165 - 0s - loss: 0.1065 - accuracy: 0.9672 - val_loss: 0.2744 - val_accuracy: 0.9402\n",
      "313/313 [==============================] - 0s 620us/step - loss: 0.2904 - accuracy: 0.9379\n",
      "[0.2904110848903656, 0.9379000067710876]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])\n",
    "print(x_train.shape, ' ', y_train.shape)\n",
    "print(x_test.shape, ' ', y_test.shape)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#keras.optimizers.Adam(learning_rate=0.01)\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# provide labels as one_hot representation => tf.keras.losses.CategoricalCrossentropy\n",
    "# provide labels as integers => tf.keras.losses.SparseCategoricalCrossentropy \n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train, batch_size=256, epochs=10, validation_split=0.3, verbose=2)\n",
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 0s 752us/step - loss: 0.2904 - accuracy: 0.9379\n",
      "[0.2904110848903656, 0.9379000067710876]\n"
     ]
    }
   ],
   "source": [
    "# save and restore method1: restore as h5 file\n",
    "model.save('dnn_mnist_model.h5')\n",
    "new_model = keras.models.load_model('dnn_mnist_model.h5')\n",
    "new_model.summary()\n",
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: saved_model\\assets\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 0s 675us/step - loss: 0.2904 - accuracy: 0.9379\n",
      "[0.2904110848903656, 0.9379000067710876]\n"
     ]
    }
   ],
   "source": [
    "# save and restore method2 \n",
    "model.save('saved_model', save_format='tf')\n",
    "new_model = keras.models.load_model('saved_model')\n",
    "new_model.summary()\n",
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 0s 637us/step - loss: 0.2904 - accuracy: 0.9379\n",
      "[0.2904110848903656, 0.9379000067710876]\n"
     ]
    }
   ],
   "source": [
    "# save and restore method3\n",
    "json_config = model.to_json()\n",
    "with open('model_config.json', 'w') as json_file:\n",
    "    json_file.write(json_config)\n",
    "\n",
    "model.save_weights('dnn_mnist_model2.h5')\n",
    " \n",
    "with open('model_config.json') as json_file:\n",
    "    json_config = json_file.read()\n",
    "new_model = keras.models.model_from_json(json_config)\n",
    " \n",
    "new_model.load_weights('dnn_mnist_model2.h5')\n",
    "new_model.summary()\n",
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)   (60000,)\n",
      "(10000, 784)   (10000,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.84267, saving model to saved_model2\n",
      "INFO:tensorflow:Assets written to: saved_model2\\assets\n",
      "165/165 - 1s - loss: 3.1228 - accuracy: 0.7300 - val_loss: 0.8536 - val_accuracy: 0.8427\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.84267 to 0.87506, saving model to saved_model2\n",
      "INFO:tensorflow:Assets written to: saved_model2\\assets\n",
      "165/165 - 1s - loss: 0.6074 - accuracy: 0.8696 - val_loss: 0.5744 - val_accuracy: 0.8751\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.87506 to 0.89583, saving model to saved_model2\n",
      "INFO:tensorflow:Assets written to: saved_model2\\assets\n",
      "165/165 - 1s - loss: 0.4043 - accuracy: 0.9017 - val_loss: 0.4533 - val_accuracy: 0.8958\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.89583 to 0.91100, saving model to saved_model2\n",
      "INFO:tensorflow:Assets written to: saved_model2\\assets\n",
      "165/165 - 1s - loss: 0.2882 - accuracy: 0.9235 - val_loss: 0.3757 - val_accuracy: 0.9110\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.91100 to 0.91433, saving model to saved_model2\n",
      "INFO:tensorflow:Assets written to: saved_model2\\assets\n",
      "165/165 - 1s - loss: 0.2262 - accuracy: 0.9372 - val_loss: 0.3595 - val_accuracy: 0.9143\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.91433 to 0.92483, saving model to saved_model2\n",
      "INFO:tensorflow:Assets written to: saved_model2\\assets\n",
      "165/165 - 1s - loss: 0.1897 - accuracy: 0.9464 - val_loss: 0.3268 - val_accuracy: 0.9248\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.92483 to 0.92544, saving model to saved_model2\n",
      "INFO:tensorflow:Assets written to: saved_model2\\assets\n",
      "165/165 - 1s - loss: 0.1587 - accuracy: 0.9534 - val_loss: 0.3215 - val_accuracy: 0.9254\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.92544 to 0.93333, saving model to saved_model2\n",
      "INFO:tensorflow:Assets written to: saved_model2\\assets\n",
      "165/165 - 1s - loss: 0.1428 - accuracy: 0.9577 - val_loss: 0.2898 - val_accuracy: 0.9333\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.93333 to 0.93561, saving model to saved_model2\n",
      "INFO:tensorflow:Assets written to: saved_model2\\assets\n",
      "165/165 - 1s - loss: 0.1228 - accuracy: 0.9623 - val_loss: 0.2843 - val_accuracy: 0.9356\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.93561\n",
      "165/165 - 0s - loss: 0.1159 - accuracy: 0.9657 - val_loss: 0.2766 - val_accuracy: 0.9349\n",
      "313/313 [==============================] - 0s 628us/step - loss: 0.2567 - accuracy: 0.9397\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])\n",
    "print(x_train.shape, ' ', y_train.shape)\n",
    "print(x_test.shape, ' ', y_test.shape)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#keras.optimizers.Adam(learning_rate=0.01)\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# provide labels as one_hot representation => tf.keras.losses.CategoricalCrossentropy\n",
    "# provide labels as integers => tf.keras.losses.SparseCategoricalCrossentropy \n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint('saved_model2', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=256, epochs=10, validation_split=0.3, verbose=2, callbacks=[checkpoint])\n",
    "result = model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
