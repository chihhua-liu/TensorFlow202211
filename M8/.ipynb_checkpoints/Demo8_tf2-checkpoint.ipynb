{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上正則向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.355628\n",
      "batch 1: loss 2.182877\n",
      "batch 2: loss 2.110263\n",
      "batch 3: loss 2.015832\n",
      "batch 4: loss 1.922021\n",
      "batch 5: loss 1.978993\n",
      "batch 6: loss 2.069015\n",
      "batch 7: loss 1.865456\n",
      "batch 8: loss 1.859898\n",
      "batch 9: loss 1.712597\n",
      "batch 10: loss 1.659049\n",
      "batch 11: loss 1.665439\n",
      "batch 12: loss 1.547476\n",
      "batch 13: loss 1.508293\n",
      "batch 14: loss 1.659315\n",
      "batch 15: loss 1.489922\n",
      "batch 16: loss 1.312801\n",
      "batch 17: loss 1.291167\n",
      "batch 18: loss 1.151451\n",
      "batch 19: loss 1.298414\n",
      "batch 20: loss 1.188966\n",
      "batch 21: loss 1.162551\n",
      "batch 22: loss 1.093741\n",
      "batch 23: loss 1.136553\n",
      "batch 24: loss 1.126664\n",
      "batch 25: loss 0.899033\n",
      "batch 26: loss 1.176222\n",
      "batch 27: loss 0.872691\n",
      "batch 28: loss 0.983082\n",
      "batch 29: loss 0.858800\n",
      "batch 30: loss 0.971009\n",
      "batch 31: loss 0.811347\n",
      "batch 32: loss 0.705345\n",
      "batch 33: loss 0.935347\n",
      "batch 34: loss 0.716346\n",
      "batch 35: loss 0.925834\n",
      "batch 36: loss 0.683442\n",
      "batch 37: loss 0.670496\n",
      "batch 38: loss 0.636420\n",
      "batch 39: loss 0.765775\n",
      "batch 40: loss 0.751668\n",
      "batch 41: loss 0.574440\n",
      "batch 42: loss 0.628535\n",
      "batch 43: loss 0.538687\n",
      "batch 44: loss 0.564701\n",
      "batch 45: loss 0.632644\n",
      "batch 46: loss 0.564710\n",
      "batch 47: loss 0.460164\n",
      "batch 48: loss 0.592807\n",
      "batch 49: loss 0.649865\n",
      "batch 50: loss 0.632823\n",
      "batch 51: loss 0.611291\n",
      "batch 52: loss 0.564503\n",
      "batch 53: loss 0.685565\n",
      "batch 54: loss 0.725019\n",
      "batch 55: loss 0.674294\n",
      "batch 56: loss 0.423701\n",
      "batch 57: loss 0.789666\n",
      "batch 58: loss 0.634448\n",
      "batch 59: loss 0.536071\n",
      "batch 60: loss 0.544219\n",
      "batch 61: loss 0.760420\n",
      "batch 62: loss 0.402272\n",
      "batch 63: loss 0.456139\n",
      "batch 64: loss 0.455368\n",
      "batch 65: loss 0.687685\n",
      "batch 66: loss 0.511675\n",
      "batch 67: loss 0.408131\n",
      "batch 68: loss 0.571216\n",
      "batch 69: loss 0.480938\n",
      "batch 70: loss 0.617711\n",
      "batch 71: loss 0.434884\n",
      "batch 72: loss 0.568036\n",
      "batch 73: loss 0.429909\n",
      "batch 74: loss 0.393827\n",
      "batch 75: loss 0.585122\n",
      "batch 76: loss 0.508015\n",
      "batch 77: loss 0.418705\n",
      "batch 78: loss 0.529425\n",
      "batch 79: loss 0.662515\n",
      "batch 80: loss 0.568012\n",
      "batch 81: loss 0.483995\n",
      "batch 82: loss 0.402792\n",
      "batch 83: loss 0.310399\n",
      "batch 84: loss 0.388006\n",
      "batch 85: loss 0.261131\n",
      "batch 86: loss 0.557948\n",
      "batch 87: loss 0.435347\n",
      "batch 88: loss 0.446811\n",
      "batch 89: loss 0.683968\n",
      "batch 90: loss 0.404806\n",
      "batch 91: loss 0.766859\n",
      "batch 92: loss 0.633395\n",
      "batch 93: loss 0.329104\n",
      "batch 94: loss 0.510709\n",
      "batch 95: loss 0.415985\n",
      "batch 96: loss 0.508909\n",
      "batch 97: loss 0.480044\n",
      "batch 98: loss 0.383168\n",
      "batch 99: loss 0.465189\n",
      "batch 100: loss 0.429182\n",
      "batch 101: loss 0.467422\n",
      "batch 102: loss 0.374609\n",
      "batch 103: loss 0.439132\n",
      "batch 104: loss 0.682544\n",
      "batch 105: loss 0.417786\n",
      "batch 106: loss 0.403286\n",
      "batch 107: loss 0.363756\n",
      "batch 108: loss 0.357069\n",
      "batch 109: loss 0.370834\n",
      "batch 110: loss 0.288708\n",
      "batch 111: loss 0.465544\n",
      "batch 112: loss 0.420782\n",
      "batch 113: loss 0.234804\n",
      "batch 114: loss 0.450076\n",
      "batch 115: loss 0.512473\n",
      "batch 116: loss 0.480293\n",
      "batch 117: loss 0.364903\n",
      "batch 118: loss 0.316699\n",
      "batch 119: loss 0.471038\n",
      "batch 120: loss 0.522140\n",
      "batch 121: loss 0.449052\n",
      "batch 122: loss 0.447156\n",
      "batch 123: loss 0.480623\n",
      "batch 124: loss 0.259112\n",
      "batch 125: loss 0.355736\n",
      "batch 126: loss 0.277777\n",
      "batch 127: loss 0.612450\n",
      "batch 128: loss 0.284701\n",
      "batch 129: loss 0.267034\n",
      "batch 130: loss 0.524086\n",
      "batch 131: loss 0.400837\n",
      "batch 132: loss 0.351486\n",
      "batch 133: loss 0.292358\n",
      "batch 134: loss 0.465361\n",
      "batch 135: loss 0.308419\n",
      "batch 136: loss 0.505829\n",
      "batch 137: loss 0.425691\n",
      "batch 138: loss 0.401170\n",
      "batch 139: loss 0.336674\n",
      "batch 140: loss 0.253794\n",
      "batch 141: loss 0.553580\n",
      "batch 142: loss 0.549928\n",
      "batch 143: loss 0.338160\n",
      "batch 144: loss 0.356474\n",
      "batch 145: loss 0.419008\n",
      "batch 146: loss 0.325505\n",
      "batch 147: loss 0.286553\n",
      "batch 148: loss 0.334565\n",
      "batch 149: loss 0.554134\n",
      "batch 150: loss 0.589756\n",
      "batch 151: loss 0.311767\n",
      "batch 152: loss 0.405393\n",
      "batch 153: loss 0.396619\n",
      "batch 154: loss 0.446363\n",
      "batch 155: loss 0.370960\n",
      "batch 156: loss 0.271114\n",
      "batch 157: loss 0.410372\n",
      "batch 158: loss 0.681465\n",
      "batch 159: loss 0.254134\n",
      "batch 160: loss 0.481986\n",
      "batch 161: loss 0.428441\n",
      "batch 162: loss 0.343259\n",
      "batch 163: loss 0.592906\n",
      "batch 164: loss 0.447791\n",
      "batch 165: loss 0.256946\n",
      "batch 166: loss 0.621910\n",
      "batch 167: loss 0.304489\n",
      "batch 168: loss 0.781775\n",
      "batch 169: loss 0.379332\n",
      "batch 170: loss 0.482942\n",
      "batch 171: loss 0.503505\n",
      "batch 172: loss 0.307325\n",
      "batch 173: loss 0.409231\n",
      "batch 174: loss 0.358708\n",
      "batch 175: loss 0.339114\n",
      "batch 176: loss 0.334917\n",
      "batch 177: loss 0.362131\n",
      "batch 178: loss 0.274647\n",
      "batch 179: loss 0.352221\n",
      "batch 180: loss 0.232132\n",
      "batch 181: loss 0.537649\n",
      "batch 182: loss 0.260489\n",
      "batch 183: loss 0.397189\n",
      "batch 184: loss 0.381234\n",
      "batch 185: loss 0.607815\n",
      "batch 186: loss 0.424633\n",
      "batch 187: loss 0.379936\n",
      "batch 188: loss 0.459724\n",
      "batch 189: loss 0.322195\n",
      "batch 190: loss 0.158018\n",
      "batch 191: loss 0.406309\n",
      "batch 192: loss 0.344306\n",
      "batch 193: loss 0.263158\n",
      "batch 194: loss 0.281802\n",
      "batch 195: loss 0.642117\n",
      "batch 196: loss 0.207221\n",
      "batch 197: loss 0.334659\n",
      "batch 198: loss 0.273551\n",
      "batch 199: loss 0.183561\n",
      "batch 200: loss 0.510184\n",
      "batch 201: loss 0.396229\n",
      "batch 202: loss 0.323391\n",
      "batch 203: loss 0.225869\n",
      "batch 204: loss 0.245430\n",
      "batch 205: loss 0.542341\n",
      "batch 206: loss 0.307874\n",
      "batch 207: loss 0.161707\n",
      "batch 208: loss 0.331836\n",
      "batch 209: loss 0.497951\n",
      "batch 210: loss 0.235610\n",
      "batch 211: loss 0.206251\n",
      "batch 212: loss 0.497831\n",
      "batch 213: loss 0.305369\n",
      "batch 214: loss 0.302588\n",
      "batch 215: loss 0.210849\n",
      "batch 216: loss 0.309320\n",
      "batch 217: loss 0.382884\n",
      "batch 218: loss 0.349392\n",
      "batch 219: loss 0.465816\n",
      "batch 220: loss 0.312977\n",
      "batch 221: loss 0.351754\n",
      "batch 222: loss 0.229413\n",
      "batch 223: loss 0.301690\n",
      "batch 224: loss 0.305768\n",
      "batch 225: loss 0.285144\n",
      "batch 226: loss 0.320052\n",
      "batch 227: loss 0.367545\n",
      "batch 228: loss 0.425450\n",
      "batch 229: loss 0.466975\n",
      "batch 230: loss 0.241751\n",
      "batch 231: loss 0.431156\n",
      "batch 232: loss 0.281467\n",
      "batch 233: loss 0.438200\n",
      "batch 234: loss 0.602036\n",
      "batch 235: loss 0.509849\n",
      "batch 236: loss 0.627169\n",
      "batch 237: loss 0.285270\n",
      "batch 238: loss 0.308366\n",
      "batch 239: loss 0.460802\n",
      "batch 240: loss 0.259753\n",
      "batch 241: loss 0.259748\n",
      "batch 242: loss 0.165587\n",
      "batch 243: loss 0.180334\n",
      "batch 244: loss 0.222404\n",
      "batch 245: loss 0.330884\n",
      "batch 246: loss 0.205674\n",
      "batch 247: loss 0.363853\n",
      "batch 248: loss 0.383279\n",
      "batch 249: loss 0.327416\n",
      "batch 250: loss 0.333412\n",
      "batch 251: loss 0.272257\n",
      "batch 252: loss 0.164648\n",
      "batch 253: loss 0.329181\n",
      "batch 254: loss 0.275272\n",
      "batch 255: loss 0.306561\n",
      "batch 256: loss 0.414515\n",
      "batch 257: loss 0.331009\n",
      "batch 258: loss 0.458401\n",
      "batch 259: loss 0.260594\n",
      "batch 260: loss 0.164567\n",
      "batch 261: loss 0.212059\n",
      "batch 262: loss 0.282983\n",
      "batch 263: loss 0.297578\n",
      "batch 264: loss 0.368988\n",
      "batch 265: loss 0.449989\n",
      "batch 266: loss 0.547949\n",
      "batch 267: loss 0.247354\n",
      "batch 268: loss 0.207441\n",
      "batch 269: loss 0.246240\n",
      "batch 270: loss 0.367345\n",
      "batch 271: loss 0.257318\n",
      "batch 272: loss 0.216319\n",
      "batch 273: loss 0.640924\n",
      "batch 274: loss 0.273129\n",
      "batch 275: loss 0.401647\n",
      "batch 276: loss 0.325119\n",
      "batch 277: loss 0.408739\n",
      "batch 278: loss 0.223406\n",
      "batch 279: loss 0.235121\n",
      "batch 280: loss 0.220661\n",
      "batch 281: loss 0.499422\n",
      "batch 282: loss 0.331757\n",
      "batch 283: loss 0.287255\n",
      "batch 284: loss 0.383093\n",
      "batch 285: loss 0.257279\n",
      "batch 286: loss 0.438859\n",
      "batch 287: loss 0.279479\n",
      "batch 288: loss 0.328677\n",
      "batch 289: loss 0.258146\n",
      "batch 290: loss 0.233406\n",
      "batch 291: loss 0.198330\n",
      "batch 292: loss 0.234838\n",
      "batch 293: loss 0.277856\n",
      "batch 294: loss 0.206417\n",
      "batch 295: loss 0.263259\n",
      "batch 296: loss 0.214441\n",
      "batch 297: loss 0.199298\n",
      "batch 298: loss 0.233097\n",
      "batch 299: loss 0.308520\n",
      "batch 300: loss 0.214838\n",
      "batch 301: loss 0.390128\n",
      "batch 302: loss 0.152883\n",
      "batch 303: loss 0.266605\n",
      "batch 304: loss 0.213345\n",
      "batch 305: loss 0.168214\n",
      "batch 306: loss 0.462677\n",
      "batch 307: loss 0.271789\n",
      "batch 308: loss 0.341956\n",
      "batch 309: loss 0.324848\n",
      "batch 310: loss 0.159434\n",
      "batch 311: loss 0.341108\n",
      "batch 312: loss 0.188425\n",
      "batch 313: loss 0.372631\n",
      "batch 314: loss 0.104537\n",
      "batch 315: loss 0.149321\n",
      "batch 316: loss 0.215032\n",
      "batch 317: loss 0.402465\n",
      "batch 318: loss 0.159671\n",
      "batch 319: loss 0.167449\n",
      "batch 320: loss 0.190074\n",
      "batch 321: loss 0.438006\n",
      "batch 322: loss 0.270318\n",
      "batch 323: loss 0.335459\n",
      "batch 324: loss 0.557439\n",
      "batch 325: loss 0.254780\n",
      "batch 326: loss 0.120618\n",
      "batch 327: loss 0.199456\n",
      "batch 328: loss 0.293371\n",
      "batch 329: loss 0.455744\n",
      "batch 330: loss 0.567474\n",
      "batch 331: loss 0.316106\n",
      "batch 332: loss 0.297397\n",
      "batch 333: loss 0.181016\n",
      "batch 334: loss 0.324423\n",
      "batch 335: loss 0.265177\n",
      "batch 336: loss 0.306032\n",
      "batch 337: loss 0.430222\n",
      "batch 338: loss 0.347364\n",
      "batch 339: loss 0.282784\n",
      "batch 340: loss 0.281464\n",
      "batch 341: loss 0.423440\n",
      "batch 342: loss 0.335292\n",
      "batch 343: loss 0.523319\n",
      "batch 344: loss 0.284232\n",
      "batch 345: loss 0.231435\n",
      "batch 346: loss 0.185271\n",
      "batch 347: loss 0.138101\n",
      "batch 348: loss 0.329939\n",
      "batch 349: loss 0.422766\n",
      "batch 350: loss 0.468074\n",
      "batch 351: loss 0.334659\n",
      "batch 352: loss 0.136764\n",
      "batch 353: loss 0.205800\n",
      "batch 354: loss 0.262733\n",
      "batch 355: loss 0.438552\n",
      "batch 356: loss 0.265792\n",
      "batch 357: loss 0.278639\n",
      "batch 358: loss 0.205657\n",
      "batch 359: loss 0.194116\n",
      "batch 360: loss 0.424639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 361: loss 0.198288\n",
      "batch 362: loss 0.204222\n",
      "batch 363: loss 0.392609\n",
      "batch 364: loss 0.429585\n",
      "batch 365: loss 0.207812\n",
      "batch 366: loss 0.226282\n",
      "batch 367: loss 0.233647\n",
      "batch 368: loss 0.177145\n",
      "batch 369: loss 0.228648\n",
      "batch 370: loss 0.187268\n",
      "batch 371: loss 0.309484\n",
      "batch 372: loss 0.389968\n",
      "batch 373: loss 0.276500\n",
      "batch 374: loss 0.421231\n",
      "batch 375: loss 0.351088\n",
      "batch 376: loss 0.151322\n",
      "batch 377: loss 0.282568\n",
      "batch 378: loss 0.289130\n",
      "batch 379: loss 0.185934\n",
      "batch 380: loss 0.363021\n",
      "batch 381: loss 0.185297\n",
      "batch 382: loss 0.293696\n",
      "batch 383: loss 0.150312\n",
      "batch 384: loss 0.205409\n",
      "batch 385: loss 0.243246\n",
      "batch 386: loss 0.534251\n",
      "batch 387: loss 0.147858\n",
      "batch 388: loss 0.162614\n",
      "batch 389: loss 0.196077\n",
      "batch 390: loss 0.307716\n",
      "batch 391: loss 0.150157\n",
      "batch 392: loss 0.254154\n",
      "batch 393: loss 0.180444\n",
      "batch 394: loss 0.227422\n",
      "batch 395: loss 0.280263\n",
      "batch 396: loss 0.178001\n",
      "batch 397: loss 0.208558\n",
      "batch 398: loss 0.302002\n",
      "batch 399: loss 0.179969\n",
      "batch 400: loss 0.167273\n",
      "batch 401: loss 0.165520\n",
      "batch 402: loss 0.174156\n",
      "batch 403: loss 0.515320\n",
      "batch 404: loss 0.151830\n",
      "batch 405: loss 0.295220\n",
      "batch 406: loss 0.258919\n",
      "batch 407: loss 0.220043\n",
      "batch 408: loss 0.245029\n",
      "batch 409: loss 0.283996\n",
      "batch 410: loss 0.279343\n",
      "batch 411: loss 0.105156\n",
      "batch 412: loss 0.156654\n",
      "batch 413: loss 0.344414\n",
      "batch 414: loss 0.336490\n",
      "batch 415: loss 0.218155\n",
      "batch 416: loss 0.297549\n",
      "batch 417: loss 0.156226\n",
      "batch 418: loss 0.240219\n",
      "batch 419: loss 0.349117\n",
      "batch 420: loss 0.125047\n",
      "batch 421: loss 0.212312\n",
      "batch 422: loss 0.276568\n",
      "batch 423: loss 0.270952\n",
      "batch 424: loss 0.283873\n",
      "batch 425: loss 0.198220\n",
      "batch 426: loss 0.175687\n",
      "batch 427: loss 0.157753\n",
      "batch 428: loss 0.216306\n",
      "batch 429: loss 0.421940\n",
      "batch 430: loss 0.166501\n",
      "batch 431: loss 0.136952\n",
      "batch 432: loss 0.260773\n",
      "batch 433: loss 0.342528\n",
      "batch 434: loss 0.240779\n",
      "batch 435: loss 0.332171\n",
      "batch 436: loss 0.407430\n",
      "batch 437: loss 0.071391\n",
      "batch 438: loss 0.344550\n",
      "batch 439: loss 0.261692\n",
      "batch 440: loss 0.094641\n",
      "batch 441: loss 0.236381\n",
      "batch 442: loss 0.125055\n",
      "batch 443: loss 0.171532\n",
      "batch 444: loss 0.231872\n",
      "batch 445: loss 0.093393\n",
      "batch 446: loss 0.142339\n",
      "batch 447: loss 0.358586\n",
      "batch 448: loss 0.263321\n",
      "batch 449: loss 0.070548\n",
      "batch 450: loss 0.131943\n",
      "batch 451: loss 0.417119\n",
      "batch 452: loss 0.184181\n",
      "batch 453: loss 0.207146\n",
      "batch 454: loss 0.153148\n",
      "batch 455: loss 0.387064\n",
      "batch 456: loss 0.093961\n",
      "batch 457: loss 0.210235\n",
      "batch 458: loss 0.290821\n",
      "batch 459: loss 0.277620\n",
      "batch 460: loss 0.338115\n",
      "batch 461: loss 0.324188\n",
      "batch 462: loss 0.266359\n",
      "batch 463: loss 0.199796\n",
      "batch 464: loss 0.087324\n",
      "batch 465: loss 0.307894\n",
      "batch 466: loss 0.152684\n",
      "batch 467: loss 0.229921\n",
      "batch 468: loss 0.267221\n",
      "batch 469: loss 0.312737\n",
      "batch 470: loss 0.654720\n",
      "batch 471: loss 0.335469\n",
      "batch 472: loss 0.255770\n",
      "batch 473: loss 0.278206\n",
      "batch 474: loss 0.170911\n",
      "batch 475: loss 0.585914\n",
      "batch 476: loss 0.175517\n",
      "batch 477: loss 0.216546\n",
      "batch 478: loss 0.209818\n",
      "batch 479: loss 0.241365\n",
      "batch 480: loss 0.278994\n",
      "batch 481: loss 0.291012\n",
      "batch 482: loss 0.277459\n",
      "batch 483: loss 0.284288\n",
      "batch 484: loss 0.271220\n",
      "batch 485: loss 0.317067\n",
      "batch 486: loss 0.151480\n",
      "batch 487: loss 0.143379\n",
      "batch 488: loss 0.431976\n",
      "batch 489: loss 0.143954\n",
      "batch 490: loss 0.217357\n",
      "batch 491: loss 0.139558\n",
      "batch 492: loss 0.321584\n",
      "batch 493: loss 0.235797\n",
      "batch 494: loss 0.188138\n",
      "batch 495: loss 0.252837\n",
      "batch 496: loss 0.164537\n",
      "batch 497: loss 0.139802\n",
      "batch 498: loss 0.170980\n",
      "batch 499: loss 0.266757\n",
      "batch 500: loss 0.162621\n",
      "batch 501: loss 0.200678\n",
      "batch 502: loss 0.292933\n",
      "batch 503: loss 0.205045\n",
      "batch 504: loss 0.242227\n",
      "batch 505: loss 0.318775\n",
      "batch 506: loss 0.218225\n",
      "batch 507: loss 0.371187\n",
      "batch 508: loss 0.270389\n",
      "batch 509: loss 0.260419\n",
      "batch 510: loss 0.118909\n",
      "batch 511: loss 0.152573\n",
      "batch 512: loss 0.081078\n",
      "batch 513: loss 0.385839\n",
      "batch 514: loss 0.211041\n",
      "batch 515: loss 0.197185\n",
      "batch 516: loss 0.337835\n",
      "batch 517: loss 0.280721\n",
      "batch 518: loss 0.467227\n",
      "batch 519: loss 0.307110\n",
      "batch 520: loss 0.329282\n",
      "batch 521: loss 0.262923\n",
      "batch 522: loss 0.529489\n",
      "batch 523: loss 0.175973\n",
      "batch 524: loss 0.115465\n",
      "batch 525: loss 0.362603\n",
      "batch 526: loss 0.340330\n",
      "batch 527: loss 0.178421\n",
      "batch 528: loss 0.185047\n",
      "batch 529: loss 0.331306\n",
      "batch 530: loss 0.363688\n",
      "batch 531: loss 0.199781\n",
      "batch 532: loss 0.262217\n",
      "batch 533: loss 0.411603\n",
      "batch 534: loss 0.420354\n",
      "batch 535: loss 0.204351\n",
      "batch 536: loss 0.225358\n",
      "batch 537: loss 0.065418\n",
      "batch 538: loss 0.128669\n",
      "batch 539: loss 0.293294\n",
      "batch 540: loss 0.246313\n",
      "batch 541: loss 0.192407\n",
      "batch 542: loss 0.433269\n",
      "batch 543: loss 0.126970\n",
      "batch 544: loss 0.347883\n",
      "batch 545: loss 0.285533\n",
      "batch 546: loss 0.246956\n",
      "batch 547: loss 0.314706\n",
      "batch 548: loss 0.283140\n",
      "batch 549: loss 0.428026\n",
      "batch 550: loss 0.374264\n",
      "batch 551: loss 0.284753\n",
      "batch 552: loss 0.191416\n",
      "batch 553: loss 0.198838\n",
      "batch 554: loss 0.239983\n",
      "batch 555: loss 0.203765\n",
      "batch 556: loss 0.154129\n",
      "batch 557: loss 0.386008\n",
      "batch 558: loss 0.177029\n",
      "batch 559: loss 0.260588\n",
      "batch 560: loss 0.112016\n",
      "batch 561: loss 0.297801\n",
      "batch 562: loss 0.481024\n",
      "batch 563: loss 0.344584\n",
      "batch 564: loss 0.184907\n",
      "batch 565: loss 0.287459\n",
      "batch 566: loss 0.177142\n",
      "batch 567: loss 0.228612\n",
      "batch 568: loss 0.172662\n",
      "batch 569: loss 0.201470\n",
      "batch 570: loss 0.229513\n",
      "batch 571: loss 0.234144\n",
      "batch 572: loss 0.174396\n",
      "batch 573: loss 0.045875\n",
      "batch 574: loss 0.220761\n",
      "batch 575: loss 0.354819\n",
      "batch 576: loss 0.246738\n",
      "batch 577: loss 0.234145\n",
      "batch 578: loss 0.296051\n",
      "batch 579: loss 0.152342\n",
      "batch 580: loss 0.247836\n",
      "batch 581: loss 0.141629\n",
      "batch 582: loss 0.203592\n",
      "batch 583: loss 0.279044\n",
      "batch 584: loss 0.316335\n",
      "batch 585: loss 0.170502\n",
      "batch 586: loss 0.288509\n",
      "batch 587: loss 0.180711\n",
      "batch 588: loss 0.095403\n",
      "batch 589: loss 0.185902\n",
      "batch 590: loss 0.197450\n",
      "batch 591: loss 0.056886\n",
      "batch 592: loss 0.280509\n",
      "batch 593: loss 0.350904\n",
      "batch 594: loss 0.277688\n",
      "batch 595: loss 0.189894\n",
      "batch 596: loss 0.376109\n",
      "batch 597: loss 0.176230\n",
      "batch 598: loss 0.216282\n",
      "batch 599: loss 0.148850\n",
      "batch 600: loss 0.138970\n",
      "batch 601: loss 0.227187\n",
      "batch 602: loss 0.130214\n",
      "batch 603: loss 0.102577\n",
      "batch 604: loss 0.174220\n",
      "batch 605: loss 0.253670\n",
      "batch 606: loss 0.090479\n",
      "batch 607: loss 0.150574\n",
      "batch 608: loss 0.136009\n",
      "batch 609: loss 0.233427\n",
      "batch 610: loss 0.359558\n",
      "batch 611: loss 0.227349\n",
      "batch 612: loss 0.240748\n",
      "batch 613: loss 0.153506\n",
      "batch 614: loss 0.277889\n",
      "batch 615: loss 0.177160\n",
      "batch 616: loss 0.157273\n",
      "batch 617: loss 0.107827\n",
      "batch 618: loss 0.121144\n",
      "batch 619: loss 0.288195\n",
      "batch 620: loss 0.113708\n",
      "batch 621: loss 0.165567\n",
      "batch 622: loss 0.111079\n",
      "batch 623: loss 0.368597\n",
      "batch 624: loss 0.072484\n",
      "batch 625: loss 0.201535\n",
      "batch 626: loss 0.093352\n",
      "batch 627: loss 0.491322\n",
      "batch 628: loss 0.190980\n",
      "batch 629: loss 0.190046\n",
      "batch 630: loss 0.147499\n",
      "batch 631: loss 0.340007\n",
      "batch 632: loss 0.267249\n",
      "batch 633: loss 0.278017\n",
      "batch 634: loss 0.182314\n",
      "batch 635: loss 0.246799\n",
      "batch 636: loss 0.138519\n",
      "batch 637: loss 0.216197\n",
      "batch 638: loss 0.145537\n",
      "batch 639: loss 0.197105\n",
      "batch 640: loss 0.305380\n",
      "batch 641: loss 0.261153\n",
      "batch 642: loss 0.188346\n",
      "batch 643: loss 0.276090\n",
      "batch 644: loss 0.307669\n",
      "batch 645: loss 0.212866\n",
      "batch 646: loss 0.218805\n",
      "batch 647: loss 0.172526\n",
      "batch 648: loss 0.413315\n",
      "batch 649: loss 0.195418\n",
      "batch 650: loss 0.251341\n",
      "batch 651: loss 0.335317\n",
      "batch 652: loss 0.100856\n",
      "batch 653: loss 0.205770\n",
      "batch 654: loss 0.204049\n",
      "batch 655: loss 0.239537\n",
      "batch 656: loss 0.274314\n",
      "batch 657: loss 0.131180\n",
      "batch 658: loss 0.129181\n",
      "batch 659: loss 0.063884\n",
      "batch 660: loss 0.529648\n",
      "batch 661: loss 0.210440\n",
      "batch 662: loss 0.519746\n",
      "batch 663: loss 0.203944\n",
      "batch 664: loss 0.240630\n",
      "batch 665: loss 0.124200\n",
      "batch 666: loss 0.153454\n",
      "batch 667: loss 0.236363\n",
      "batch 668: loss 0.158435\n",
      "batch 669: loss 0.222836\n",
      "batch 670: loss 0.137816\n",
      "batch 671: loss 0.273732\n",
      "batch 672: loss 0.338313\n",
      "batch 673: loss 0.355849\n",
      "batch 674: loss 0.466531\n",
      "batch 675: loss 0.198035\n",
      "batch 676: loss 0.104413\n",
      "batch 677: loss 0.126099\n",
      "batch 678: loss 0.241174\n",
      "batch 679: loss 0.248041\n",
      "batch 680: loss 0.208823\n",
      "batch 681: loss 0.164811\n",
      "batch 682: loss 0.061732\n",
      "batch 683: loss 0.208322\n",
      "batch 684: loss 0.127481\n",
      "batch 685: loss 0.289338\n",
      "batch 686: loss 0.186332\n",
      "batch 687: loss 0.194797\n",
      "batch 688: loss 0.143495\n",
      "batch 689: loss 0.280304\n",
      "batch 690: loss 0.178411\n",
      "batch 691: loss 0.432507\n",
      "batch 692: loss 0.273760\n",
      "batch 693: loss 0.422379\n",
      "batch 694: loss 0.329031\n",
      "batch 695: loss 0.305225\n",
      "batch 696: loss 0.185937\n",
      "batch 697: loss 0.227764\n",
      "batch 698: loss 0.170157\n",
      "batch 699: loss 0.180760\n",
      "batch 700: loss 0.172919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 701: loss 0.078842\n",
      "batch 702: loss 0.173240\n",
      "batch 703: loss 0.220704\n",
      "batch 704: loss 0.447445\n",
      "batch 705: loss 0.111000\n",
      "batch 706: loss 0.354454\n",
      "batch 707: loss 0.204895\n",
      "batch 708: loss 0.144724\n",
      "batch 709: loss 0.081692\n",
      "batch 710: loss 0.213199\n",
      "batch 711: loss 0.231329\n",
      "batch 712: loss 0.132877\n",
      "batch 713: loss 0.281272\n",
      "batch 714: loss 0.408756\n",
      "batch 715: loss 0.168518\n",
      "batch 716: loss 0.239556\n",
      "batch 717: loss 0.130466\n",
      "batch 718: loss 0.113562\n",
      "batch 719: loss 0.361526\n",
      "batch 720: loss 0.092073\n",
      "batch 721: loss 0.111096\n",
      "batch 722: loss 0.312562\n",
      "batch 723: loss 0.159329\n",
      "batch 724: loss 0.323645\n",
      "batch 725: loss 0.093828\n",
      "batch 726: loss 0.150611\n",
      "batch 727: loss 0.222713\n",
      "batch 728: loss 0.126348\n",
      "batch 729: loss 0.213858\n",
      "batch 730: loss 0.165899\n",
      "batch 731: loss 0.102664\n",
      "batch 732: loss 0.240372\n",
      "batch 733: loss 0.041348\n",
      "batch 734: loss 0.290801\n",
      "batch 735: loss 0.173212\n",
      "batch 736: loss 0.094161\n",
      "batch 737: loss 0.395226\n",
      "batch 738: loss 0.182474\n",
      "batch 739: loss 0.194468\n",
      "batch 740: loss 0.194869\n",
      "batch 741: loss 0.123795\n",
      "batch 742: loss 0.225746\n",
      "batch 743: loss 0.245021\n",
      "batch 744: loss 0.190457\n",
      "batch 745: loss 0.125905\n",
      "batch 746: loss 0.145113\n",
      "batch 747: loss 0.479711\n",
      "batch 748: loss 0.349649\n",
      "batch 749: loss 0.309176\n",
      "batch 750: loss 0.535765\n",
      "batch 751: loss 0.204174\n",
      "batch 752: loss 0.242514\n",
      "batch 753: loss 0.285027\n",
      "batch 754: loss 0.253127\n",
      "batch 755: loss 0.137431\n",
      "batch 756: loss 0.237409\n",
      "batch 757: loss 0.252868\n",
      "batch 758: loss 0.078642\n",
      "batch 759: loss 0.269039\n",
      "batch 760: loss 0.068117\n",
      "batch 761: loss 0.305436\n",
      "batch 762: loss 0.161983\n",
      "batch 763: loss 0.250443\n",
      "batch 764: loss 0.108653\n",
      "batch 765: loss 0.320198\n",
      "batch 766: loss 0.194969\n",
      "batch 767: loss 0.117560\n",
      "batch 768: loss 0.227196\n",
      "batch 769: loss 0.146337\n",
      "batch 770: loss 0.358873\n",
      "batch 771: loss 0.295371\n",
      "batch 772: loss 0.121209\n",
      "batch 773: loss 0.187825\n",
      "batch 774: loss 0.104628\n",
      "batch 775: loss 0.131760\n",
      "batch 776: loss 0.116164\n",
      "batch 777: loss 0.227947\n",
      "batch 778: loss 0.148093\n",
      "batch 779: loss 0.151885\n",
      "batch 780: loss 0.272961\n",
      "batch 781: loss 0.088326\n",
      "batch 782: loss 0.254504\n",
      "batch 783: loss 0.155311\n",
      "batch 784: loss 0.240040\n",
      "batch 785: loss 0.157706\n",
      "batch 786: loss 0.198186\n",
      "batch 787: loss 0.303193\n",
      "batch 788: loss 0.101085\n",
      "batch 789: loss 0.160585\n",
      "batch 790: loss 0.155904\n",
      "batch 791: loss 0.131153\n",
      "batch 792: loss 0.232187\n",
      "batch 793: loss 0.136988\n",
      "batch 794: loss 0.252427\n",
      "batch 795: loss 0.167174\n",
      "batch 796: loss 0.094575\n",
      "batch 797: loss 0.155281\n",
      "batch 798: loss 0.299982\n",
      "batch 799: loss 0.131230\n",
      "batch 800: loss 0.049143\n",
      "batch 801: loss 0.215347\n",
      "batch 802: loss 0.313885\n",
      "batch 803: loss 0.102267\n",
      "batch 804: loss 0.287432\n",
      "batch 805: loss 0.157472\n",
      "batch 806: loss 0.186504\n",
      "batch 807: loss 0.228285\n",
      "batch 808: loss 0.222069\n",
      "batch 809: loss 0.160490\n",
      "batch 810: loss 0.094235\n",
      "batch 811: loss 0.109206\n",
      "batch 812: loss 0.405697\n",
      "batch 813: loss 0.297295\n",
      "batch 814: loss 0.383220\n",
      "batch 815: loss 0.091722\n",
      "batch 816: loss 0.277540\n",
      "batch 817: loss 0.278450\n",
      "batch 818: loss 0.236702\n",
      "batch 819: loss 0.079789\n",
      "batch 820: loss 0.297961\n",
      "batch 821: loss 0.142825\n",
      "batch 822: loss 0.364040\n",
      "batch 823: loss 0.127998\n",
      "batch 824: loss 0.189026\n",
      "batch 825: loss 0.360161\n",
      "batch 826: loss 0.118031\n",
      "batch 827: loss 0.127955\n",
      "batch 828: loss 0.200906\n",
      "batch 829: loss 0.242654\n",
      "batch 830: loss 0.270499\n",
      "batch 831: loss 0.270242\n",
      "batch 832: loss 0.153150\n",
      "batch 833: loss 0.158401\n",
      "batch 834: loss 0.309761\n",
      "batch 835: loss 0.325285\n",
      "batch 836: loss 0.068201\n",
      "batch 837: loss 0.280787\n",
      "batch 838: loss 0.042672\n",
      "batch 839: loss 0.277120\n",
      "batch 840: loss 0.258565\n",
      "batch 841: loss 0.193493\n",
      "batch 842: loss 0.241837\n",
      "batch 843: loss 0.181395\n",
      "batch 844: loss 0.277389\n",
      "batch 845: loss 0.237126\n",
      "batch 846: loss 0.223122\n",
      "batch 847: loss 0.067762\n",
      "batch 848: loss 0.159998\n",
      "batch 849: loss 0.387886\n",
      "batch 850: loss 0.327962\n",
      "batch 851: loss 0.123502\n",
      "batch 852: loss 0.137851\n",
      "batch 853: loss 0.187691\n",
      "batch 854: loss 0.132123\n",
      "batch 855: loss 0.229201\n",
      "batch 856: loss 0.196119\n",
      "batch 857: loss 0.142473\n",
      "batch 858: loss 0.183516\n",
      "batch 859: loss 0.096835\n",
      "batch 860: loss 0.290747\n",
      "batch 861: loss 0.187610\n",
      "batch 862: loss 0.207749\n",
      "batch 863: loss 0.145096\n",
      "batch 864: loss 0.089766\n",
      "batch 865: loss 0.159233\n",
      "batch 866: loss 0.207882\n",
      "batch 867: loss 0.339092\n",
      "batch 868: loss 0.254585\n",
      "batch 869: loss 0.165999\n",
      "batch 870: loss 0.197453\n",
      "batch 871: loss 0.133747\n",
      "batch 872: loss 0.202832\n",
      "batch 873: loss 0.219469\n",
      "batch 874: loss 0.160368\n",
      "batch 875: loss 0.090577\n",
      "batch 876: loss 0.227367\n",
      "batch 877: loss 0.068838\n",
      "batch 878: loss 0.101509\n",
      "batch 879: loss 0.106498\n",
      "batch 880: loss 0.176159\n",
      "batch 881: loss 0.325303\n",
      "batch 882: loss 0.175586\n",
      "batch 883: loss 0.167252\n",
      "batch 884: loss 0.230238\n",
      "batch 885: loss 0.320094\n",
      "batch 886: loss 0.353974\n",
      "batch 887: loss 0.142037\n",
      "batch 888: loss 0.097486\n",
      "batch 889: loss 0.111216\n",
      "batch 890: loss 0.042569\n",
      "batch 891: loss 0.086633\n",
      "batch 892: loss 0.287475\n",
      "batch 893: loss 0.206968\n",
      "batch 894: loss 0.384914\n",
      "batch 895: loss 0.213592\n",
      "batch 896: loss 0.097162\n",
      "batch 897: loss 0.234337\n",
      "batch 898: loss 0.125134\n",
      "batch 899: loss 0.307486\n",
      "batch 900: loss 0.043997\n",
      "batch 901: loss 0.358942\n",
      "batch 902: loss 0.095413\n",
      "batch 903: loss 0.118263\n",
      "batch 904: loss 0.134315\n",
      "batch 905: loss 0.129946\n",
      "batch 906: loss 0.122226\n",
      "batch 907: loss 0.555302\n",
      "batch 908: loss 0.326759\n",
      "batch 909: loss 0.265375\n",
      "batch 910: loss 0.134224\n",
      "batch 911: loss 0.343741\n",
      "batch 912: loss 0.073732\n",
      "batch 913: loss 0.224732\n",
      "batch 914: loss 0.172714\n",
      "batch 915: loss 0.096244\n",
      "batch 916: loss 0.268023\n",
      "batch 917: loss 0.176576\n",
      "batch 918: loss 0.145218\n",
      "batch 919: loss 0.072577\n",
      "batch 920: loss 0.058042\n",
      "batch 921: loss 0.165622\n",
      "batch 922: loss 0.165371\n",
      "batch 923: loss 0.163650\n",
      "batch 924: loss 0.162388\n",
      "batch 925: loss 0.106242\n",
      "batch 926: loss 0.053525\n",
      "batch 927: loss 0.253530\n",
      "batch 928: loss 0.130471\n",
      "batch 929: loss 0.214603\n",
      "batch 930: loss 0.147349\n",
      "batch 931: loss 0.242256\n",
      "batch 932: loss 0.140945\n",
      "batch 933: loss 0.044997\n",
      "batch 934: loss 0.206301\n",
      "batch 935: loss 0.244081\n",
      "batch 936: loss 0.178840\n",
      "batch 937: loss 0.263807\n",
      "batch 938: loss 0.106221\n",
      "batch 939: loss 0.168820\n",
      "batch 940: loss 0.056825\n",
      "batch 941: loss 0.235121\n",
      "batch 942: loss 0.447089\n",
      "batch 943: loss 0.181148\n",
      "batch 944: loss 0.188835\n",
      "batch 945: loss 0.077981\n",
      "batch 946: loss 0.104798\n",
      "batch 947: loss 0.127183\n",
      "batch 948: loss 0.384668\n",
      "batch 949: loss 0.254191\n",
      "batch 950: loss 0.030107\n",
      "batch 951: loss 0.210883\n",
      "batch 952: loss 0.105787\n",
      "batch 953: loss 0.178328\n",
      "batch 954: loss 0.088203\n",
      "batch 955: loss 0.295724\n",
      "batch 956: loss 0.203291\n",
      "batch 957: loss 0.220735\n",
      "batch 958: loss 0.186806\n",
      "batch 959: loss 0.145146\n",
      "batch 960: loss 0.107845\n",
      "batch 961: loss 0.132072\n",
      "batch 962: loss 0.308530\n",
      "batch 963: loss 0.201369\n",
      "batch 964: loss 0.178503\n",
      "batch 965: loss 0.367752\n",
      "batch 966: loss 0.113978\n",
      "batch 967: loss 0.341984\n",
      "batch 968: loss 0.210460\n",
      "batch 969: loss 0.125356\n",
      "batch 970: loss 0.120926\n",
      "batch 971: loss 0.094617\n",
      "batch 972: loss 0.218902\n",
      "batch 973: loss 0.324580\n",
      "batch 974: loss 0.153023\n",
      "batch 975: loss 0.174250\n",
      "batch 976: loss 0.050847\n",
      "batch 977: loss 0.140521\n",
      "batch 978: loss 0.432094\n",
      "batch 979: loss 0.128815\n",
      "batch 980: loss 0.228141\n",
      "batch 981: loss 0.072014\n",
      "batch 982: loss 0.093747\n",
      "batch 983: loss 0.151589\n",
      "batch 984: loss 0.343650\n",
      "batch 985: loss 0.218406\n",
      "batch 986: loss 0.243588\n",
      "batch 987: loss 0.350990\n",
      "batch 988: loss 0.297183\n",
      "batch 989: loss 0.092491\n",
      "batch 990: loss 0.152149\n",
      "batch 991: loss 0.067513\n",
      "batch 992: loss 0.231300\n",
      "batch 993: loss 0.411783\n",
      "batch 994: loss 0.181014\n",
      "batch 995: loss 0.202284\n",
      "batch 996: loss 0.068329\n",
      "batch 997: loss 0.160872\n",
      "batch 998: loss 0.328826\n",
      "batch 999: loss 0.161698\n",
      "batch 1000: loss 0.450752\n",
      "batch 1001: loss 0.163287\n",
      "batch 1002: loss 0.177615\n",
      "batch 1003: loss 0.212435\n",
      "batch 1004: loss 0.133064\n",
      "batch 1005: loss 0.218129\n",
      "batch 1006: loss 0.181400\n",
      "batch 1007: loss 0.204477\n",
      "batch 1008: loss 0.144968\n",
      "batch 1009: loss 0.101742\n",
      "batch 1010: loss 0.083622\n",
      "batch 1011: loss 0.080828\n",
      "batch 1012: loss 0.155268\n",
      "batch 1013: loss 0.176507\n",
      "batch 1014: loss 0.120813\n",
      "batch 1015: loss 0.263974\n",
      "batch 1016: loss 0.154147\n",
      "batch 1017: loss 0.260836\n",
      "batch 1018: loss 0.089329\n",
      "batch 1019: loss 0.056995\n",
      "batch 1020: loss 0.103615\n",
      "batch 1021: loss 0.200788\n",
      "batch 1022: loss 0.248007\n",
      "batch 1023: loss 0.043191\n",
      "batch 1024: loss 0.103190\n",
      "batch 1025: loss 0.517713\n",
      "batch 1026: loss 0.260850\n",
      "batch 1027: loss 0.140450\n",
      "batch 1028: loss 0.156062\n",
      "batch 1029: loss 0.167304\n",
      "batch 1030: loss 0.180305\n",
      "batch 1031: loss 0.081610\n",
      "batch 1032: loss 0.170974\n",
      "batch 1033: loss 0.201564\n",
      "batch 1034: loss 0.090875\n",
      "batch 1035: loss 0.131850\n",
      "batch 1036: loss 0.199472\n",
      "batch 1037: loss 0.049251\n",
      "batch 1038: loss 0.065245\n",
      "batch 1039: loss 0.097491\n",
      "batch 1040: loss 0.212832\n",
      "batch 1041: loss 0.138956\n",
      "batch 1042: loss 0.159083\n",
      "batch 1043: loss 0.202880\n",
      "batch 1044: loss 0.141207\n",
      "batch 1045: loss 0.095760\n",
      "batch 1046: loss 0.102537\n",
      "batch 1047: loss 0.052821\n",
      "batch 1048: loss 0.267390\n",
      "batch 1049: loss 0.210128\n",
      "batch 1050: loss 0.134140\n",
      "batch 1051: loss 0.228937\n",
      "batch 1052: loss 0.158972\n",
      "batch 1053: loss 0.205741\n",
      "batch 1054: loss 0.047186\n",
      "batch 1055: loss 0.221128\n",
      "batch 1056: loss 0.127775\n",
      "batch 1057: loss 0.123280\n",
      "batch 1058: loss 0.117634\n",
      "batch 1059: loss 0.066343\n",
      "batch 1060: loss 0.342381\n",
      "batch 1061: loss 0.085021\n",
      "batch 1062: loss 0.198657\n",
      "batch 1063: loss 0.146343\n",
      "batch 1064: loss 0.171062\n",
      "batch 1065: loss 0.103097\n",
      "batch 1066: loss 0.101225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1067: loss 0.134571\n",
      "batch 1068: loss 0.217119\n",
      "batch 1069: loss 0.182993\n",
      "batch 1070: loss 0.134819\n",
      "batch 1071: loss 0.275676\n",
      "batch 1072: loss 0.032868\n",
      "batch 1073: loss 0.270422\n",
      "batch 1074: loss 0.166943\n",
      "batch 1075: loss 0.089149\n",
      "batch 1076: loss 0.173658\n",
      "batch 1077: loss 0.117611\n",
      "batch 1078: loss 0.263426\n",
      "batch 1079: loss 0.113881\n",
      "batch 1080: loss 0.094509\n",
      "batch 1081: loss 0.185416\n",
      "batch 1082: loss 0.369902\n",
      "batch 1083: loss 0.134503\n",
      "batch 1084: loss 0.090664\n",
      "batch 1085: loss 0.084974\n",
      "batch 1086: loss 0.130885\n",
      "batch 1087: loss 0.243194\n",
      "batch 1088: loss 0.065666\n",
      "batch 1089: loss 0.282935\n",
      "batch 1090: loss 0.059518\n",
      "batch 1091: loss 0.397819\n",
      "batch 1092: loss 0.112928\n",
      "batch 1093: loss 0.159376\n",
      "batch 1094: loss 0.078019\n",
      "batch 1095: loss 0.224094\n",
      "batch 1096: loss 0.393412\n",
      "batch 1097: loss 0.269618\n",
      "batch 1098: loss 0.206974\n",
      "batch 1099: loss 0.634742\n",
      "batch 1100: loss 0.103223\n",
      "batch 1101: loss 0.107710\n",
      "batch 1102: loss 0.047399\n",
      "batch 1103: loss 0.105512\n",
      "batch 1104: loss 0.189481\n",
      "batch 1105: loss 0.209161\n",
      "batch 1106: loss 0.062683\n",
      "batch 1107: loss 0.121071\n",
      "batch 1108: loss 0.142520\n",
      "batch 1109: loss 0.153849\n",
      "batch 1110: loss 0.165570\n",
      "batch 1111: loss 0.186823\n",
      "batch 1112: loss 0.084627\n",
      "batch 1113: loss 0.145989\n",
      "batch 1114: loss 0.184130\n",
      "batch 1115: loss 0.124522\n",
      "batch 1116: loss 0.046378\n",
      "batch 1117: loss 0.202557\n",
      "batch 1118: loss 0.132947\n",
      "batch 1119: loss 0.099866\n",
      "batch 1120: loss 0.159895\n",
      "batch 1121: loss 0.162025\n",
      "batch 1122: loss 0.153237\n",
      "batch 1123: loss 0.111141\n",
      "batch 1124: loss 0.072368\n",
      "batch 1125: loss 0.333229\n",
      "batch 1126: loss 0.122819\n",
      "batch 1127: loss 0.104159\n",
      "batch 1128: loss 0.151909\n",
      "batch 1129: loss 0.176860\n",
      "batch 1130: loss 0.080518\n",
      "batch 1131: loss 0.085458\n",
      "batch 1132: loss 0.210677\n",
      "batch 1133: loss 0.158363\n",
      "batch 1134: loss 0.183454\n",
      "batch 1135: loss 0.228935\n",
      "batch 1136: loss 0.197873\n",
      "batch 1137: loss 0.177637\n",
      "batch 1138: loss 0.374357\n",
      "batch 1139: loss 0.081664\n",
      "batch 1140: loss 0.106860\n",
      "batch 1141: loss 0.070839\n",
      "batch 1142: loss 0.240021\n",
      "batch 1143: loss 0.347914\n",
      "batch 1144: loss 0.110925\n",
      "batch 1145: loss 0.034010\n",
      "batch 1146: loss 0.117034\n",
      "batch 1147: loss 0.116870\n",
      "batch 1148: loss 0.122091\n",
      "batch 1149: loss 0.239505\n",
      "batch 1150: loss 0.106682\n",
      "batch 1151: loss 0.080297\n",
      "batch 1152: loss 0.395236\n",
      "batch 1153: loss 0.104120\n",
      "batch 1154: loss 0.164347\n",
      "batch 1155: loss 0.250869\n",
      "batch 1156: loss 0.117660\n",
      "batch 1157: loss 0.171969\n",
      "batch 1158: loss 0.062824\n",
      "batch 1159: loss 0.065525\n",
      "batch 1160: loss 0.205036\n",
      "batch 1161: loss 0.073145\n",
      "batch 1162: loss 0.129995\n",
      "batch 1163: loss 0.056909\n",
      "batch 1164: loss 0.191654\n",
      "batch 1165: loss 0.213387\n",
      "batch 1166: loss 0.214133\n",
      "batch 1167: loss 0.069832\n",
      "batch 1168: loss 0.071880\n",
      "batch 1169: loss 0.111374\n",
      "batch 1170: loss 0.179374\n",
      "batch 1171: loss 0.043736\n",
      "batch 1172: loss 0.178975\n",
      "batch 1173: loss 0.234638\n",
      "batch 1174: loss 0.102364\n",
      "batch 1175: loss 0.211639\n",
      "batch 1176: loss 0.063242\n",
      "batch 1177: loss 0.165545\n",
      "batch 1178: loss 0.045347\n",
      "batch 1179: loss 0.206871\n",
      "batch 1180: loss 0.106677\n",
      "batch 1181: loss 0.592028\n",
      "batch 1182: loss 0.100947\n",
      "batch 1183: loss 0.103503\n",
      "batch 1184: loss 0.076948\n",
      "batch 1185: loss 0.104199\n",
      "batch 1186: loss 0.247900\n",
      "batch 1187: loss 0.221807\n",
      "batch 1188: loss 0.210346\n",
      "batch 1189: loss 0.130285\n",
      "batch 1190: loss 0.263766\n",
      "batch 1191: loss 0.255189\n",
      "batch 1192: loss 0.254144\n",
      "batch 1193: loss 0.131073\n",
      "batch 1194: loss 0.080354\n",
      "batch 1195: loss 0.188186\n",
      "batch 1196: loss 0.180268\n",
      "batch 1197: loss 0.174750\n",
      "batch 1198: loss 0.152107\n",
      "batch 1199: loss 0.160997\n",
      "batch 1200: loss 0.242758\n",
      "batch 1201: loss 0.216302\n",
      "batch 1202: loss 0.180701\n",
      "batch 1203: loss 0.106163\n",
      "batch 1204: loss 0.213464\n",
      "batch 1205: loss 0.078228\n",
      "batch 1206: loss 0.172246\n",
      "batch 1207: loss 0.175806\n",
      "batch 1208: loss 0.172855\n",
      "batch 1209: loss 0.159958\n",
      "batch 1210: loss 0.045911\n",
      "batch 1211: loss 0.248612\n",
      "batch 1212: loss 0.063024\n",
      "batch 1213: loss 0.164225\n",
      "batch 1214: loss 0.350093\n",
      "batch 1215: loss 0.198101\n",
      "batch 1216: loss 0.103564\n",
      "batch 1217: loss 0.097589\n",
      "batch 1218: loss 0.094203\n",
      "batch 1219: loss 0.190376\n",
      "batch 1220: loss 0.210084\n",
      "batch 1221: loss 0.331741\n",
      "batch 1222: loss 0.195804\n",
      "batch 1223: loss 0.309596\n",
      "batch 1224: loss 0.111735\n",
      "batch 1225: loss 0.139260\n",
      "batch 1226: loss 0.146015\n",
      "batch 1227: loss 0.256060\n",
      "batch 1228: loss 0.049985\n",
      "batch 1229: loss 0.127061\n",
      "batch 1230: loss 0.362312\n",
      "batch 1231: loss 0.166978\n",
      "batch 1232: loss 0.086806\n",
      "batch 1233: loss 0.050446\n",
      "batch 1234: loss 0.147063\n",
      "batch 1235: loss 0.251935\n",
      "batch 1236: loss 0.084172\n",
      "batch 1237: loss 0.097244\n",
      "batch 1238: loss 0.153477\n",
      "batch 1239: loss 0.165696\n",
      "batch 1240: loss 0.113013\n",
      "batch 1241: loss 0.056014\n",
      "batch 1242: loss 0.131082\n",
      "batch 1243: loss 0.091424\n",
      "batch 1244: loss 0.141318\n",
      "batch 1245: loss 0.286269\n",
      "batch 1246: loss 0.067726\n",
      "batch 1247: loss 0.236412\n",
      "batch 1248: loss 0.232429\n",
      "batch 1249: loss 0.302709\n",
      "batch 1250: loss 0.216570\n",
      "batch 1251: loss 0.220373\n",
      "batch 1252: loss 0.084771\n",
      "batch 1253: loss 0.081575\n",
      "batch 1254: loss 0.230466\n",
      "batch 1255: loss 0.085156\n",
      "batch 1256: loss 0.061106\n",
      "batch 1257: loss 0.153811\n",
      "batch 1258: loss 0.165605\n",
      "batch 1259: loss 0.154026\n",
      "batch 1260: loss 0.090981\n",
      "batch 1261: loss 0.107311\n",
      "batch 1262: loss 0.141013\n",
      "batch 1263: loss 0.371968\n",
      "batch 1264: loss 0.109893\n",
      "batch 1265: loss 0.079719\n",
      "batch 1266: loss 0.086591\n",
      "batch 1267: loss 0.177946\n",
      "batch 1268: loss 0.040496\n",
      "batch 1269: loss 0.127993\n",
      "batch 1270: loss 0.269151\n",
      "batch 1271: loss 0.195838\n",
      "batch 1272: loss 0.198821\n",
      "batch 1273: loss 0.168189\n",
      "batch 1274: loss 0.150830\n",
      "batch 1275: loss 0.139344\n",
      "batch 1276: loss 0.158027\n",
      "batch 1277: loss 0.127867\n",
      "batch 1278: loss 0.119463\n",
      "batch 1279: loss 0.067172\n",
      "batch 1280: loss 0.081263\n",
      "batch 1281: loss 0.102218\n",
      "batch 1282: loss 0.124564\n",
      "batch 1283: loss 0.138705\n",
      "batch 1284: loss 0.203013\n",
      "batch 1285: loss 0.176385\n",
      "batch 1286: loss 0.121361\n",
      "batch 1287: loss 0.120918\n",
      "batch 1288: loss 0.169452\n",
      "batch 1289: loss 0.040777\n",
      "batch 1290: loss 0.162396\n",
      "batch 1291: loss 0.168062\n",
      "batch 1292: loss 0.145489\n",
      "batch 1293: loss 0.196692\n",
      "batch 1294: loss 0.154485\n",
      "batch 1295: loss 0.163286\n",
      "batch 1296: loss 0.324634\n",
      "batch 1297: loss 0.261166\n",
      "batch 1298: loss 0.045578\n",
      "batch 1299: loss 0.123238\n",
      "batch 1300: loss 0.198044\n",
      "batch 1301: loss 0.041067\n",
      "batch 1302: loss 0.143835\n",
      "batch 1303: loss 0.288935\n",
      "batch 1304: loss 0.079111\n",
      "batch 1305: loss 0.053590\n",
      "batch 1306: loss 0.113185\n",
      "batch 1307: loss 0.048464\n",
      "batch 1308: loss 0.351955\n",
      "batch 1309: loss 0.115849\n",
      "batch 1310: loss 0.101772\n",
      "batch 1311: loss 0.159380\n",
      "batch 1312: loss 0.214617\n",
      "batch 1313: loss 0.130977\n",
      "batch 1314: loss 0.050210\n",
      "batch 1315: loss 0.065992\n",
      "batch 1316: loss 0.055219\n",
      "batch 1317: loss 0.205916\n",
      "batch 1318: loss 0.056623\n",
      "batch 1319: loss 0.242650\n",
      "batch 1320: loss 0.129582\n",
      "batch 1321: loss 0.191666\n",
      "batch 1322: loss 0.211072\n",
      "batch 1323: loss 0.089825\n",
      "batch 1324: loss 0.126549\n",
      "batch 1325: loss 0.280253\n",
      "batch 1326: loss 0.094613\n",
      "batch 1327: loss 0.104655\n",
      "batch 1328: loss 0.124823\n",
      "batch 1329: loss 0.034051\n",
      "batch 1330: loss 0.108633\n",
      "batch 1331: loss 0.084346\n",
      "batch 1332: loss 0.134572\n",
      "batch 1333: loss 0.218018\n",
      "batch 1334: loss 0.257809\n",
      "batch 1335: loss 0.074017\n",
      "batch 1336: loss 0.071329\n",
      "batch 1337: loss 0.170396\n",
      "batch 1338: loss 0.134439\n",
      "batch 1339: loss 0.096035\n",
      "batch 1340: loss 0.233538\n",
      "batch 1341: loss 0.079772\n",
      "batch 1342: loss 0.187597\n",
      "batch 1343: loss 0.128145\n",
      "batch 1344: loss 0.102344\n",
      "batch 1345: loss 0.212098\n",
      "batch 1346: loss 0.102488\n",
      "batch 1347: loss 0.084679\n",
      "batch 1348: loss 0.178231\n",
      "batch 1349: loss 0.253571\n",
      "batch 1350: loss 0.323639\n",
      "batch 1351: loss 0.148089\n",
      "batch 1352: loss 0.142209\n",
      "batch 1353: loss 0.119051\n",
      "batch 1354: loss 0.177671\n",
      "batch 1355: loss 0.244627\n",
      "batch 1356: loss 0.115177\n",
      "batch 1357: loss 0.086277\n",
      "batch 1358: loss 0.148658\n",
      "batch 1359: loss 0.062854\n",
      "batch 1360: loss 0.239457\n",
      "batch 1361: loss 0.244820\n",
      "batch 1362: loss 0.131511\n",
      "batch 1363: loss 0.110552\n",
      "batch 1364: loss 0.282527\n",
      "batch 1365: loss 0.081998\n",
      "batch 1366: loss 0.136821\n",
      "batch 1367: loss 0.087394\n",
      "batch 1368: loss 0.125228\n",
      "batch 1369: loss 0.054397\n",
      "batch 1370: loss 0.054772\n",
      "batch 1371: loss 0.324112\n",
      "batch 1372: loss 0.125128\n",
      "batch 1373: loss 0.149140\n",
      "batch 1374: loss 0.200859\n",
      "batch 1375: loss 0.098413\n",
      "batch 1376: loss 0.194651\n",
      "batch 1377: loss 0.076456\n",
      "batch 1378: loss 0.239544\n",
      "batch 1379: loss 0.214847\n",
      "batch 1380: loss 0.109888\n",
      "batch 1381: loss 0.176776\n",
      "batch 1382: loss 0.123185\n",
      "batch 1383: loss 0.115123\n",
      "batch 1384: loss 0.058611\n",
      "batch 1385: loss 0.145960\n",
      "batch 1386: loss 0.195316\n",
      "batch 1387: loss 0.191611\n",
      "batch 1388: loss 0.048143\n",
      "batch 1389: loss 0.085956\n",
      "batch 1390: loss 0.214486\n",
      "batch 1391: loss 0.084703\n",
      "batch 1392: loss 0.234893\n",
      "batch 1393: loss 0.037162\n",
      "batch 1394: loss 0.289461\n",
      "batch 1395: loss 0.095124\n",
      "batch 1396: loss 0.093713\n",
      "batch 1397: loss 0.090679\n",
      "batch 1398: loss 0.209202\n",
      "batch 1399: loss 0.289850\n",
      "batch 1400: loss 0.111580\n",
      "batch 1401: loss 0.236545\n",
      "batch 1402: loss 0.111357\n",
      "batch 1403: loss 0.218101\n",
      "batch 1404: loss 0.044734\n",
      "batch 1405: loss 0.157549\n",
      "batch 1406: loss 0.196155\n",
      "batch 1407: loss 0.235891\n",
      "batch 1408: loss 0.084817\n",
      "batch 1409: loss 0.290748\n",
      "batch 1410: loss 0.148276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1411: loss 0.053276\n",
      "batch 1412: loss 0.086477\n",
      "batch 1413: loss 0.267957\n",
      "batch 1414: loss 0.148571\n",
      "batch 1415: loss 0.087308\n",
      "batch 1416: loss 0.121278\n",
      "batch 1417: loss 0.210576\n",
      "batch 1418: loss 0.218443\n",
      "batch 1419: loss 0.259052\n",
      "batch 1420: loss 0.094896\n",
      "batch 1421: loss 0.064542\n",
      "batch 1422: loss 0.275740\n",
      "batch 1423: loss 0.183538\n",
      "batch 1424: loss 0.131137\n",
      "batch 1425: loss 0.102233\n",
      "batch 1426: loss 0.225624\n",
      "batch 1427: loss 0.127284\n",
      "batch 1428: loss 0.105332\n",
      "batch 1429: loss 0.229272\n",
      "batch 1430: loss 0.044808\n",
      "batch 1431: loss 0.141671\n",
      "batch 1432: loss 0.045562\n",
      "batch 1433: loss 0.119901\n",
      "batch 1434: loss 0.080404\n",
      "batch 1435: loss 0.132061\n",
      "batch 1436: loss 0.261130\n",
      "batch 1437: loss 0.046229\n",
      "batch 1438: loss 0.400458\n",
      "batch 1439: loss 0.247831\n",
      "batch 1440: loss 0.117507\n",
      "batch 1441: loss 0.219576\n",
      "batch 1442: loss 0.246362\n",
      "batch 1443: loss 0.267738\n",
      "batch 1444: loss 0.186810\n",
      "batch 1445: loss 0.111919\n",
      "batch 1446: loss 0.096704\n",
      "batch 1447: loss 0.067791\n",
      "batch 1448: loss 0.135071\n",
      "batch 1449: loss 0.049477\n",
      "batch 1450: loss 0.217217\n",
      "batch 1451: loss 0.052637\n",
      "batch 1452: loss 0.205645\n",
      "batch 1453: loss 0.119121\n",
      "batch 1454: loss 0.068880\n",
      "batch 1455: loss 0.184899\n",
      "batch 1456: loss 0.153374\n",
      "batch 1457: loss 0.144517\n",
      "batch 1458: loss 0.219347\n",
      "batch 1459: loss 0.149801\n",
      "batch 1460: loss 0.255035\n",
      "batch 1461: loss 0.242522\n",
      "batch 1462: loss 0.158708\n",
      "batch 1463: loss 0.188541\n",
      "batch 1464: loss 0.092533\n",
      "batch 1465: loss 0.169873\n",
      "batch 1466: loss 0.019626\n",
      "batch 1467: loss 0.261280\n",
      "batch 1468: loss 0.141234\n",
      "batch 1469: loss 0.185752\n",
      "batch 1470: loss 0.078622\n",
      "batch 1471: loss 0.102757\n",
      "batch 1472: loss 0.221868\n",
      "batch 1473: loss 0.064250\n",
      "batch 1474: loss 0.131786\n",
      "batch 1475: loss 0.243799\n",
      "batch 1476: loss 0.054571\n",
      "batch 1477: loss 0.059843\n",
      "batch 1478: loss 0.337129\n",
      "batch 1479: loss 0.123226\n",
      "batch 1480: loss 0.172655\n",
      "batch 1481: loss 0.226667\n",
      "batch 1482: loss 0.105043\n",
      "batch 1483: loss 0.076613\n",
      "batch 1484: loss 0.100094\n",
      "batch 1485: loss 0.262172\n",
      "batch 1486: loss 0.196756\n",
      "batch 1487: loss 0.150472\n",
      "batch 1488: loss 0.053599\n",
      "batch 1489: loss 0.103295\n",
      "batch 1490: loss 0.208019\n",
      "batch 1491: loss 0.242662\n",
      "batch 1492: loss 0.172323\n",
      "batch 1493: loss 0.102757\n",
      "batch 1494: loss 0.080043\n",
      "batch 1495: loss 0.178838\n",
      "batch 1496: loss 0.196547\n",
      "batch 1497: loss 0.078628\n",
      "batch 1498: loss 0.048455\n",
      "batch 1499: loss 0.094124\n",
      "batch 1500: loss 0.255100\n",
      "batch 1501: loss 0.232714\n",
      "batch 1502: loss 0.308756\n",
      "batch 1503: loss 0.191763\n",
      "batch 1504: loss 0.126723\n",
      "batch 1505: loss 0.052105\n",
      "batch 1506: loss 0.256190\n",
      "batch 1507: loss 0.176295\n",
      "batch 1508: loss 0.091568\n",
      "batch 1509: loss 0.097115\n",
      "batch 1510: loss 0.111841\n",
      "batch 1511: loss 0.120241\n",
      "batch 1512: loss 0.222368\n",
      "batch 1513: loss 0.199483\n",
      "batch 1514: loss 0.061210\n",
      "batch 1515: loss 0.215805\n",
      "batch 1516: loss 0.043712\n",
      "batch 1517: loss 0.192667\n",
      "batch 1518: loss 0.137424\n",
      "batch 1519: loss 0.072549\n",
      "batch 1520: loss 0.121464\n",
      "batch 1521: loss 0.135009\n",
      "batch 1522: loss 0.172892\n",
      "batch 1523: loss 0.157694\n",
      "batch 1524: loss 0.226614\n",
      "batch 1525: loss 0.114800\n",
      "batch 1526: loss 0.085606\n",
      "batch 1527: loss 0.177567\n",
      "batch 1528: loss 0.116166\n",
      "batch 1529: loss 0.235311\n",
      "batch 1530: loss 0.050020\n",
      "batch 1531: loss 0.053901\n",
      "batch 1532: loss 0.176689\n",
      "batch 1533: loss 0.092549\n",
      "batch 1534: loss 0.072791\n",
      "batch 1535: loss 0.062170\n",
      "batch 1536: loss 0.135947\n",
      "batch 1537: loss 0.245036\n",
      "batch 1538: loss 0.170946\n",
      "batch 1539: loss 0.115505\n",
      "batch 1540: loss 0.401231\n",
      "batch 1541: loss 0.044795\n",
      "batch 1542: loss 0.328690\n",
      "batch 1543: loss 0.080822\n",
      "batch 1544: loss 0.031917\n",
      "batch 1545: loss 0.177087\n",
      "batch 1546: loss 0.217344\n",
      "batch 1547: loss 0.146390\n",
      "batch 1548: loss 0.096684\n",
      "batch 1549: loss 0.131292\n",
      "batch 1550: loss 0.089212\n",
      "batch 1551: loss 0.139908\n",
      "batch 1552: loss 0.160697\n",
      "batch 1553: loss 0.327367\n",
      "batch 1554: loss 0.410740\n",
      "batch 1555: loss 0.132604\n",
      "batch 1556: loss 0.143329\n",
      "batch 1557: loss 0.158533\n",
      "batch 1558: loss 0.063709\n",
      "batch 1559: loss 0.032710\n",
      "batch 1560: loss 0.102024\n",
      "batch 1561: loss 0.112475\n",
      "batch 1562: loss 0.119001\n",
      "batch 1563: loss 0.114596\n",
      "batch 1564: loss 0.218974\n",
      "batch 1565: loss 0.106091\n",
      "batch 1566: loss 0.113988\n",
      "batch 1567: loss 0.110967\n",
      "batch 1568: loss 0.143443\n",
      "batch 1569: loss 0.157490\n",
      "batch 1570: loss 0.121230\n",
      "batch 1571: loss 0.130544\n",
      "batch 1572: loss 0.221266\n",
      "batch 1573: loss 0.130728\n",
      "batch 1574: loss 0.098329\n",
      "batch 1575: loss 0.300830\n",
      "batch 1576: loss 0.141062\n",
      "batch 1577: loss 0.107813\n",
      "batch 1578: loss 0.098152\n",
      "batch 1579: loss 0.191191\n",
      "batch 1580: loss 0.069128\n",
      "batch 1581: loss 0.124949\n",
      "batch 1582: loss 0.158340\n",
      "batch 1583: loss 0.086356\n",
      "batch 1584: loss 0.174934\n",
      "batch 1585: loss 0.153518\n",
      "batch 1586: loss 0.405391\n",
      "batch 1587: loss 0.093590\n",
      "batch 1588: loss 0.283066\n",
      "batch 1589: loss 0.087763\n",
      "batch 1590: loss 0.154997\n",
      "batch 1591: loss 0.053141\n",
      "batch 1592: loss 0.061823\n",
      "batch 1593: loss 0.055561\n",
      "batch 1594: loss 0.068372\n",
      "batch 1595: loss 0.275069\n",
      "batch 1596: loss 0.061919\n",
      "batch 1597: loss 0.082937\n",
      "batch 1598: loss 0.143224\n",
      "batch 1599: loss 0.215897\n",
      "batch 1600: loss 0.129091\n",
      "batch 1601: loss 0.054323\n",
      "batch 1602: loss 0.170670\n",
      "batch 1603: loss 0.078623\n",
      "batch 1604: loss 0.094255\n",
      "batch 1605: loss 0.055549\n",
      "batch 1606: loss 0.076726\n",
      "batch 1607: loss 0.048213\n",
      "batch 1608: loss 0.153216\n",
      "batch 1609: loss 0.067713\n",
      "batch 1610: loss 0.295654\n",
      "batch 1611: loss 0.069077\n",
      "batch 1612: loss 0.093932\n",
      "batch 1613: loss 0.216573\n",
      "batch 1614: loss 0.185957\n",
      "batch 1615: loss 0.073024\n",
      "batch 1616: loss 0.165705\n",
      "batch 1617: loss 0.129285\n",
      "batch 1618: loss 0.122287\n",
      "batch 1619: loss 0.198352\n",
      "batch 1620: loss 0.143021\n",
      "batch 1621: loss 0.116471\n",
      "batch 1622: loss 0.176717\n",
      "batch 1623: loss 0.053310\n",
      "batch 1624: loss 0.041933\n",
      "batch 1625: loss 0.081314\n",
      "batch 1626: loss 0.051486\n",
      "batch 1627: loss 0.317326\n",
      "batch 1628: loss 0.180317\n",
      "batch 1629: loss 0.127970\n",
      "batch 1630: loss 0.047161\n",
      "batch 1631: loss 0.174134\n",
      "batch 1632: loss 0.116734\n",
      "batch 1633: loss 0.070011\n",
      "batch 1634: loss 0.087173\n",
      "batch 1635: loss 0.165774\n",
      "batch 1636: loss 0.167273\n",
      "batch 1637: loss 0.083967\n",
      "batch 1638: loss 0.135652\n",
      "batch 1639: loss 0.086820\n",
      "batch 1640: loss 0.139018\n",
      "batch 1641: loss 0.113492\n",
      "batch 1642: loss 0.056744\n",
      "batch 1643: loss 0.140516\n",
      "batch 1644: loss 0.196558\n",
      "batch 1645: loss 0.057152\n",
      "batch 1646: loss 0.130004\n",
      "batch 1647: loss 0.041790\n",
      "batch 1648: loss 0.232098\n",
      "batch 1649: loss 0.166725\n",
      "batch 1650: loss 0.169001\n",
      "batch 1651: loss 0.103285\n",
      "batch 1652: loss 0.107721\n",
      "batch 1653: loss 0.073110\n",
      "batch 1654: loss 0.166186\n",
      "batch 1655: loss 0.117531\n",
      "batch 1656: loss 0.057649\n",
      "batch 1657: loss 0.096318\n",
      "batch 1658: loss 0.116979\n",
      "batch 1659: loss 0.122521\n",
      "batch 1660: loss 0.173418\n",
      "batch 1661: loss 0.089420\n",
      "batch 1662: loss 0.031402\n",
      "batch 1663: loss 0.177904\n",
      "batch 1664: loss 0.151177\n",
      "batch 1665: loss 0.097859\n",
      "batch 1666: loss 0.196377\n",
      "batch 1667: loss 0.202109\n",
      "batch 1668: loss 0.120918\n",
      "batch 1669: loss 0.190518\n",
      "batch 1670: loss 0.089955\n",
      "batch 1671: loss 0.114618\n",
      "batch 1672: loss 0.184426\n",
      "batch 1673: loss 0.349652\n",
      "batch 1674: loss 0.208848\n",
      "batch 1675: loss 0.113423\n",
      "batch 1676: loss 0.140515\n",
      "batch 1677: loss 0.053988\n",
      "batch 1678: loss 0.269794\n",
      "batch 1679: loss 0.137415\n",
      "batch 1680: loss 0.079864\n",
      "batch 1681: loss 0.106527\n",
      "batch 1682: loss 0.335286\n",
      "batch 1683: loss 0.198724\n",
      "batch 1684: loss 0.054045\n",
      "batch 1685: loss 0.085524\n",
      "batch 1686: loss 0.372208\n",
      "batch 1687: loss 0.089105\n",
      "batch 1688: loss 0.351122\n",
      "batch 1689: loss 0.226762\n",
      "batch 1690: loss 0.132540\n",
      "batch 1691: loss 0.100116\n",
      "batch 1692: loss 0.176320\n",
      "batch 1693: loss 0.073248\n",
      "batch 1694: loss 0.126290\n",
      "batch 1695: loss 0.061683\n",
      "batch 1696: loss 0.141018\n",
      "batch 1697: loss 0.161492\n",
      "batch 1698: loss 0.263072\n",
      "batch 1699: loss 0.075793\n",
      "batch 1700: loss 0.064122\n",
      "batch 1701: loss 0.106919\n",
      "batch 1702: loss 0.117948\n",
      "batch 1703: loss 0.082005\n",
      "batch 1704: loss 0.052497\n",
      "batch 1705: loss 0.170376\n",
      "batch 1706: loss 0.165271\n",
      "batch 1707: loss 0.235186\n",
      "batch 1708: loss 0.022607\n",
      "batch 1709: loss 0.090555\n",
      "batch 1710: loss 0.076318\n",
      "batch 1711: loss 0.194724\n",
      "batch 1712: loss 0.061737\n",
      "batch 1713: loss 0.128715\n",
      "batch 1714: loss 0.050839\n",
      "batch 1715: loss 0.101330\n",
      "batch 1716: loss 0.142290\n",
      "batch 1717: loss 0.130103\n",
      "batch 1718: loss 0.085619\n",
      "batch 1719: loss 0.136937\n",
      "batch 1720: loss 0.110500\n",
      "batch 1721: loss 0.243342\n",
      "batch 1722: loss 0.058459\n",
      "batch 1723: loss 0.183990\n",
      "batch 1724: loss 0.082465\n",
      "batch 1725: loss 0.147207\n",
      "batch 1726: loss 0.037973\n",
      "batch 1727: loss 0.106882\n",
      "batch 1728: loss 0.095281\n",
      "batch 1729: loss 0.075586\n",
      "batch 1730: loss 0.070177\n",
      "batch 1731: loss 0.186746\n",
      "batch 1732: loss 0.118629\n",
      "batch 1733: loss 0.156749\n",
      "batch 1734: loss 0.165290\n",
      "batch 1735: loss 0.204572\n",
      "batch 1736: loss 0.094429\n",
      "batch 1737: loss 0.074322\n",
      "batch 1738: loss 0.062007\n",
      "batch 1739: loss 0.300841\n",
      "batch 1740: loss 0.102878\n",
      "batch 1741: loss 0.112357\n",
      "batch 1742: loss 0.107317\n",
      "batch 1743: loss 0.072458\n",
      "batch 1744: loss 0.035078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1745: loss 0.200638\n",
      "batch 1746: loss 0.095865\n",
      "batch 1747: loss 0.059877\n",
      "batch 1748: loss 0.080962\n",
      "batch 1749: loss 0.116163\n",
      "batch 1750: loss 0.120429\n",
      "batch 1751: loss 0.080854\n",
      "batch 1752: loss 0.098912\n",
      "batch 1753: loss 0.129841\n",
      "batch 1754: loss 0.193209\n",
      "batch 1755: loss 0.336082\n",
      "batch 1756: loss 0.084969\n",
      "batch 1757: loss 0.208545\n",
      "batch 1758: loss 0.038920\n",
      "batch 1759: loss 0.176589\n",
      "batch 1760: loss 0.042646\n",
      "batch 1761: loss 0.045145\n",
      "batch 1762: loss 0.131183\n",
      "batch 1763: loss 0.078262\n",
      "batch 1764: loss 0.123118\n",
      "batch 1765: loss 0.128311\n",
      "batch 1766: loss 0.214126\n",
      "batch 1767: loss 0.094980\n",
      "batch 1768: loss 0.056668\n",
      "batch 1769: loss 0.088236\n",
      "batch 1770: loss 0.041018\n",
      "batch 1771: loss 0.046241\n",
      "batch 1772: loss 0.056153\n",
      "batch 1773: loss 0.168603\n",
      "batch 1774: loss 0.089199\n",
      "batch 1775: loss 0.059297\n",
      "batch 1776: loss 0.131241\n",
      "batch 1777: loss 0.166587\n",
      "batch 1778: loss 0.176661\n",
      "batch 1779: loss 0.114837\n",
      "batch 1780: loss 0.227022\n",
      "batch 1781: loss 0.088265\n",
      "batch 1782: loss 0.089389\n",
      "batch 1783: loss 0.210891\n",
      "batch 1784: loss 0.148465\n",
      "batch 1785: loss 0.256494\n",
      "batch 1786: loss 0.207423\n",
      "batch 1787: loss 0.072837\n",
      "batch 1788: loss 0.066096\n",
      "batch 1789: loss 0.183445\n",
      "batch 1790: loss 0.064449\n",
      "batch 1791: loss 0.057764\n",
      "batch 1792: loss 0.139353\n",
      "batch 1793: loss 0.161513\n",
      "batch 1794: loss 0.084971\n",
      "batch 1795: loss 0.021118\n",
      "batch 1796: loss 0.031976\n",
      "batch 1797: loss 0.101297\n",
      "batch 1798: loss 0.095686\n",
      "batch 1799: loss 0.197734\n",
      "batch 1800: loss 0.054221\n",
      "batch 1801: loss 0.135304\n",
      "batch 1802: loss 0.092483\n",
      "batch 1803: loss 0.291264\n",
      "batch 1804: loss 0.208155\n",
      "batch 1805: loss 0.157118\n",
      "batch 1806: loss 0.131471\n",
      "batch 1807: loss 0.113823\n",
      "batch 1808: loss 0.192864\n",
      "batch 1809: loss 0.275062\n",
      "batch 1810: loss 0.093682\n",
      "batch 1811: loss 0.053117\n",
      "batch 1812: loss 0.082608\n",
      "batch 1813: loss 0.028564\n",
      "batch 1814: loss 0.082920\n",
      "batch 1815: loss 0.118117\n",
      "batch 1816: loss 0.035151\n",
      "batch 1817: loss 0.048977\n",
      "batch 1818: loss 0.303204\n",
      "batch 1819: loss 0.296800\n",
      "batch 1820: loss 0.140974\n",
      "batch 1821: loss 0.100260\n",
      "batch 1822: loss 0.108534\n",
      "batch 1823: loss 0.171358\n",
      "batch 1824: loss 0.213172\n",
      "batch 1825: loss 0.103597\n",
      "batch 1826: loss 0.163642\n",
      "batch 1827: loss 0.243861\n",
      "batch 1828: loss 0.110699\n",
      "batch 1829: loss 0.101570\n",
      "batch 1830: loss 0.071079\n",
      "batch 1831: loss 0.208354\n",
      "batch 1832: loss 0.094177\n",
      "batch 1833: loss 0.149331\n",
      "batch 1834: loss 0.233993\n",
      "batch 1835: loss 0.073300\n",
      "batch 1836: loss 0.078882\n",
      "batch 1837: loss 0.184894\n",
      "batch 1838: loss 0.039924\n",
      "batch 1839: loss 0.228233\n",
      "batch 1840: loss 0.181036\n",
      "batch 1841: loss 0.327984\n",
      "batch 1842: loss 0.121526\n",
      "batch 1843: loss 0.216521\n",
      "batch 1844: loss 0.079562\n",
      "batch 1845: loss 0.246164\n",
      "batch 1846: loss 0.073090\n",
      "batch 1847: loss 0.104576\n",
      "batch 1848: loss 0.249754\n",
      "batch 1849: loss 0.061922\n",
      "batch 1850: loss 0.111241\n",
      "batch 1851: loss 0.032021\n",
      "batch 1852: loss 0.102977\n",
      "batch 1853: loss 0.135140\n",
      "batch 1854: loss 0.143120\n",
      "batch 1855: loss 0.119173\n",
      "batch 1856: loss 0.037935\n",
      "batch 1857: loss 0.146299\n",
      "batch 1858: loss 0.113527\n",
      "batch 1859: loss 0.062780\n",
      "batch 1860: loss 0.125115\n",
      "batch 1861: loss 0.123150\n",
      "batch 1862: loss 0.158402\n",
      "batch 1863: loss 0.159286\n",
      "batch 1864: loss 0.116929\n",
      "batch 1865: loss 0.104037\n",
      "batch 1866: loss 0.036916\n",
      "batch 1867: loss 0.188419\n",
      "batch 1868: loss 0.075319\n",
      "batch 1869: loss 0.117903\n",
      "batch 1870: loss 0.132672\n",
      "batch 1871: loss 0.034397\n",
      "batch 1872: loss 0.356273\n",
      "batch 1873: loss 0.146976\n",
      "batch 1874: loss 0.091397\n",
      "batch 1875: loss 0.224087\n",
      "batch 1876: loss 0.124921\n",
      "batch 1877: loss 0.132620\n",
      "batch 1878: loss 0.074979\n",
      "batch 1879: loss 0.040038\n",
      "batch 1880: loss 0.049319\n",
      "batch 1881: loss 0.116164\n",
      "batch 1882: loss 0.243563\n",
      "batch 1883: loss 0.103360\n",
      "batch 1884: loss 0.095536\n",
      "batch 1885: loss 0.038098\n",
      "batch 1886: loss 0.053896\n",
      "batch 1887: loss 0.166425\n",
      "batch 1888: loss 0.028564\n",
      "batch 1889: loss 0.130675\n",
      "batch 1890: loss 0.103386\n",
      "batch 1891: loss 0.053024\n",
      "batch 1892: loss 0.187194\n",
      "batch 1893: loss 0.188115\n",
      "batch 1894: loss 0.036435\n",
      "batch 1895: loss 0.075621\n",
      "batch 1896: loss 0.029037\n",
      "batch 1897: loss 0.270739\n",
      "batch 1898: loss 0.047834\n",
      "batch 1899: loss 0.119660\n",
      "batch 1900: loss 0.089414\n",
      "batch 1901: loss 0.043927\n",
      "batch 1902: loss 0.084141\n",
      "batch 1903: loss 0.171131\n",
      "batch 1904: loss 0.037795\n",
      "batch 1905: loss 0.286339\n",
      "batch 1906: loss 0.129540\n",
      "batch 1907: loss 0.106544\n",
      "batch 1908: loss 0.086939\n",
      "batch 1909: loss 0.145172\n",
      "batch 1910: loss 0.087415\n",
      "batch 1911: loss 0.035029\n",
      "batch 1912: loss 0.063689\n",
      "batch 1913: loss 0.084253\n",
      "batch 1914: loss 0.288291\n",
      "batch 1915: loss 0.053001\n",
      "batch 1916: loss 0.314034\n",
      "batch 1917: loss 0.204248\n",
      "batch 1918: loss 0.099745\n",
      "batch 1919: loss 0.134319\n",
      "batch 1920: loss 0.172770\n",
      "batch 1921: loss 0.103680\n",
      "batch 1922: loss 0.077634\n",
      "batch 1923: loss 0.099722\n",
      "batch 1924: loss 0.101388\n",
      "batch 1925: loss 0.194199\n",
      "batch 1926: loss 0.109628\n",
      "batch 1927: loss 0.096095\n",
      "batch 1928: loss 0.114887\n",
      "batch 1929: loss 0.044254\n",
      "batch 1930: loss 0.068577\n",
      "batch 1931: loss 0.015705\n",
      "batch 1932: loss 0.054670\n",
      "batch 1933: loss 0.110505\n",
      "batch 1934: loss 0.154629\n",
      "batch 1935: loss 0.071817\n",
      "batch 1936: loss 0.201129\n",
      "batch 1937: loss 0.210342\n",
      "batch 1938: loss 0.122477\n",
      "batch 1939: loss 0.024459\n",
      "batch 1940: loss 0.197112\n",
      "batch 1941: loss 0.078821\n",
      "batch 1942: loss 0.074245\n",
      "batch 1943: loss 0.059986\n",
      "batch 1944: loss 0.089001\n",
      "batch 1945: loss 0.113625\n",
      "batch 1946: loss 0.073245\n",
      "batch 1947: loss 0.160226\n",
      "batch 1948: loss 0.055984\n",
      "batch 1949: loss 0.272597\n",
      "batch 1950: loss 0.081534\n",
      "batch 1951: loss 0.108184\n",
      "batch 1952: loss 0.149889\n",
      "batch 1953: loss 0.099863\n",
      "batch 1954: loss 0.157545\n",
      "batch 1955: loss 0.054881\n",
      "batch 1956: loss 0.085888\n",
      "batch 1957: loss 0.243234\n",
      "batch 1958: loss 0.218689\n",
      "batch 1959: loss 0.128575\n",
      "batch 1960: loss 0.275331\n",
      "batch 1961: loss 0.080123\n",
      "batch 1962: loss 0.153248\n",
      "batch 1963: loss 0.080316\n",
      "batch 1964: loss 0.036580\n",
      "batch 1965: loss 0.092166\n",
      "batch 1966: loss 0.187246\n",
      "batch 1967: loss 0.126673\n",
      "batch 1968: loss 0.056584\n",
      "batch 1969: loss 0.183888\n",
      "batch 1970: loss 0.095453\n",
      "batch 1971: loss 0.143982\n",
      "batch 1972: loss 0.215612\n",
      "batch 1973: loss 0.120802\n",
      "batch 1974: loss 0.203956\n",
      "batch 1975: loss 0.301642\n",
      "batch 1976: loss 0.050862\n",
      "batch 1977: loss 0.182870\n",
      "batch 1978: loss 0.226715\n",
      "batch 1979: loss 0.061381\n",
      "batch 1980: loss 0.074233\n",
      "batch 1981: loss 0.044495\n",
      "batch 1982: loss 0.061886\n",
      "batch 1983: loss 0.022753\n",
      "batch 1984: loss 0.045217\n",
      "batch 1985: loss 0.151361\n",
      "batch 1986: loss 0.033091\n",
      "batch 1987: loss 0.176705\n",
      "batch 1988: loss 0.091591\n",
      "batch 1989: loss 0.212811\n",
      "batch 1990: loss 0.031469\n",
      "batch 1991: loss 0.106872\n",
      "batch 1992: loss 0.066104\n",
      "batch 1993: loss 0.210604\n",
      "batch 1994: loss 0.119135\n",
      "batch 1995: loss 0.090636\n",
      "batch 1996: loss 0.090099\n",
      "batch 1997: loss 0.158981\n",
      "batch 1998: loss 0.147130\n",
      "batch 1999: loss 0.066757\n",
      "batch 2000: loss 0.045203\n",
      "batch 2001: loss 0.248271\n",
      "batch 2002: loss 0.171262\n",
      "batch 2003: loss 0.072260\n",
      "batch 2004: loss 0.109065\n",
      "batch 2005: loss 0.099361\n",
      "batch 2006: loss 0.144374\n",
      "batch 2007: loss 0.146836\n",
      "batch 2008: loss 0.126670\n",
      "batch 2009: loss 0.065454\n",
      "batch 2010: loss 0.103401\n",
      "batch 2011: loss 0.092023\n",
      "batch 2012: loss 0.295275\n",
      "batch 2013: loss 0.157295\n",
      "batch 2014: loss 0.059556\n",
      "batch 2015: loss 0.052181\n",
      "batch 2016: loss 0.034145\n",
      "batch 2017: loss 0.126102\n",
      "batch 2018: loss 0.171198\n",
      "batch 2019: loss 0.066976\n",
      "batch 2020: loss 0.047397\n",
      "batch 2021: loss 0.079593\n",
      "batch 2022: loss 0.124791\n",
      "batch 2023: loss 0.166458\n",
      "batch 2024: loss 0.200240\n",
      "batch 2025: loss 0.065319\n",
      "batch 2026: loss 0.136189\n",
      "batch 2027: loss 0.176371\n",
      "batch 2028: loss 0.071602\n",
      "batch 2029: loss 0.091695\n",
      "batch 2030: loss 0.149709\n",
      "batch 2031: loss 0.145641\n",
      "batch 2032: loss 0.128572\n",
      "batch 2033: loss 0.056329\n",
      "batch 2034: loss 0.078490\n",
      "batch 2035: loss 0.107047\n",
      "batch 2036: loss 0.083318\n",
      "batch 2037: loss 0.115050\n",
      "batch 2038: loss 0.061271\n",
      "batch 2039: loss 0.206433\n",
      "batch 2040: loss 0.108365\n",
      "batch 2041: loss 0.156368\n",
      "batch 2042: loss 0.109013\n",
      "batch 2043: loss 0.080199\n",
      "batch 2044: loss 0.199644\n",
      "batch 2045: loss 0.191734\n",
      "batch 2046: loss 0.041309\n",
      "batch 2047: loss 0.176167\n",
      "batch 2048: loss 0.123382\n",
      "batch 2049: loss 0.095750\n",
      "batch 2050: loss 0.103633\n",
      "batch 2051: loss 0.087876\n",
      "batch 2052: loss 0.081973\n",
      "batch 2053: loss 0.076117\n",
      "batch 2054: loss 0.052275\n",
      "batch 2055: loss 0.059502\n",
      "batch 2056: loss 0.045578\n",
      "batch 2057: loss 0.169879\n",
      "batch 2058: loss 0.120032\n",
      "batch 2059: loss 0.097849\n",
      "batch 2060: loss 0.037812\n",
      "batch 2061: loss 0.134226\n",
      "batch 2062: loss 0.199293\n",
      "batch 2063: loss 0.144840\n",
      "batch 2064: loss 0.118837\n",
      "batch 2065: loss 0.123152\n",
      "batch 2066: loss 0.186245\n",
      "batch 2067: loss 0.112042\n",
      "batch 2068: loss 0.075950\n",
      "batch 2069: loss 0.092579\n",
      "batch 2070: loss 0.250221\n",
      "batch 2071: loss 0.095876\n",
      "batch 2072: loss 0.075534\n",
      "batch 2073: loss 0.112636\n",
      "batch 2074: loss 0.113114\n",
      "batch 2075: loss 0.234043\n",
      "batch 2076: loss 0.211088\n",
      "batch 2077: loss 0.123046\n",
      "batch 2078: loss 0.215049\n",
      "batch 2079: loss 0.270004\n",
      "batch 2080: loss 0.159343\n",
      "batch 2081: loss 0.092549\n",
      "batch 2082: loss 0.128876\n",
      "batch 2083: loss 0.047916\n",
      "batch 2084: loss 0.121188\n",
      "batch 2085: loss 0.139819\n",
      "batch 2086: loss 0.153626\n",
      "batch 2087: loss 0.053357\n",
      "batch 2088: loss 0.032613\n",
      "batch 2089: loss 0.252330\n",
      "batch 2090: loss 0.137850\n",
      "batch 2091: loss 0.152332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2092: loss 0.078894\n",
      "batch 2093: loss 0.086149\n",
      "batch 2094: loss 0.136060\n",
      "batch 2095: loss 0.066341\n",
      "batch 2096: loss 0.073639\n",
      "batch 2097: loss 0.134368\n",
      "batch 2098: loss 0.028928\n",
      "batch 2099: loss 0.034954\n",
      "batch 2100: loss 0.291795\n",
      "batch 2101: loss 0.031101\n",
      "batch 2102: loss 0.017809\n",
      "batch 2103: loss 0.114474\n",
      "batch 2104: loss 0.151301\n",
      "batch 2105: loss 0.032342\n",
      "batch 2106: loss 0.217680\n",
      "batch 2107: loss 0.126201\n",
      "batch 2108: loss 0.121448\n",
      "batch 2109: loss 0.021626\n",
      "batch 2110: loss 0.163116\n",
      "batch 2111: loss 0.066725\n",
      "batch 2112: loss 0.074153\n",
      "batch 2113: loss 0.108764\n",
      "batch 2114: loss 0.210369\n",
      "batch 2115: loss 0.226893\n",
      "batch 2116: loss 0.114991\n",
      "batch 2117: loss 0.166502\n",
      "batch 2118: loss 0.058992\n",
      "batch 2119: loss 0.153011\n",
      "batch 2120: loss 0.182712\n",
      "batch 2121: loss 0.089573\n",
      "batch 2122: loss 0.053213\n",
      "batch 2123: loss 0.108049\n",
      "batch 2124: loss 0.077180\n",
      "batch 2125: loss 0.087246\n",
      "batch 2126: loss 0.205444\n",
      "batch 2127: loss 0.202767\n",
      "batch 2128: loss 0.149517\n",
      "batch 2129: loss 0.019549\n",
      "batch 2130: loss 0.032759\n",
      "batch 2131: loss 0.019259\n",
      "batch 2132: loss 0.044965\n",
      "batch 2133: loss 0.112776\n",
      "batch 2134: loss 0.149415\n",
      "batch 2135: loss 0.220397\n",
      "batch 2136: loss 0.060864\n",
      "batch 2137: loss 0.189528\n",
      "batch 2138: loss 0.091692\n",
      "batch 2139: loss 0.077969\n",
      "batch 2140: loss 0.051338\n",
      "batch 2141: loss 0.045706\n",
      "batch 2142: loss 0.141196\n",
      "batch 2143: loss 0.043638\n",
      "batch 2144: loss 0.113378\n",
      "batch 2145: loss 0.050732\n",
      "batch 2146: loss 0.081129\n",
      "batch 2147: loss 0.306999\n",
      "batch 2148: loss 0.102705\n",
      "batch 2149: loss 0.169297\n",
      "batch 2150: loss 0.066905\n",
      "batch 2151: loss 0.044078\n",
      "batch 2152: loss 0.025110\n",
      "batch 2153: loss 0.215661\n",
      "batch 2154: loss 0.126067\n",
      "batch 2155: loss 0.158311\n",
      "batch 2156: loss 0.177937\n",
      "batch 2157: loss 0.220697\n",
      "batch 2158: loss 0.082302\n",
      "batch 2159: loss 0.185818\n",
      "batch 2160: loss 0.129941\n",
      "batch 2161: loss 0.083585\n",
      "batch 2162: loss 0.031724\n",
      "batch 2163: loss 0.041266\n",
      "batch 2164: loss 0.172423\n",
      "batch 2165: loss 0.047652\n",
      "batch 2166: loss 0.173807\n",
      "batch 2167: loss 0.151274\n",
      "batch 2168: loss 0.090468\n",
      "batch 2169: loss 0.168711\n",
      "batch 2170: loss 0.069342\n",
      "batch 2171: loss 0.047959\n",
      "batch 2172: loss 0.065581\n",
      "batch 2173: loss 0.111477\n",
      "batch 2174: loss 0.080741\n",
      "batch 2175: loss 0.129347\n",
      "batch 2176: loss 0.053974\n",
      "batch 2177: loss 0.066544\n",
      "batch 2178: loss 0.082850\n",
      "batch 2179: loss 0.051156\n",
      "batch 2180: loss 0.190113\n",
      "batch 2181: loss 0.194008\n",
      "batch 2182: loss 0.056354\n",
      "batch 2183: loss 0.205259\n",
      "batch 2184: loss 0.170406\n",
      "batch 2185: loss 0.090476\n",
      "batch 2186: loss 0.246372\n",
      "batch 2187: loss 0.073147\n",
      "batch 2188: loss 0.085466\n",
      "batch 2189: loss 0.046200\n",
      "batch 2190: loss 0.122022\n",
      "batch 2191: loss 0.302828\n",
      "batch 2192: loss 0.022567\n",
      "batch 2193: loss 0.022034\n",
      "batch 2194: loss 0.076124\n",
      "batch 2195: loss 0.144327\n",
      "batch 2196: loss 0.158479\n",
      "batch 2197: loss 0.067808\n",
      "batch 2198: loss 0.081015\n",
      "batch 2199: loss 0.089167\n",
      "batch 2200: loss 0.110211\n",
      "batch 2201: loss 0.246714\n",
      "batch 2202: loss 0.061188\n",
      "batch 2203: loss 0.052266\n",
      "batch 2204: loss 0.197677\n",
      "batch 2205: loss 0.366375\n",
      "batch 2206: loss 0.051731\n",
      "batch 2207: loss 0.121802\n",
      "batch 2208: loss 0.068308\n",
      "batch 2209: loss 0.093638\n",
      "batch 2210: loss 0.088075\n",
      "batch 2211: loss 0.086310\n",
      "batch 2212: loss 0.063116\n",
      "batch 2213: loss 0.036424\n",
      "batch 2214: loss 0.191675\n",
      "batch 2215: loss 0.060479\n",
      "batch 2216: loss 0.130832\n",
      "batch 2217: loss 0.037777\n",
      "batch 2218: loss 0.170719\n",
      "batch 2219: loss 0.042390\n",
      "batch 2220: loss 0.091361\n",
      "batch 2221: loss 0.056324\n",
      "batch 2222: loss 0.146833\n",
      "batch 2223: loss 0.039477\n",
      "batch 2224: loss 0.102458\n",
      "batch 2225: loss 0.094587\n",
      "batch 2226: loss 0.233846\n",
      "batch 2227: loss 0.099698\n",
      "batch 2228: loss 0.105886\n",
      "batch 2229: loss 0.224453\n",
      "batch 2230: loss 0.077592\n",
      "batch 2231: loss 0.127521\n",
      "batch 2232: loss 0.093296\n",
      "batch 2233: loss 0.333533\n",
      "batch 2234: loss 0.137544\n",
      "batch 2235: loss 0.082625\n",
      "batch 2236: loss 0.035591\n",
      "batch 2237: loss 0.158270\n",
      "batch 2238: loss 0.052242\n",
      "batch 2239: loss 0.179473\n",
      "batch 2240: loss 0.011268\n",
      "batch 2241: loss 0.090831\n",
      "batch 2242: loss 0.065399\n",
      "batch 2243: loss 0.037535\n",
      "batch 2244: loss 0.141691\n",
      "batch 2245: loss 0.043796\n",
      "batch 2246: loss 0.056689\n",
      "batch 2247: loss 0.150138\n",
      "batch 2248: loss 0.100259\n",
      "batch 2249: loss 0.117759\n",
      "batch 2250: loss 0.059512\n",
      "batch 2251: loss 0.142425\n",
      "batch 2252: loss 0.140708\n",
      "batch 2253: loss 0.054586\n",
      "batch 2254: loss 0.058408\n",
      "batch 2255: loss 0.025842\n",
      "batch 2256: loss 0.210955\n",
      "batch 2257: loss 0.123011\n",
      "batch 2258: loss 0.187271\n",
      "batch 2259: loss 0.174202\n",
      "batch 2260: loss 0.041863\n",
      "batch 2261: loss 0.124695\n",
      "batch 2262: loss 0.074296\n",
      "batch 2263: loss 0.078592\n",
      "batch 2264: loss 0.020568\n",
      "batch 2265: loss 0.016672\n",
      "batch 2266: loss 0.186744\n",
      "batch 2267: loss 0.170523\n",
      "batch 2268: loss 0.098164\n",
      "batch 2269: loss 0.058005\n",
      "batch 2270: loss 0.039456\n",
      "batch 2271: loss 0.047767\n",
      "batch 2272: loss 0.257772\n",
      "batch 2273: loss 0.065005\n",
      "batch 2274: loss 0.091147\n",
      "batch 2275: loss 0.155273\n",
      "batch 2276: loss 0.237086\n",
      "batch 2277: loss 0.055832\n",
      "batch 2278: loss 0.021599\n",
      "batch 2279: loss 0.069330\n",
      "batch 2280: loss 0.044813\n",
      "batch 2281: loss 0.152993\n",
      "batch 2282: loss 0.148461\n",
      "batch 2283: loss 0.049711\n",
      "batch 2284: loss 0.061669\n",
      "batch 2285: loss 0.041355\n",
      "batch 2286: loss 0.092472\n",
      "batch 2287: loss 0.040488\n",
      "batch 2288: loss 0.186315\n",
      "batch 2289: loss 0.136107\n",
      "batch 2290: loss 0.114978\n",
      "batch 2291: loss 0.102770\n",
      "batch 2292: loss 0.051748\n",
      "batch 2293: loss 0.107999\n",
      "batch 2294: loss 0.113382\n",
      "batch 2295: loss 0.057565\n",
      "batch 2296: loss 0.055368\n",
      "batch 2297: loss 0.062624\n",
      "batch 2298: loss 0.049118\n",
      "batch 2299: loss 0.052640\n",
      "batch 2300: loss 0.027528\n",
      "batch 2301: loss 0.089090\n",
      "batch 2302: loss 0.332144\n",
      "batch 2303: loss 0.124301\n",
      "batch 2304: loss 0.039449\n",
      "batch 2305: loss 0.180642\n",
      "batch 2306: loss 0.064042\n",
      "batch 2307: loss 0.074079\n",
      "batch 2308: loss 0.104686\n",
      "batch 2309: loss 0.041281\n",
      "batch 2310: loss 0.116884\n",
      "batch 2311: loss 0.077547\n",
      "batch 2312: loss 0.151628\n",
      "batch 2313: loss 0.139184\n",
      "batch 2314: loss 0.074371\n",
      "batch 2315: loss 0.258056\n",
      "batch 2316: loss 0.075329\n",
      "batch 2317: loss 0.105482\n",
      "batch 2318: loss 0.049924\n",
      "batch 2319: loss 0.090940\n",
      "batch 2320: loss 0.053647\n",
      "batch 2321: loss 0.168047\n",
      "batch 2322: loss 0.153647\n",
      "batch 2323: loss 0.134973\n",
      "batch 2324: loss 0.077215\n",
      "batch 2325: loss 0.033229\n",
      "batch 2326: loss 0.111475\n",
      "batch 2327: loss 0.238410\n",
      "batch 2328: loss 0.122180\n",
      "batch 2329: loss 0.091884\n",
      "batch 2330: loss 0.175472\n",
      "batch 2331: loss 0.035263\n",
      "batch 2332: loss 0.081839\n",
      "batch 2333: loss 0.135141\n",
      "batch 2334: loss 0.041928\n",
      "batch 2335: loss 0.147789\n",
      "batch 2336: loss 0.177601\n",
      "batch 2337: loss 0.219316\n",
      "batch 2338: loss 0.156555\n",
      "batch 2339: loss 0.120840\n",
      "batch 2340: loss 0.247625\n",
      "batch 2341: loss 0.097640\n",
      "batch 2342: loss 0.035462\n",
      "batch 2343: loss 0.181469\n",
      "batch 2344: loss 0.024283\n",
      "batch 2345: loss 0.350228\n",
      "batch 2346: loss 0.084885\n",
      "batch 2347: loss 0.037832\n",
      "batch 2348: loss 0.028413\n",
      "batch 2349: loss 0.088916\n",
      "batch 2350: loss 0.104133\n",
      "batch 2351: loss 0.024586\n",
      "batch 2352: loss 0.088459\n",
      "batch 2353: loss 0.142570\n",
      "batch 2354: loss 0.125281\n",
      "batch 2355: loss 0.086338\n",
      "batch 2356: loss 0.047817\n",
      "batch 2357: loss 0.092871\n",
      "batch 2358: loss 0.162808\n",
      "batch 2359: loss 0.141603\n",
      "batch 2360: loss 0.025230\n",
      "batch 2361: loss 0.171865\n",
      "batch 2362: loss 0.043944\n",
      "batch 2363: loss 0.155044\n",
      "batch 2364: loss 0.036854\n",
      "batch 2365: loss 0.207492\n",
      "batch 2366: loss 0.100609\n",
      "batch 2367: loss 0.048701\n",
      "batch 2368: loss 0.034124\n",
      "batch 2369: loss 0.022883\n",
      "batch 2370: loss 0.105386\n",
      "batch 2371: loss 0.053538\n",
      "batch 2372: loss 0.076292\n",
      "batch 2373: loss 0.020592\n",
      "batch 2374: loss 0.281567\n",
      "batch 2375: loss 0.028481\n",
      "batch 2376: loss 0.090701\n",
      "batch 2377: loss 0.063258\n",
      "batch 2378: loss 0.044655\n",
      "batch 2379: loss 0.058967\n",
      "batch 2380: loss 0.088523\n",
      "batch 2381: loss 0.046462\n",
      "batch 2382: loss 0.073318\n",
      "batch 2383: loss 0.132782\n",
      "batch 2384: loss 0.137308\n",
      "batch 2385: loss 0.083693\n",
      "batch 2386: loss 0.101510\n",
      "batch 2387: loss 0.066946\n",
      "batch 2388: loss 0.168518\n",
      "batch 2389: loss 0.070871\n",
      "batch 2390: loss 0.117932\n",
      "batch 2391: loss 0.130658\n",
      "batch 2392: loss 0.038155\n",
      "batch 2393: loss 0.064528\n",
      "batch 2394: loss 0.121996\n",
      "batch 2395: loss 0.056670\n",
      "batch 2396: loss 0.061953\n",
      "batch 2397: loss 0.136776\n",
      "batch 2398: loss 0.210967\n",
      "batch 2399: loss 0.170739\n",
      "batch 2400: loss 0.053031\n",
      "batch 2401: loss 0.082221\n",
      "batch 2402: loss 0.109588\n",
      "batch 2403: loss 0.053254\n",
      "batch 2404: loss 0.111051\n",
      "batch 2405: loss 0.061881\n",
      "batch 2406: loss 0.093918\n",
      "batch 2407: loss 0.025452\n",
      "batch 2408: loss 0.100440\n",
      "batch 2409: loss 0.141973\n",
      "batch 2410: loss 0.069130\n",
      "batch 2411: loss 0.283675\n",
      "batch 2412: loss 0.123361\n",
      "batch 2413: loss 0.291639\n",
      "batch 2414: loss 0.085883\n",
      "batch 2415: loss 0.086159\n",
      "batch 2416: loss 0.063306\n",
      "batch 2417: loss 0.025746\n",
      "batch 2418: loss 0.077603\n",
      "batch 2419: loss 0.158474\n",
      "batch 2420: loss 0.062932\n",
      "batch 2421: loss 0.162149\n",
      "batch 2422: loss 0.029757\n",
      "batch 2423: loss 0.058897\n",
      "batch 2424: loss 0.092760\n",
      "batch 2425: loss 0.188567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2426: loss 0.092342\n",
      "batch 2427: loss 0.042075\n",
      "batch 2428: loss 0.118304\n",
      "batch 2429: loss 0.031642\n",
      "batch 2430: loss 0.081528\n",
      "batch 2431: loss 0.109851\n",
      "batch 2432: loss 0.150151\n",
      "batch 2433: loss 0.069280\n",
      "batch 2434: loss 0.346908\n",
      "batch 2435: loss 0.083859\n",
      "batch 2436: loss 0.146617\n",
      "batch 2437: loss 0.057186\n",
      "batch 2438: loss 0.122216\n",
      "batch 2439: loss 0.037322\n",
      "batch 2440: loss 0.128428\n",
      "batch 2441: loss 0.059479\n",
      "batch 2442: loss 0.147311\n",
      "batch 2443: loss 0.144581\n",
      "batch 2444: loss 0.110834\n",
      "batch 2445: loss 0.163967\n",
      "batch 2446: loss 0.124461\n",
      "batch 2447: loss 0.111820\n",
      "batch 2448: loss 0.187663\n",
      "batch 2449: loss 0.143985\n",
      "batch 2450: loss 0.039973\n",
      "batch 2451: loss 0.070139\n",
      "batch 2452: loss 0.069986\n",
      "batch 2453: loss 0.139983\n",
      "batch 2454: loss 0.062261\n",
      "batch 2455: loss 0.013478\n",
      "batch 2456: loss 0.063562\n",
      "batch 2457: loss 0.079477\n",
      "batch 2458: loss 0.137453\n",
      "batch 2459: loss 0.233212\n",
      "batch 2460: loss 0.118460\n",
      "batch 2461: loss 0.225574\n",
      "batch 2462: loss 0.105603\n",
      "batch 2463: loss 0.130811\n",
      "batch 2464: loss 0.192559\n",
      "batch 2465: loss 0.149434\n",
      "batch 2466: loss 0.156678\n",
      "batch 2467: loss 0.080892\n",
      "batch 2468: loss 0.132640\n",
      "batch 2469: loss 0.102739\n",
      "batch 2470: loss 0.062384\n",
      "batch 2471: loss 0.117413\n",
      "batch 2472: loss 0.109702\n",
      "batch 2473: loss 0.236231\n",
      "batch 2474: loss 0.193036\n",
      "batch 2475: loss 0.125677\n",
      "batch 2476: loss 0.108954\n",
      "batch 2477: loss 0.148627\n",
      "batch 2478: loss 0.148955\n",
      "batch 2479: loss 0.162648\n",
      "batch 2480: loss 0.102980\n",
      "batch 2481: loss 0.328614\n",
      "batch 2482: loss 0.079264\n",
      "batch 2483: loss 0.227019\n",
      "batch 2484: loss 0.073017\n",
      "batch 2485: loss 0.264505\n",
      "batch 2486: loss 0.091248\n",
      "batch 2487: loss 0.069448\n",
      "batch 2488: loss 0.131529\n",
      "batch 2489: loss 0.142944\n",
      "batch 2490: loss 0.053510\n",
      "batch 2491: loss 0.095151\n",
      "batch 2492: loss 0.022949\n",
      "batch 2493: loss 0.087667\n",
      "batch 2494: loss 0.035248\n",
      "batch 2495: loss 0.244316\n",
      "batch 2496: loss 0.176157\n",
      "batch 2497: loss 0.078956\n",
      "batch 2498: loss 0.159168\n",
      "batch 2499: loss 0.093820\n",
      "batch 2500: loss 0.152270\n",
      "batch 2501: loss 0.069678\n",
      "batch 2502: loss 0.037304\n",
      "batch 2503: loss 0.090989\n",
      "batch 2504: loss 0.072096\n",
      "batch 2505: loss 0.176212\n",
      "batch 2506: loss 0.165558\n",
      "batch 2507: loss 0.052717\n",
      "batch 2508: loss 0.079504\n",
      "batch 2509: loss 0.101935\n",
      "batch 2510: loss 0.050465\n",
      "batch 2511: loss 0.057859\n",
      "batch 2512: loss 0.051092\n",
      "batch 2513: loss 0.154090\n",
      "batch 2514: loss 0.438353\n",
      "batch 2515: loss 0.185261\n",
      "batch 2516: loss 0.092555\n",
      "batch 2517: loss 0.020930\n",
      "batch 2518: loss 0.121107\n",
      "batch 2519: loss 0.107530\n",
      "batch 2520: loss 0.195706\n",
      "batch 2521: loss 0.131245\n",
      "batch 2522: loss 0.052542\n",
      "batch 2523: loss 0.046524\n",
      "batch 2524: loss 0.037927\n",
      "batch 2525: loss 0.180671\n",
      "batch 2526: loss 0.030296\n",
      "batch 2527: loss 0.190670\n",
      "batch 2528: loss 0.030459\n",
      "batch 2529: loss 0.070375\n",
      "batch 2530: loss 0.071436\n",
      "batch 2531: loss 0.031504\n",
      "batch 2532: loss 0.353264\n",
      "batch 2533: loss 0.025813\n",
      "batch 2534: loss 0.143874\n",
      "batch 2535: loss 0.066426\n",
      "batch 2536: loss 0.203435\n",
      "batch 2537: loss 0.145267\n",
      "batch 2538: loss 0.052243\n",
      "batch 2539: loss 0.075164\n",
      "batch 2540: loss 0.047328\n",
      "batch 2541: loss 0.049394\n",
      "batch 2542: loss 0.131170\n",
      "batch 2543: loss 0.161758\n",
      "batch 2544: loss 0.129939\n",
      "batch 2545: loss 0.194696\n",
      "batch 2546: loss 0.066622\n",
      "batch 2547: loss 0.065587\n",
      "batch 2548: loss 0.157557\n",
      "batch 2549: loss 0.282002\n",
      "batch 2550: loss 0.283841\n",
      "batch 2551: loss 0.131739\n",
      "batch 2552: loss 0.131270\n",
      "batch 2553: loss 0.026573\n",
      "batch 2554: loss 0.155620\n",
      "batch 2555: loss 0.080893\n",
      "batch 2556: loss 0.092741\n",
      "batch 2557: loss 0.050542\n",
      "batch 2558: loss 0.078154\n",
      "batch 2559: loss 0.223309\n",
      "batch 2560: loss 0.050213\n",
      "batch 2561: loss 0.115312\n",
      "batch 2562: loss 0.049980\n",
      "batch 2563: loss 0.114293\n",
      "batch 2564: loss 0.088929\n",
      "batch 2565: loss 0.044621\n",
      "batch 2566: loss 0.081013\n",
      "batch 2567: loss 0.106273\n",
      "batch 2568: loss 0.056829\n",
      "batch 2569: loss 0.122148\n",
      "batch 2570: loss 0.075722\n",
      "batch 2571: loss 0.078619\n",
      "batch 2572: loss 0.038047\n",
      "batch 2573: loss 0.167505\n",
      "batch 2574: loss 0.126196\n",
      "batch 2575: loss 0.101363\n",
      "batch 2576: loss 0.144348\n",
      "batch 2577: loss 0.080943\n",
      "batch 2578: loss 0.155492\n",
      "batch 2579: loss 0.062266\n",
      "batch 2580: loss 0.092550\n",
      "batch 2581: loss 0.071075\n",
      "batch 2582: loss 0.316462\n",
      "batch 2583: loss 0.025034\n",
      "batch 2584: loss 0.052295\n",
      "batch 2585: loss 0.097233\n",
      "batch 2586: loss 0.138491\n",
      "batch 2587: loss 0.240972\n",
      "batch 2588: loss 0.095207\n",
      "batch 2589: loss 0.091158\n",
      "batch 2590: loss 0.062275\n",
      "batch 2591: loss 0.120597\n",
      "batch 2592: loss 0.023519\n",
      "batch 2593: loss 0.033616\n",
      "batch 2594: loss 0.158707\n",
      "batch 2595: loss 0.021353\n",
      "batch 2596: loss 0.264596\n",
      "batch 2597: loss 0.084034\n",
      "batch 2598: loss 0.095013\n",
      "batch 2599: loss 0.064839\n",
      "batch 2600: loss 0.152073\n",
      "batch 2601: loss 0.047213\n",
      "batch 2602: loss 0.063262\n",
      "batch 2603: loss 0.138658\n",
      "batch 2604: loss 0.023297\n",
      "batch 2605: loss 0.025262\n",
      "batch 2606: loss 0.104967\n",
      "batch 2607: loss 0.131647\n",
      "batch 2608: loss 0.167112\n",
      "batch 2609: loss 0.093691\n",
      "batch 2610: loss 0.306853\n",
      "batch 2611: loss 0.111302\n",
      "batch 2612: loss 0.097013\n",
      "batch 2613: loss 0.043435\n",
      "batch 2614: loss 0.030980\n",
      "batch 2615: loss 0.064392\n",
      "batch 2616: loss 0.143841\n",
      "batch 2617: loss 0.055632\n",
      "batch 2618: loss 0.057409\n",
      "batch 2619: loss 0.014682\n",
      "batch 2620: loss 0.084572\n",
      "batch 2621: loss 0.059182\n",
      "batch 2622: loss 0.035506\n",
      "batch 2623: loss 0.040361\n",
      "batch 2624: loss 0.071205\n",
      "batch 2625: loss 0.179912\n",
      "batch 2626: loss 0.027938\n",
      "batch 2627: loss 0.048298\n",
      "batch 2628: loss 0.217797\n",
      "batch 2629: loss 0.143076\n",
      "batch 2630: loss 0.068935\n",
      "batch 2631: loss 0.140530\n",
      "batch 2632: loss 0.123334\n",
      "batch 2633: loss 0.177872\n",
      "batch 2634: loss 0.063944\n",
      "batch 2635: loss 0.120386\n",
      "batch 2636: loss 0.084781\n",
      "batch 2637: loss 0.134595\n",
      "batch 2638: loss 0.015503\n",
      "batch 2639: loss 0.167238\n",
      "batch 2640: loss 0.174434\n",
      "batch 2641: loss 0.161544\n",
      "batch 2642: loss 0.058310\n",
      "batch 2643: loss 0.162768\n",
      "batch 2644: loss 0.055484\n",
      "batch 2645: loss 0.171256\n",
      "batch 2646: loss 0.050150\n",
      "batch 2647: loss 0.061632\n",
      "batch 2648: loss 0.052717\n",
      "batch 2649: loss 0.070130\n",
      "batch 2650: loss 0.054894\n",
      "batch 2651: loss 0.072277\n",
      "batch 2652: loss 0.231273\n",
      "batch 2653: loss 0.037370\n",
      "batch 2654: loss 0.130098\n",
      "batch 2655: loss 0.058150\n",
      "batch 2656: loss 0.180016\n",
      "batch 2657: loss 0.034784\n",
      "batch 2658: loss 0.170457\n",
      "batch 2659: loss 0.162453\n",
      "batch 2660: loss 0.104393\n",
      "batch 2661: loss 0.037274\n",
      "batch 2662: loss 0.080156\n",
      "batch 2663: loss 0.090070\n",
      "batch 2664: loss 0.186577\n",
      "batch 2665: loss 0.045487\n",
      "batch 2666: loss 0.113006\n",
      "batch 2667: loss 0.070300\n",
      "batch 2668: loss 0.060312\n",
      "batch 2669: loss 0.109133\n",
      "batch 2670: loss 0.055746\n",
      "batch 2671: loss 0.049589\n",
      "batch 2672: loss 0.093069\n",
      "batch 2673: loss 0.046465\n",
      "batch 2674: loss 0.065329\n",
      "batch 2675: loss 0.078937\n",
      "batch 2676: loss 0.127357\n",
      "batch 2677: loss 0.154940\n",
      "batch 2678: loss 0.172305\n",
      "batch 2679: loss 0.079655\n",
      "batch 2680: loss 0.060638\n",
      "batch 2681: loss 0.154919\n",
      "batch 2682: loss 0.107205\n",
      "batch 2683: loss 0.050265\n",
      "batch 2684: loss 0.093312\n",
      "batch 2685: loss 0.038540\n",
      "batch 2686: loss 0.082059\n",
      "batch 2687: loss 0.079606\n",
      "batch 2688: loss 0.014815\n",
      "batch 2689: loss 0.122359\n",
      "batch 2690: loss 0.035611\n",
      "batch 2691: loss 0.118983\n",
      "batch 2692: loss 0.056224\n",
      "batch 2693: loss 0.054809\n",
      "batch 2694: loss 0.185248\n",
      "batch 2695: loss 0.099732\n",
      "batch 2696: loss 0.088460\n",
      "batch 2697: loss 0.184206\n",
      "batch 2698: loss 0.038037\n",
      "batch 2699: loss 0.057004\n",
      "batch 2700: loss 0.010302\n",
      "batch 2701: loss 0.079176\n",
      "batch 2702: loss 0.060214\n",
      "batch 2703: loss 0.049914\n",
      "batch 2704: loss 0.126135\n",
      "batch 2705: loss 0.160680\n",
      "batch 2706: loss 0.086575\n",
      "batch 2707: loss 0.043444\n",
      "batch 2708: loss 0.102984\n",
      "batch 2709: loss 0.045769\n",
      "batch 2710: loss 0.032649\n",
      "batch 2711: loss 0.039785\n",
      "batch 2712: loss 0.199154\n",
      "batch 2713: loss 0.042267\n",
      "batch 2714: loss 0.165152\n",
      "batch 2715: loss 0.045251\n",
      "batch 2716: loss 0.068169\n",
      "batch 2717: loss 0.084947\n",
      "batch 2718: loss 0.108567\n",
      "batch 2719: loss 0.041995\n",
      "batch 2720: loss 0.041313\n",
      "batch 2721: loss 0.041867\n",
      "batch 2722: loss 0.148061\n",
      "batch 2723: loss 0.063285\n",
      "batch 2724: loss 0.062872\n",
      "batch 2725: loss 0.096236\n",
      "batch 2726: loss 0.116604\n",
      "batch 2727: loss 0.065907\n",
      "batch 2728: loss 0.239015\n",
      "batch 2729: loss 0.082265\n",
      "batch 2730: loss 0.054086\n",
      "batch 2731: loss 0.072856\n",
      "batch 2732: loss 0.158996\n",
      "batch 2733: loss 0.038413\n",
      "batch 2734: loss 0.148596\n",
      "batch 2735: loss 0.037836\n",
      "batch 2736: loss 0.077168\n",
      "batch 2737: loss 0.055194\n",
      "batch 2738: loss 0.122601\n",
      "batch 2739: loss 0.036006\n",
      "batch 2740: loss 0.110687\n",
      "batch 2741: loss 0.091647\n",
      "batch 2742: loss 0.094299\n",
      "batch 2743: loss 0.196087\n",
      "batch 2744: loss 0.070375\n",
      "batch 2745: loss 0.037241\n",
      "batch 2746: loss 0.100036\n",
      "batch 2747: loss 0.037265\n",
      "batch 2748: loss 0.146428\n",
      "batch 2749: loss 0.150357\n",
      "batch 2750: loss 0.050026\n",
      "batch 2751: loss 0.239839\n",
      "batch 2752: loss 0.089667\n",
      "batch 2753: loss 0.045679\n",
      "batch 2754: loss 0.048491\n",
      "batch 2755: loss 0.057915\n",
      "batch 2756: loss 0.140220\n",
      "batch 2757: loss 0.081542\n",
      "batch 2758: loss 0.100792\n",
      "batch 2759: loss 0.042159\n",
      "batch 2760: loss 0.026426\n",
      "batch 2761: loss 0.183066\n",
      "batch 2762: loss 0.044814\n",
      "batch 2763: loss 0.035564\n",
      "batch 2764: loss 0.038993\n",
      "batch 2765: loss 0.091914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2766: loss 0.012767\n",
      "batch 2767: loss 0.038075\n",
      "batch 2768: loss 0.063535\n",
      "batch 2769: loss 0.031152\n",
      "batch 2770: loss 0.159058\n",
      "batch 2771: loss 0.056965\n",
      "batch 2772: loss 0.155945\n",
      "batch 2773: loss 0.028515\n",
      "batch 2774: loss 0.086616\n",
      "batch 2775: loss 0.033096\n",
      "batch 2776: loss 0.100445\n",
      "batch 2777: loss 0.228410\n",
      "batch 2778: loss 0.134527\n",
      "batch 2779: loss 0.026589\n",
      "batch 2780: loss 0.108853\n",
      "batch 2781: loss 0.164948\n",
      "batch 2782: loss 0.197621\n",
      "batch 2783: loss 0.211598\n",
      "batch 2784: loss 0.078119\n",
      "batch 2785: loss 0.054689\n",
      "batch 2786: loss 0.138103\n",
      "batch 2787: loss 0.018633\n",
      "batch 2788: loss 0.082666\n",
      "batch 2789: loss 0.258701\n",
      "batch 2790: loss 0.058435\n",
      "batch 2791: loss 0.090804\n",
      "batch 2792: loss 0.173138\n",
      "batch 2793: loss 0.123597\n",
      "batch 2794: loss 0.098430\n",
      "batch 2795: loss 0.124684\n",
      "batch 2796: loss 0.089875\n",
      "batch 2797: loss 0.061262\n",
      "batch 2798: loss 0.244196\n",
      "batch 2799: loss 0.082091\n",
      "batch 2800: loss 0.130923\n",
      "batch 2801: loss 0.141149\n",
      "batch 2802: loss 0.208882\n",
      "batch 2803: loss 0.123651\n",
      "batch 2804: loss 0.079682\n",
      "batch 2805: loss 0.269964\n",
      "batch 2806: loss 0.078229\n",
      "batch 2807: loss 0.193773\n",
      "batch 2808: loss 0.016056\n",
      "batch 2809: loss 0.058771\n",
      "batch 2810: loss 0.021068\n",
      "batch 2811: loss 0.103434\n",
      "batch 2812: loss 0.022011\n",
      "batch 2813: loss 0.122863\n",
      "batch 2814: loss 0.198184\n",
      "batch 2815: loss 0.023267\n",
      "batch 2816: loss 0.032278\n",
      "batch 2817: loss 0.062824\n",
      "batch 2818: loss 0.129245\n",
      "batch 2819: loss 0.048147\n",
      "batch 2820: loss 0.025781\n",
      "batch 2821: loss 0.058444\n",
      "batch 2822: loss 0.029467\n",
      "batch 2823: loss 0.087507\n",
      "batch 2824: loss 0.086909\n",
      "batch 2825: loss 0.046219\n",
      "batch 2826: loss 0.061116\n",
      "batch 2827: loss 0.146303\n",
      "batch 2828: loss 0.122561\n",
      "batch 2829: loss 0.069014\n",
      "batch 2830: loss 0.090245\n",
      "batch 2831: loss 0.112151\n",
      "batch 2832: loss 0.078215\n",
      "batch 2833: loss 0.247985\n",
      "batch 2834: loss 0.018008\n",
      "batch 2835: loss 0.073912\n",
      "batch 2836: loss 0.273597\n",
      "batch 2837: loss 0.075679\n",
      "batch 2838: loss 0.025001\n",
      "batch 2839: loss 0.144844\n",
      "batch 2840: loss 0.045326\n",
      "batch 2841: loss 0.133295\n",
      "batch 2842: loss 0.062162\n",
      "batch 2843: loss 0.024730\n",
      "batch 2844: loss 0.236771\n",
      "batch 2845: loss 0.067387\n",
      "batch 2846: loss 0.124880\n",
      "batch 2847: loss 0.045625\n",
      "batch 2848: loss 0.043363\n",
      "batch 2849: loss 0.163030\n",
      "batch 2850: loss 0.157972\n",
      "batch 2851: loss 0.172208\n",
      "batch 2852: loss 0.046341\n",
      "batch 2853: loss 0.019200\n",
      "batch 2854: loss 0.058544\n",
      "batch 2855: loss 0.039171\n",
      "batch 2856: loss 0.042174\n",
      "batch 2857: loss 0.114075\n",
      "batch 2858: loss 0.058853\n",
      "batch 2859: loss 0.025500\n",
      "batch 2860: loss 0.044521\n",
      "batch 2861: loss 0.051008\n",
      "batch 2862: loss 0.191447\n",
      "batch 2863: loss 0.182065\n",
      "batch 2864: loss 0.213638\n",
      "batch 2865: loss 0.110680\n",
      "batch 2866: loss 0.086876\n",
      "batch 2867: loss 0.121535\n",
      "batch 2868: loss 0.106316\n",
      "batch 2869: loss 0.120138\n",
      "batch 2870: loss 0.134046\n",
      "batch 2871: loss 0.080222\n",
      "batch 2872: loss 0.121159\n",
      "batch 2873: loss 0.082849\n",
      "batch 2874: loss 0.058735\n",
      "batch 2875: loss 0.082430\n",
      "batch 2876: loss 0.040946\n",
      "batch 2877: loss 0.091570\n",
      "batch 2878: loss 0.027152\n",
      "batch 2879: loss 0.142154\n",
      "batch 2880: loss 0.055100\n",
      "batch 2881: loss 0.291182\n",
      "batch 2882: loss 0.035477\n",
      "batch 2883: loss 0.101210\n",
      "batch 2884: loss 0.049063\n",
      "batch 2885: loss 0.117294\n",
      "batch 2886: loss 0.096756\n",
      "batch 2887: loss 0.047471\n",
      "batch 2888: loss 0.058737\n",
      "batch 2889: loss 0.050155\n",
      "batch 2890: loss 0.084145\n",
      "batch 2891: loss 0.068220\n",
      "batch 2892: loss 0.031294\n",
      "batch 2893: loss 0.039640\n",
      "batch 2894: loss 0.088841\n",
      "batch 2895: loss 0.120032\n",
      "batch 2896: loss 0.075451\n",
      "batch 2897: loss 0.059589\n",
      "batch 2898: loss 0.142726\n",
      "batch 2899: loss 0.162144\n",
      "batch 2900: loss 0.115219\n",
      "batch 2901: loss 0.120836\n",
      "batch 2902: loss 0.027322\n",
      "batch 2903: loss 0.081683\n",
      "batch 2904: loss 0.133553\n",
      "batch 2905: loss 0.023024\n",
      "batch 2906: loss 0.024751\n",
      "batch 2907: loss 0.312936\n",
      "batch 2908: loss 0.133549\n",
      "batch 2909: loss 0.080937\n",
      "batch 2910: loss 0.047665\n",
      "batch 2911: loss 0.152349\n",
      "batch 2912: loss 0.050751\n",
      "batch 2913: loss 0.089340\n",
      "batch 2914: loss 0.028912\n",
      "batch 2915: loss 0.124424\n",
      "batch 2916: loss 0.028064\n",
      "batch 2917: loss 0.081582\n",
      "batch 2918: loss 0.104105\n",
      "batch 2919: loss 0.033839\n",
      "batch 2920: loss 0.051960\n",
      "batch 2921: loss 0.098018\n",
      "batch 2922: loss 0.247102\n",
      "batch 2923: loss 0.055784\n",
      "batch 2924: loss 0.011795\n",
      "batch 2925: loss 0.066142\n",
      "batch 2926: loss 0.224348\n",
      "batch 2927: loss 0.020848\n",
      "batch 2928: loss 0.142889\n",
      "batch 2929: loss 0.100418\n",
      "batch 2930: loss 0.066610\n",
      "batch 2931: loss 0.036183\n",
      "batch 2932: loss 0.011745\n",
      "batch 2933: loss 0.024214\n",
      "batch 2934: loss 0.031222\n",
      "batch 2935: loss 0.088863\n",
      "batch 2936: loss 0.146018\n",
      "batch 2937: loss 0.092244\n",
      "batch 2938: loss 0.031609\n",
      "batch 2939: loss 0.135422\n",
      "batch 2940: loss 0.085460\n",
      "batch 2941: loss 0.017916\n",
      "batch 2942: loss 0.018712\n",
      "batch 2943: loss 0.046926\n",
      "batch 2944: loss 0.198532\n",
      "batch 2945: loss 0.067396\n",
      "batch 2946: loss 0.015481\n",
      "batch 2947: loss 0.171089\n",
      "batch 2948: loss 0.132853\n",
      "batch 2949: loss 0.072952\n",
      "batch 2950: loss 0.011078\n",
      "batch 2951: loss 0.074115\n",
      "batch 2952: loss 0.135575\n",
      "batch 2953: loss 0.262281\n",
      "batch 2954: loss 0.127306\n",
      "batch 2955: loss 0.077230\n",
      "batch 2956: loss 0.084701\n",
      "batch 2957: loss 0.083963\n",
      "batch 2958: loss 0.139103\n",
      "batch 2959: loss 0.048275\n",
      "batch 2960: loss 0.043003\n",
      "batch 2961: loss 0.044778\n",
      "batch 2962: loss 0.171500\n",
      "batch 2963: loss 0.113403\n",
      "batch 2964: loss 0.066987\n",
      "batch 2965: loss 0.205433\n",
      "batch 2966: loss 0.048509\n",
      "batch 2967: loss 0.057169\n",
      "batch 2968: loss 0.255637\n",
      "batch 2969: loss 0.148784\n",
      "batch 2970: loss 0.086192\n",
      "batch 2971: loss 0.062615\n",
      "batch 2972: loss 0.112452\n",
      "batch 2973: loss 0.124279\n",
      "batch 2974: loss 0.028474\n",
      "batch 2975: loss 0.164119\n",
      "batch 2976: loss 0.047444\n",
      "batch 2977: loss 0.044554\n",
      "batch 2978: loss 0.090202\n",
      "batch 2979: loss 0.024118\n",
      "batch 2980: loss 0.027492\n",
      "batch 2981: loss 0.052855\n",
      "batch 2982: loss 0.073628\n",
      "batch 2983: loss 0.124463\n",
      "batch 2984: loss 0.016473\n",
      "batch 2985: loss 0.125656\n",
      "batch 2986: loss 0.042007\n",
      "batch 2987: loss 0.065576\n",
      "batch 2988: loss 0.036122\n",
      "batch 2989: loss 0.072296\n",
      "batch 2990: loss 0.017289\n",
      "batch 2991: loss 0.104339\n",
      "batch 2992: loss 0.139451\n",
      "batch 2993: loss 0.027513\n",
      "batch 2994: loss 0.111466\n",
      "batch 2995: loss 0.109483\n",
      "batch 2996: loss 0.041958\n",
      "batch 2997: loss 0.210755\n",
      "batch 2998: loss 0.049603\n",
      "batch 2999: loss 0.135184\n",
      "batch 3000: loss 0.059838\n",
      "batch 3001: loss 0.023001\n",
      "batch 3002: loss 0.083183\n",
      "batch 3003: loss 0.244967\n",
      "batch 3004: loss 0.066723\n",
      "batch 3005: loss 0.059600\n",
      "batch 3006: loss 0.056280\n",
      "batch 3007: loss 0.084623\n",
      "batch 3008: loss 0.032587\n",
      "batch 3009: loss 0.054858\n",
      "batch 3010: loss 0.124836\n",
      "batch 3011: loss 0.066558\n",
      "batch 3012: loss 0.024211\n",
      "batch 3013: loss 0.052222\n",
      "batch 3014: loss 0.067196\n",
      "batch 3015: loss 0.159306\n",
      "batch 3016: loss 0.042454\n",
      "batch 3017: loss 0.095682\n",
      "batch 3018: loss 0.036684\n",
      "batch 3019: loss 0.038529\n",
      "batch 3020: loss 0.078591\n",
      "batch 3021: loss 0.033041\n",
      "batch 3022: loss 0.014498\n",
      "batch 3023: loss 0.190158\n",
      "batch 3024: loss 0.057247\n",
      "batch 3025: loss 0.132360\n",
      "batch 3026: loss 0.103642\n",
      "batch 3027: loss 0.109483\n",
      "batch 3028: loss 0.029884\n",
      "batch 3029: loss 0.035407\n",
      "batch 3030: loss 0.030394\n",
      "batch 3031: loss 0.016234\n",
      "batch 3032: loss 0.062224\n",
      "batch 3033: loss 0.053598\n",
      "batch 3034: loss 0.024598\n",
      "batch 3035: loss 0.019258\n",
      "batch 3036: loss 0.146197\n",
      "batch 3037: loss 0.047048\n",
      "batch 3038: loss 0.060501\n",
      "batch 3039: loss 0.071182\n",
      "batch 3040: loss 0.045960\n",
      "batch 3041: loss 0.028456\n",
      "batch 3042: loss 0.104490\n",
      "batch 3043: loss 0.075190\n",
      "batch 3044: loss 0.015280\n",
      "batch 3045: loss 0.042963\n",
      "batch 3046: loss 0.029455\n",
      "batch 3047: loss 0.161729\n",
      "batch 3048: loss 0.050525\n",
      "batch 3049: loss 0.025026\n",
      "batch 3050: loss 0.097613\n",
      "batch 3051: loss 0.069327\n",
      "batch 3052: loss 0.064503\n",
      "batch 3053: loss 0.083502\n",
      "batch 3054: loss 0.071419\n",
      "batch 3055: loss 0.191509\n",
      "batch 3056: loss 0.067833\n",
      "batch 3057: loss 0.150014\n",
      "batch 3058: loss 0.035934\n",
      "batch 3059: loss 0.036295\n",
      "batch 3060: loss 0.079420\n",
      "batch 3061: loss 0.103774\n",
      "batch 3062: loss 0.036497\n",
      "batch 3063: loss 0.056135\n",
      "batch 3064: loss 0.036849\n",
      "batch 3065: loss 0.039060\n",
      "batch 3066: loss 0.192540\n",
      "batch 3067: loss 0.103566\n",
      "batch 3068: loss 0.030618\n",
      "batch 3069: loss 0.036091\n",
      "batch 3070: loss 0.045583\n",
      "batch 3071: loss 0.104116\n",
      "batch 3072: loss 0.068721\n",
      "batch 3073: loss 0.071670\n",
      "batch 3074: loss 0.248947\n",
      "batch 3075: loss 0.127921\n",
      "batch 3076: loss 0.269168\n",
      "batch 3077: loss 0.032303\n",
      "batch 3078: loss 0.029266\n",
      "batch 3079: loss 0.050970\n",
      "batch 3080: loss 0.312903\n",
      "batch 3081: loss 0.156918\n",
      "batch 3082: loss 0.100318\n",
      "batch 3083: loss 0.104454\n",
      "batch 3084: loss 0.123606\n",
      "batch 3085: loss 0.043772\n",
      "batch 3086: loss 0.048832\n",
      "batch 3087: loss 0.031179\n",
      "batch 3088: loss 0.061266\n",
      "batch 3089: loss 0.044076\n",
      "batch 3090: loss 0.207600\n",
      "batch 3091: loss 0.075611\n",
      "batch 3092: loss 0.077636\n",
      "batch 3093: loss 0.088647\n",
      "batch 3094: loss 0.057931\n",
      "batch 3095: loss 0.154989\n",
      "batch 3096: loss 0.080741\n",
      "batch 3097: loss 0.046971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3098: loss 0.068820\n",
      "batch 3099: loss 0.039196\n",
      "batch 3100: loss 0.030294\n",
      "batch 3101: loss 0.074461\n",
      "batch 3102: loss 0.064643\n",
      "batch 3103: loss 0.116151\n",
      "batch 3104: loss 0.035986\n",
      "batch 3105: loss 0.074147\n",
      "batch 3106: loss 0.250962\n",
      "batch 3107: loss 0.062614\n",
      "batch 3108: loss 0.117857\n",
      "batch 3109: loss 0.130425\n",
      "batch 3110: loss 0.082339\n",
      "batch 3111: loss 0.063900\n",
      "batch 3112: loss 0.029177\n",
      "batch 3113: loss 0.057164\n",
      "batch 3114: loss 0.063341\n",
      "batch 3115: loss 0.056770\n",
      "batch 3116: loss 0.075559\n",
      "batch 3117: loss 0.019764\n",
      "batch 3118: loss 0.048118\n",
      "batch 3119: loss 0.150343\n",
      "batch 3120: loss 0.005146\n",
      "batch 3121: loss 0.125943\n",
      "batch 3122: loss 0.074121\n",
      "batch 3123: loss 0.077855\n",
      "batch 3124: loss 0.089128\n",
      "batch 3125: loss 0.175243\n",
      "batch 3126: loss 0.040726\n",
      "batch 3127: loss 0.059753\n",
      "batch 3128: loss 0.045760\n",
      "batch 3129: loss 0.103126\n",
      "batch 3130: loss 0.043852\n",
      "batch 3131: loss 0.109908\n",
      "batch 3132: loss 0.270958\n",
      "batch 3133: loss 0.142520\n",
      "batch 3134: loss 0.095833\n",
      "batch 3135: loss 0.054855\n",
      "batch 3136: loss 0.068784\n",
      "batch 3137: loss 0.025546\n",
      "batch 3138: loss 0.101232\n",
      "batch 3139: loss 0.088275\n",
      "batch 3140: loss 0.109791\n",
      "batch 3141: loss 0.188482\n",
      "batch 3142: loss 0.082869\n",
      "batch 3143: loss 0.059137\n",
      "batch 3144: loss 0.046590\n",
      "batch 3145: loss 0.027823\n",
      "batch 3146: loss 0.069031\n",
      "batch 3147: loss 0.074371\n",
      "batch 3148: loss 0.080499\n",
      "batch 3149: loss 0.020261\n",
      "batch 3150: loss 0.107369\n",
      "batch 3151: loss 0.068644\n",
      "batch 3152: loss 0.032162\n",
      "batch 3153: loss 0.079489\n",
      "batch 3154: loss 0.062571\n",
      "batch 3155: loss 0.059636\n",
      "batch 3156: loss 0.502568\n",
      "batch 3157: loss 0.037602\n",
      "batch 3158: loss 0.045051\n",
      "batch 3159: loss 0.109920\n",
      "batch 3160: loss 0.046332\n",
      "batch 3161: loss 0.030423\n",
      "batch 3162: loss 0.033432\n",
      "batch 3163: loss 0.063122\n",
      "batch 3164: loss 0.049918\n",
      "batch 3165: loss 0.070734\n",
      "batch 3166: loss 0.029993\n",
      "batch 3167: loss 0.190971\n",
      "batch 3168: loss 0.062813\n",
      "batch 3169: loss 0.171357\n",
      "batch 3170: loss 0.044129\n",
      "batch 3171: loss 0.059598\n",
      "batch 3172: loss 0.217900\n",
      "batch 3173: loss 0.104879\n",
      "batch 3174: loss 0.029390\n",
      "batch 3175: loss 0.085186\n",
      "batch 3176: loss 0.105555\n",
      "batch 3177: loss 0.036041\n",
      "batch 3178: loss 0.040219\n",
      "batch 3179: loss 0.104594\n",
      "batch 3180: loss 0.042830\n",
      "batch 3181: loss 0.157685\n",
      "batch 3182: loss 0.113241\n",
      "batch 3183: loss 0.305159\n",
      "batch 3184: loss 0.053706\n",
      "batch 3185: loss 0.021366\n",
      "batch 3186: loss 0.156210\n",
      "batch 3187: loss 0.127821\n",
      "batch 3188: loss 0.026225\n",
      "batch 3189: loss 0.055294\n",
      "batch 3190: loss 0.062089\n",
      "batch 3191: loss 0.101307\n",
      "batch 3192: loss 0.038763\n",
      "batch 3193: loss 0.116341\n",
      "batch 3194: loss 0.154166\n",
      "batch 3195: loss 0.039645\n",
      "batch 3196: loss 0.043660\n",
      "batch 3197: loss 0.074282\n",
      "batch 3198: loss 0.053924\n",
      "batch 3199: loss 0.176693\n",
      "batch 3200: loss 0.111289\n",
      "batch 3201: loss 0.077378\n",
      "batch 3202: loss 0.088271\n",
      "batch 3203: loss 0.082963\n",
      "batch 3204: loss 0.070100\n",
      "batch 3205: loss 0.053299\n",
      "batch 3206: loss 0.106744\n",
      "batch 3207: loss 0.052305\n",
      "batch 3208: loss 0.195489\n",
      "batch 3209: loss 0.146912\n",
      "batch 3210: loss 0.037894\n",
      "batch 3211: loss 0.044121\n",
      "batch 3212: loss 0.081013\n",
      "batch 3213: loss 0.029347\n",
      "batch 3214: loss 0.060082\n",
      "batch 3215: loss 0.080867\n",
      "batch 3216: loss 0.069058\n",
      "batch 3217: loss 0.039656\n",
      "batch 3218: loss 0.033014\n",
      "batch 3219: loss 0.085667\n",
      "batch 3220: loss 0.018678\n",
      "batch 3221: loss 0.203516\n",
      "batch 3222: loss 0.060107\n",
      "batch 3223: loss 0.090207\n",
      "batch 3224: loss 0.037347\n",
      "batch 3225: loss 0.031008\n",
      "batch 3226: loss 0.015825\n",
      "batch 3227: loss 0.054841\n",
      "batch 3228: loss 0.165428\n",
      "batch 3229: loss 0.233130\n",
      "batch 3230: loss 0.144559\n",
      "batch 3231: loss 0.019172\n",
      "batch 3232: loss 0.074193\n",
      "batch 3233: loss 0.042215\n",
      "batch 3234: loss 0.043248\n",
      "batch 3235: loss 0.050872\n",
      "batch 3236: loss 0.087563\n",
      "batch 3237: loss 0.037756\n",
      "batch 3238: loss 0.244768\n",
      "batch 3239: loss 0.060863\n",
      "batch 3240: loss 0.042653\n",
      "batch 3241: loss 0.056018\n",
      "batch 3242: loss 0.062819\n",
      "batch 3243: loss 0.050217\n",
      "batch 3244: loss 0.370125\n",
      "batch 3245: loss 0.063517\n",
      "batch 3246: loss 0.066523\n",
      "batch 3247: loss 0.057552\n",
      "batch 3248: loss 0.031958\n",
      "batch 3249: loss 0.116302\n",
      "batch 3250: loss 0.063956\n",
      "batch 3251: loss 0.101161\n",
      "batch 3252: loss 0.073996\n",
      "batch 3253: loss 0.127111\n",
      "batch 3254: loss 0.040821\n",
      "batch 3255: loss 0.029222\n",
      "batch 3256: loss 0.035207\n",
      "batch 3257: loss 0.113901\n",
      "batch 3258: loss 0.016129\n",
      "batch 3259: loss 0.090825\n",
      "batch 3260: loss 0.039157\n",
      "batch 3261: loss 0.065872\n",
      "batch 3262: loss 0.116738\n",
      "batch 3263: loss 0.111446\n",
      "batch 3264: loss 0.060352\n",
      "batch 3265: loss 0.013941\n",
      "batch 3266: loss 0.053885\n",
      "batch 3267: loss 0.202908\n",
      "batch 3268: loss 0.186107\n",
      "batch 3269: loss 0.036571\n",
      "batch 3270: loss 0.024542\n",
      "batch 3271: loss 0.074392\n",
      "batch 3272: loss 0.027376\n",
      "batch 3273: loss 0.085281\n",
      "batch 3274: loss 0.043600\n",
      "batch 3275: loss 0.104137\n",
      "batch 3276: loss 0.042942\n",
      "batch 3277: loss 0.040094\n",
      "batch 3278: loss 0.110999\n",
      "batch 3279: loss 0.047492\n",
      "batch 3280: loss 0.161962\n",
      "batch 3281: loss 0.051785\n",
      "batch 3282: loss 0.038732\n",
      "batch 3283: loss 0.031410\n",
      "batch 3284: loss 0.098088\n",
      "batch 3285: loss 0.057005\n",
      "batch 3286: loss 0.119532\n",
      "batch 3287: loss 0.056972\n",
      "batch 3288: loss 0.119490\n",
      "batch 3289: loss 0.051866\n",
      "batch 3290: loss 0.111319\n",
      "batch 3291: loss 0.039299\n",
      "batch 3292: loss 0.021711\n",
      "batch 3293: loss 0.056339\n",
      "batch 3294: loss 0.034472\n",
      "batch 3295: loss 0.219281\n",
      "batch 3296: loss 0.076853\n",
      "batch 3297: loss 0.110755\n",
      "batch 3298: loss 0.034457\n",
      "batch 3299: loss 0.076526\n",
      "batch 3300: loss 0.050096\n",
      "batch 3301: loss 0.109776\n",
      "batch 3302: loss 0.015817\n",
      "batch 3303: loss 0.088075\n",
      "batch 3304: loss 0.114906\n",
      "batch 3305: loss 0.021068\n",
      "batch 3306: loss 0.203225\n",
      "batch 3307: loss 0.020295\n",
      "batch 3308: loss 0.032874\n",
      "batch 3309: loss 0.047287\n",
      "batch 3310: loss 0.136572\n",
      "batch 3311: loss 0.036795\n",
      "batch 3312: loss 0.100066\n",
      "batch 3313: loss 0.031612\n",
      "batch 3314: loss 0.023391\n",
      "batch 3315: loss 0.122810\n",
      "batch 3316: loss 0.152180\n",
      "batch 3317: loss 0.176169\n",
      "batch 3318: loss 0.060595\n",
      "batch 3319: loss 0.141391\n",
      "batch 3320: loss 0.046099\n",
      "batch 3321: loss 0.221357\n",
      "batch 3322: loss 0.074733\n",
      "batch 3323: loss 0.095745\n",
      "batch 3324: loss 0.128783\n",
      "batch 3325: loss 0.066950\n",
      "batch 3326: loss 0.161549\n",
      "batch 3327: loss 0.074822\n",
      "batch 3328: loss 0.143190\n",
      "batch 3329: loss 0.176401\n",
      "batch 3330: loss 0.041199\n",
      "batch 3331: loss 0.017181\n",
      "batch 3332: loss 0.170465\n",
      "batch 3333: loss 0.072364\n",
      "batch 3334: loss 0.163615\n",
      "batch 3335: loss 0.042739\n",
      "batch 3336: loss 0.272249\n",
      "batch 3337: loss 0.024701\n",
      "batch 3338: loss 0.154978\n",
      "batch 3339: loss 0.101033\n",
      "batch 3340: loss 0.106190\n",
      "batch 3341: loss 0.060757\n",
      "batch 3342: loss 0.146771\n",
      "batch 3343: loss 0.020967\n",
      "batch 3344: loss 0.143612\n",
      "batch 3345: loss 0.053488\n",
      "batch 3346: loss 0.134525\n",
      "batch 3347: loss 0.049071\n",
      "batch 3348: loss 0.075218\n",
      "batch 3349: loss 0.047353\n",
      "batch 3350: loss 0.018342\n",
      "batch 3351: loss 0.139713\n",
      "batch 3352: loss 0.103824\n",
      "batch 3353: loss 0.059613\n",
      "batch 3354: loss 0.125503\n",
      "batch 3355: loss 0.017844\n",
      "batch 3356: loss 0.024316\n",
      "batch 3357: loss 0.121815\n",
      "batch 3358: loss 0.072034\n",
      "batch 3359: loss 0.109846\n",
      "batch 3360: loss 0.051031\n",
      "batch 3361: loss 0.028423\n",
      "batch 3362: loss 0.142367\n",
      "batch 3363: loss 0.140191\n",
      "batch 3364: loss 0.152367\n",
      "batch 3365: loss 0.034750\n",
      "batch 3366: loss 0.044541\n",
      "batch 3367: loss 0.117375\n",
      "batch 3368: loss 0.151054\n",
      "batch 3369: loss 0.120475\n",
      "batch 3370: loss 0.040063\n",
      "batch 3371: loss 0.018909\n",
      "batch 3372: loss 0.137824\n",
      "batch 3373: loss 0.020128\n",
      "batch 3374: loss 0.011578\n",
      "batch 3375: loss 0.128783\n",
      "batch 3376: loss 0.051643\n",
      "batch 3377: loss 0.083017\n",
      "batch 3378: loss 0.140227\n",
      "batch 3379: loss 0.032677\n",
      "batch 3380: loss 0.097992\n",
      "batch 3381: loss 0.090952\n",
      "batch 3382: loss 0.278031\n",
      "batch 3383: loss 0.124014\n",
      "batch 3384: loss 0.078258\n",
      "batch 3385: loss 0.095800\n",
      "batch 3386: loss 0.100512\n",
      "batch 3387: loss 0.092374\n",
      "batch 3388: loss 0.057701\n",
      "batch 3389: loss 0.183892\n",
      "batch 3390: loss 0.061743\n",
      "batch 3391: loss 0.073240\n",
      "batch 3392: loss 0.020992\n",
      "batch 3393: loss 0.013692\n",
      "batch 3394: loss 0.137010\n",
      "batch 3395: loss 0.034840\n",
      "batch 3396: loss 0.055163\n",
      "batch 3397: loss 0.100114\n",
      "batch 3398: loss 0.088138\n",
      "batch 3399: loss 0.038316\n",
      "batch 3400: loss 0.057063\n",
      "batch 3401: loss 0.169846\n",
      "batch 3402: loss 0.081917\n",
      "batch 3403: loss 0.021479\n",
      "batch 3404: loss 0.090270\n",
      "batch 3405: loss 0.259443\n",
      "batch 3406: loss 0.036564\n",
      "batch 3407: loss 0.021509\n",
      "batch 3408: loss 0.083226\n",
      "batch 3409: loss 0.110985\n",
      "batch 3410: loss 0.140804\n",
      "batch 3411: loss 0.061868\n",
      "batch 3412: loss 0.050124\n",
      "batch 3413: loss 0.082248\n",
      "batch 3414: loss 0.011824\n",
      "batch 3415: loss 0.025220\n",
      "batch 3416: loss 0.084810\n",
      "batch 3417: loss 0.187642\n",
      "batch 3418: loss 0.061557\n",
      "batch 3419: loss 0.020964\n",
      "batch 3420: loss 0.013682\n",
      "batch 3421: loss 0.081682\n",
      "batch 3422: loss 0.094350\n",
      "batch 3423: loss 0.094047\n",
      "batch 3424: loss 0.038451\n",
      "batch 3425: loss 0.043528\n",
      "batch 3426: loss 0.088477\n",
      "batch 3427: loss 0.102698\n",
      "batch 3428: loss 0.028002\n",
      "batch 3429: loss 0.062616\n",
      "batch 3430: loss 0.054344\n",
      "batch 3431: loss 0.135426\n",
      "batch 3432: loss 0.124276\n",
      "batch 3433: loss 0.067334\n",
      "batch 3434: loss 0.065100\n",
      "batch 3435: loss 0.137664\n",
      "batch 3436: loss 0.021128\n",
      "batch 3437: loss 0.068202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3438: loss 0.092016\n",
      "batch 3439: loss 0.112044\n",
      "batch 3440: loss 0.060249\n",
      "batch 3441: loss 0.013812\n",
      "batch 3442: loss 0.028046\n",
      "batch 3443: loss 0.061894\n",
      "batch 3444: loss 0.116213\n",
      "batch 3445: loss 0.031280\n",
      "batch 3446: loss 0.025860\n",
      "batch 3447: loss 0.030440\n",
      "batch 3448: loss 0.079613\n",
      "batch 3449: loss 0.038523\n",
      "batch 3450: loss 0.103243\n",
      "batch 3451: loss 0.031568\n",
      "batch 3452: loss 0.181674\n",
      "batch 3453: loss 0.109747\n",
      "batch 3454: loss 0.115662\n",
      "batch 3455: loss 0.036017\n",
      "batch 3456: loss 0.049928\n",
      "batch 3457: loss 0.060049\n",
      "batch 3458: loss 0.062487\n",
      "batch 3459: loss 0.009641\n",
      "batch 3460: loss 0.051450\n",
      "batch 3461: loss 0.019149\n",
      "batch 3462: loss 0.020679\n",
      "batch 3463: loss 0.023090\n",
      "batch 3464: loss 0.023732\n",
      "batch 3465: loss 0.023922\n",
      "batch 3466: loss 0.111149\n",
      "batch 3467: loss 0.043209\n",
      "batch 3468: loss 0.132074\n",
      "batch 3469: loss 0.076362\n",
      "batch 3470: loss 0.088863\n",
      "batch 3471: loss 0.059402\n",
      "batch 3472: loss 0.197303\n",
      "batch 3473: loss 0.037307\n",
      "batch 3474: loss 0.091858\n",
      "batch 3475: loss 0.034278\n",
      "batch 3476: loss 0.037259\n",
      "batch 3477: loss 0.083633\n",
      "batch 3478: loss 0.028234\n",
      "batch 3479: loss 0.022444\n",
      "batch 3480: loss 0.044296\n",
      "batch 3481: loss 0.083854\n",
      "batch 3482: loss 0.012265\n",
      "batch 3483: loss 0.018552\n",
      "batch 3484: loss 0.168807\n",
      "batch 3485: loss 0.077829\n",
      "batch 3486: loss 0.064597\n",
      "batch 3487: loss 0.047351\n",
      "batch 3488: loss 0.099495\n",
      "batch 3489: loss 0.103966\n",
      "batch 3490: loss 0.155549\n",
      "batch 3491: loss 0.127721\n",
      "batch 3492: loss 0.078270\n",
      "batch 3493: loss 0.015526\n",
      "batch 3494: loss 0.030205\n",
      "batch 3495: loss 0.041149\n",
      "batch 3496: loss 0.031893\n",
      "batch 3497: loss 0.057938\n",
      "batch 3498: loss 0.056697\n",
      "batch 3499: loss 0.171008\n",
      "batch 3500: loss 0.063630\n",
      "batch 3501: loss 0.073533\n",
      "batch 3502: loss 0.082364\n",
      "batch 3503: loss 0.099052\n",
      "batch 3504: loss 0.095527\n",
      "batch 3505: loss 0.053245\n",
      "batch 3506: loss 0.022763\n",
      "batch 3507: loss 0.012716\n",
      "batch 3508: loss 0.069933\n",
      "batch 3509: loss 0.094905\n",
      "batch 3510: loss 0.176588\n",
      "batch 3511: loss 0.087167\n",
      "batch 3512: loss 0.081471\n",
      "batch 3513: loss 0.058271\n",
      "batch 3514: loss 0.048820\n",
      "batch 3515: loss 0.082965\n",
      "batch 3516: loss 0.130517\n",
      "batch 3517: loss 0.121354\n",
      "batch 3518: loss 0.048234\n",
      "batch 3519: loss 0.106045\n",
      "batch 3520: loss 0.058861\n",
      "batch 3521: loss 0.076688\n",
      "batch 3522: loss 0.078122\n",
      "batch 3523: loss 0.038931\n",
      "batch 3524: loss 0.048584\n",
      "batch 3525: loss 0.021576\n",
      "batch 3526: loss 0.021994\n",
      "batch 3527: loss 0.069630\n",
      "batch 3528: loss 0.107569\n",
      "batch 3529: loss 0.116904\n",
      "batch 3530: loss 0.156686\n",
      "batch 3531: loss 0.034218\n",
      "batch 3532: loss 0.178395\n",
      "batch 3533: loss 0.134835\n",
      "batch 3534: loss 0.087308\n",
      "batch 3535: loss 0.024980\n",
      "batch 3536: loss 0.072481\n",
      "batch 3537: loss 0.017209\n",
      "batch 3538: loss 0.020044\n",
      "batch 3539: loss 0.030417\n",
      "batch 3540: loss 0.134488\n",
      "batch 3541: loss 0.099507\n",
      "batch 3542: loss 0.094519\n",
      "batch 3543: loss 0.042145\n",
      "batch 3544: loss 0.045488\n",
      "batch 3545: loss 0.080717\n",
      "batch 3546: loss 0.051734\n",
      "batch 3547: loss 0.051228\n",
      "batch 3548: loss 0.065478\n",
      "batch 3549: loss 0.030105\n",
      "batch 3550: loss 0.027089\n",
      "batch 3551: loss 0.062049\n",
      "batch 3552: loss 0.073478\n",
      "batch 3553: loss 0.019278\n",
      "batch 3554: loss 0.032219\n",
      "batch 3555: loss 0.032247\n",
      "batch 3556: loss 0.029632\n",
      "batch 3557: loss 0.036612\n",
      "batch 3558: loss 0.059832\n",
      "batch 3559: loss 0.158721\n",
      "batch 3560: loss 0.080259\n",
      "batch 3561: loss 0.041515\n",
      "batch 3562: loss 0.084306\n",
      "batch 3563: loss 0.095400\n",
      "batch 3564: loss 0.053944\n",
      "batch 3565: loss 0.011030\n",
      "batch 3566: loss 0.019198\n",
      "batch 3567: loss 0.155426\n",
      "batch 3568: loss 0.023418\n",
      "batch 3569: loss 0.060320\n",
      "batch 3570: loss 0.142833\n",
      "batch 3571: loss 0.095312\n",
      "batch 3572: loss 0.082633\n",
      "batch 3573: loss 0.119232\n",
      "batch 3574: loss 0.092573\n",
      "batch 3575: loss 0.055770\n",
      "batch 3576: loss 0.042386\n",
      "batch 3577: loss 0.029666\n",
      "batch 3578: loss 0.051269\n",
      "batch 3579: loss 0.249811\n",
      "batch 3580: loss 0.021183\n",
      "batch 3581: loss 0.036117\n",
      "batch 3582: loss 0.077950\n",
      "batch 3583: loss 0.043013\n",
      "batch 3584: loss 0.071364\n",
      "batch 3585: loss 0.047519\n",
      "batch 3586: loss 0.134081\n",
      "batch 3587: loss 0.081536\n",
      "batch 3588: loss 0.049210\n",
      "batch 3589: loss 0.092545\n",
      "batch 3590: loss 0.037340\n",
      "batch 3591: loss 0.114583\n",
      "batch 3592: loss 0.103329\n",
      "batch 3593: loss 0.055575\n",
      "batch 3594: loss 0.026652\n",
      "batch 3595: loss 0.079857\n",
      "batch 3596: loss 0.052070\n",
      "batch 3597: loss 0.128295\n",
      "batch 3598: loss 0.077814\n",
      "batch 3599: loss 0.167512\n",
      "batch 3600: loss 0.051967\n",
      "batch 3601: loss 0.035551\n",
      "batch 3602: loss 0.035535\n",
      "batch 3603: loss 0.065615\n",
      "batch 3604: loss 0.011529\n",
      "batch 3605: loss 0.091854\n",
      "batch 3606: loss 0.040493\n",
      "batch 3607: loss 0.024961\n",
      "batch 3608: loss 0.101216\n",
      "batch 3609: loss 0.034996\n",
      "batch 3610: loss 0.079509\n",
      "batch 3611: loss 0.085440\n",
      "batch 3612: loss 0.070647\n",
      "batch 3613: loss 0.023525\n",
      "batch 3614: loss 0.036550\n",
      "batch 3615: loss 0.106761\n",
      "batch 3616: loss 0.018349\n",
      "batch 3617: loss 0.046003\n",
      "batch 3618: loss 0.016733\n",
      "batch 3619: loss 0.102107\n",
      "batch 3620: loss 0.056866\n",
      "batch 3621: loss 0.018661\n",
      "batch 3622: loss 0.039969\n",
      "batch 3623: loss 0.182750\n",
      "batch 3624: loss 0.047687\n",
      "batch 3625: loss 0.034916\n",
      "batch 3626: loss 0.040603\n",
      "batch 3627: loss 0.010150\n",
      "batch 3628: loss 0.046064\n",
      "batch 3629: loss 0.014627\n",
      "batch 3630: loss 0.031563\n",
      "batch 3631: loss 0.016722\n",
      "batch 3632: loss 0.068578\n",
      "batch 3633: loss 0.103152\n",
      "batch 3634: loss 0.011682\n",
      "batch 3635: loss 0.036745\n",
      "batch 3636: loss 0.120704\n",
      "batch 3637: loss 0.094286\n",
      "batch 3638: loss 0.023179\n",
      "batch 3639: loss 0.041857\n",
      "batch 3640: loss 0.037849\n",
      "batch 3641: loss 0.080807\n",
      "batch 3642: loss 0.078676\n",
      "batch 3643: loss 0.010205\n",
      "batch 3644: loss 0.045399\n",
      "batch 3645: loss 0.052221\n",
      "batch 3646: loss 0.120448\n",
      "batch 3647: loss 0.045875\n",
      "batch 3648: loss 0.027616\n",
      "batch 3649: loss 0.070609\n",
      "batch 3650: loss 0.004528\n",
      "batch 3651: loss 0.116339\n",
      "batch 3652: loss 0.042710\n",
      "batch 3653: loss 0.065374\n",
      "batch 3654: loss 0.031520\n",
      "batch 3655: loss 0.025853\n",
      "batch 3656: loss 0.025318\n",
      "batch 3657: loss 0.391131\n",
      "batch 3658: loss 0.189432\n",
      "batch 3659: loss 0.035124\n",
      "batch 3660: loss 0.056951\n",
      "batch 3661: loss 0.030116\n",
      "batch 3662: loss 0.046770\n",
      "batch 3663: loss 0.116654\n",
      "batch 3664: loss 0.155400\n",
      "batch 3665: loss 0.184311\n",
      "batch 3666: loss 0.054869\n",
      "batch 3667: loss 0.015868\n",
      "batch 3668: loss 0.107317\n",
      "batch 3669: loss 0.056520\n",
      "batch 3670: loss 0.101701\n",
      "batch 3671: loss 0.307737\n",
      "batch 3672: loss 0.021245\n",
      "batch 3673: loss 0.133533\n",
      "batch 3674: loss 0.131654\n",
      "batch 3675: loss 0.032043\n",
      "batch 3676: loss 0.192302\n",
      "batch 3677: loss 0.046509\n",
      "batch 3678: loss 0.192633\n",
      "batch 3679: loss 0.024208\n",
      "batch 3680: loss 0.063717\n",
      "batch 3681: loss 0.019198\n",
      "batch 3682: loss 0.198819\n",
      "batch 3683: loss 0.013766\n",
      "batch 3684: loss 0.100335\n",
      "batch 3685: loss 0.063020\n",
      "batch 3686: loss 0.057076\n",
      "batch 3687: loss 0.097789\n",
      "batch 3688: loss 0.140345\n",
      "batch 3689: loss 0.052572\n",
      "batch 3690: loss 0.121367\n",
      "batch 3691: loss 0.092431\n",
      "batch 3692: loss 0.023537\n",
      "batch 3693: loss 0.176135\n",
      "batch 3694: loss 0.147766\n",
      "batch 3695: loss 0.079535\n",
      "batch 3696: loss 0.102406\n",
      "batch 3697: loss 0.141880\n",
      "batch 3698: loss 0.077324\n",
      "batch 3699: loss 0.086686\n",
      "batch 3700: loss 0.023016\n",
      "batch 3701: loss 0.052856\n",
      "batch 3702: loss 0.048649\n",
      "batch 3703: loss 0.219763\n",
      "batch 3704: loss 0.036015\n",
      "batch 3705: loss 0.085513\n",
      "batch 3706: loss 0.046954\n",
      "batch 3707: loss 0.100452\n",
      "batch 3708: loss 0.091637\n",
      "batch 3709: loss 0.064258\n",
      "batch 3710: loss 0.144166\n",
      "batch 3711: loss 0.058544\n",
      "batch 3712: loss 0.148129\n",
      "batch 3713: loss 0.084809\n",
      "batch 3714: loss 0.015374\n",
      "batch 3715: loss 0.033503\n",
      "batch 3716: loss 0.017569\n",
      "batch 3717: loss 0.224048\n",
      "batch 3718: loss 0.179674\n",
      "batch 3719: loss 0.133561\n",
      "batch 3720: loss 0.112529\n",
      "batch 3721: loss 0.234288\n",
      "batch 3722: loss 0.033518\n",
      "batch 3723: loss 0.050798\n",
      "batch 3724: loss 0.046536\n",
      "batch 3725: loss 0.061803\n",
      "batch 3726: loss 0.040031\n",
      "batch 3727: loss 0.057889\n",
      "batch 3728: loss 0.021021\n",
      "batch 3729: loss 0.168457\n",
      "batch 3730: loss 0.052710\n",
      "batch 3731: loss 0.024390\n",
      "batch 3732: loss 0.095485\n",
      "batch 3733: loss 0.080406\n",
      "batch 3734: loss 0.039930\n",
      "batch 3735: loss 0.062725\n",
      "batch 3736: loss 0.065419\n",
      "batch 3737: loss 0.069023\n",
      "batch 3738: loss 0.057975\n",
      "batch 3739: loss 0.118315\n",
      "batch 3740: loss 0.051329\n",
      "batch 3741: loss 0.110582\n",
      "batch 3742: loss 0.114076\n",
      "batch 3743: loss 0.091396\n",
      "batch 3744: loss 0.035167\n",
      "batch 3745: loss 0.033326\n",
      "batch 3746: loss 0.038713\n",
      "batch 3747: loss 0.060795\n",
      "batch 3748: loss 0.013344\n",
      "batch 3749: loss 0.018411\n",
      "batch 3750: loss 0.028674\n",
      "batch 3751: loss 0.296297\n",
      "batch 3752: loss 0.106214\n",
      "batch 3753: loss 0.057849\n",
      "batch 3754: loss 0.061492\n",
      "batch 3755: loss 0.092109\n",
      "batch 3756: loss 0.050846\n",
      "batch 3757: loss 0.060832\n",
      "batch 3758: loss 0.086387\n",
      "batch 3759: loss 0.097680\n",
      "batch 3760: loss 0.045707\n",
      "batch 3761: loss 0.015200\n",
      "batch 3762: loss 0.013185\n",
      "batch 3763: loss 0.071369\n",
      "batch 3764: loss 0.023138\n",
      "batch 3765: loss 0.083652\n",
      "batch 3766: loss 0.037856\n",
      "batch 3767: loss 0.027982\n",
      "batch 3768: loss 0.066252\n",
      "batch 3769: loss 0.204937\n",
      "batch 3770: loss 0.047644\n",
      "batch 3771: loss 0.056393\n",
      "batch 3772: loss 0.014347\n",
      "batch 3773: loss 0.074154\n",
      "batch 3774: loss 0.017542\n",
      "batch 3775: loss 0.063054\n",
      "batch 3776: loss 0.064505\n",
      "batch 3777: loss 0.112808\n",
      "batch 3778: loss 0.036569\n",
      "batch 3779: loss 0.110636\n",
      "batch 3780: loss 0.054941\n",
      "batch 3781: loss 0.015628\n",
      "batch 3782: loss 0.020439\n",
      "batch 3783: loss 0.070476\n",
      "batch 3784: loss 0.077249\n",
      "batch 3785: loss 0.051919\n",
      "batch 3786: loss 0.069005\n",
      "batch 3787: loss 0.015591\n",
      "batch 3788: loss 0.061555\n",
      "batch 3789: loss 0.159402\n",
      "batch 3790: loss 0.055828\n",
      "batch 3791: loss 0.076263\n",
      "batch 3792: loss 0.028643\n",
      "batch 3793: loss 0.027460\n",
      "batch 3794: loss 0.173389\n",
      "batch 3795: loss 0.007774\n",
      "batch 3796: loss 0.034442\n",
      "batch 3797: loss 0.027035\n",
      "batch 3798: loss 0.042113\n",
      "batch 3799: loss 0.005829\n",
      "batch 3800: loss 0.068186\n",
      "batch 3801: loss 0.014880\n",
      "batch 3802: loss 0.072718\n",
      "batch 3803: loss 0.092752\n",
      "batch 3804: loss 0.228692\n",
      "batch 3805: loss 0.047152\n",
      "batch 3806: loss 0.066375\n",
      "batch 3807: loss 0.030821\n",
      "batch 3808: loss 0.014735\n",
      "batch 3809: loss 0.063526\n",
      "batch 3810: loss 0.031677\n",
      "batch 3811: loss 0.013256\n",
      "batch 3812: loss 0.068352\n",
      "batch 3813: loss 0.038887\n",
      "batch 3814: loss 0.196807\n",
      "batch 3815: loss 0.034512\n",
      "batch 3816: loss 0.029471\n",
      "batch 3817: loss 0.046305\n",
      "batch 3818: loss 0.030907\n",
      "batch 3819: loss 0.021188\n",
      "batch 3820: loss 0.088778\n",
      "batch 3821: loss 0.203685\n",
      "batch 3822: loss 0.041236\n",
      "batch 3823: loss 0.026203\n",
      "batch 3824: loss 0.158816\n",
      "batch 3825: loss 0.199981\n",
      "batch 3826: loss 0.032334\n",
      "batch 3827: loss 0.020799\n",
      "batch 3828: loss 0.053606\n",
      "batch 3829: loss 0.035456\n",
      "batch 3830: loss 0.055847\n",
      "batch 3831: loss 0.016426\n",
      "batch 3832: loss 0.096972\n",
      "batch 3833: loss 0.038962\n",
      "batch 3834: loss 0.120077\n",
      "batch 3835: loss 0.021702\n",
      "batch 3836: loss 0.074728\n",
      "batch 3837: loss 0.026168\n",
      "batch 3838: loss 0.182432\n",
      "batch 3839: loss 0.038519\n",
      "batch 3840: loss 0.062538\n",
      "batch 3841: loss 0.157883\n",
      "batch 3842: loss 0.033726\n",
      "batch 3843: loss 0.066776\n",
      "batch 3844: loss 0.038798\n",
      "batch 3845: loss 0.059210\n",
      "batch 3846: loss 0.032418\n",
      "batch 3847: loss 0.194354\n",
      "batch 3848: loss 0.097351\n",
      "batch 3849: loss 0.122897\n",
      "batch 3850: loss 0.026012\n",
      "batch 3851: loss 0.041703\n",
      "batch 3852: loss 0.041306\n",
      "batch 3853: loss 0.027432\n",
      "batch 3854: loss 0.042000\n",
      "batch 3855: loss 0.072787\n",
      "batch 3856: loss 0.116470\n",
      "batch 3857: loss 0.010604\n",
      "batch 3858: loss 0.015586\n",
      "batch 3859: loss 0.054373\n",
      "batch 3860: loss 0.054416\n",
      "batch 3861: loss 0.017257\n",
      "batch 3862: loss 0.058973\n",
      "batch 3863: loss 0.009014\n",
      "batch 3864: loss 0.023165\n",
      "batch 3865: loss 0.032559\n",
      "batch 3866: loss 0.063252\n",
      "batch 3867: loss 0.037259\n",
      "batch 3868: loss 0.092984\n",
      "batch 3869: loss 0.015955\n",
      "batch 3870: loss 0.054408\n",
      "batch 3871: loss 0.100564\n",
      "batch 3872: loss 0.149942\n",
      "batch 3873: loss 0.106336\n",
      "batch 3874: loss 0.014562\n",
      "batch 3875: loss 0.032674\n",
      "batch 3876: loss 0.013575\n",
      "batch 3877: loss 0.260647\n",
      "batch 3878: loss 0.094663\n",
      "batch 3879: loss 0.100179\n",
      "batch 3880: loss 0.062314\n",
      "batch 3881: loss 0.048895\n",
      "batch 3882: loss 0.030910\n",
      "batch 3883: loss 0.050420\n",
      "batch 3884: loss 0.059321\n",
      "batch 3885: loss 0.079213\n",
      "batch 3886: loss 0.103944\n",
      "batch 3887: loss 0.023016\n",
      "batch 3888: loss 0.153317\n",
      "batch 3889: loss 0.016898\n",
      "batch 3890: loss 0.035671\n",
      "batch 3891: loss 0.113251\n",
      "batch 3892: loss 0.069046\n",
      "batch 3893: loss 0.086335\n",
      "batch 3894: loss 0.012648\n",
      "batch 3895: loss 0.071627\n",
      "batch 3896: loss 0.134464\n",
      "batch 3897: loss 0.035668\n",
      "batch 3898: loss 0.095479\n",
      "batch 3899: loss 0.017193\n",
      "batch 3900: loss 0.028135\n",
      "batch 3901: loss 0.134565\n",
      "batch 3902: loss 0.291235\n",
      "batch 3903: loss 0.077040\n",
      "batch 3904: loss 0.088554\n",
      "batch 3905: loss 0.064291\n",
      "batch 3906: loss 0.188308\n",
      "batch 3907: loss 0.151504\n",
      "batch 3908: loss 0.116828\n",
      "batch 3909: loss 0.017942\n",
      "batch 3910: loss 0.059701\n",
      "batch 3911: loss 0.098538\n",
      "batch 3912: loss 0.209401\n",
      "batch 3913: loss 0.064604\n",
      "batch 3914: loss 0.097436\n",
      "batch 3915: loss 0.097840\n",
      "batch 3916: loss 0.361281\n",
      "batch 3917: loss 0.054502\n",
      "batch 3918: loss 0.045710\n",
      "batch 3919: loss 0.053142\n",
      "batch 3920: loss 0.082073\n",
      "batch 3921: loss 0.030606\n",
      "batch 3922: loss 0.169880\n",
      "batch 3923: loss 0.054858\n",
      "batch 3924: loss 0.077736\n",
      "batch 3925: loss 0.176346\n",
      "batch 3926: loss 0.054164\n",
      "batch 3927: loss 0.051525\n",
      "batch 3928: loss 0.036472\n",
      "batch 3929: loss 0.180518\n",
      "batch 3930: loss 0.091501\n",
      "batch 3931: loss 0.032421\n",
      "batch 3932: loss 0.073441\n",
      "batch 3933: loss 0.063814\n",
      "batch 3934: loss 0.140918\n",
      "batch 3935: loss 0.118283\n",
      "batch 3936: loss 0.042769\n",
      "batch 3937: loss 0.068433\n",
      "batch 3938: loss 0.046603\n",
      "batch 3939: loss 0.027519\n",
      "batch 3940: loss 0.064276\n",
      "batch 3941: loss 0.136042\n",
      "batch 3942: loss 0.111878\n",
      "batch 3943: loss 0.109309\n",
      "batch 3944: loss 0.155644\n",
      "batch 3945: loss 0.065380\n",
      "batch 3946: loss 0.033320\n",
      "batch 3947: loss 0.016136\n",
      "batch 3948: loss 0.095545\n",
      "batch 3949: loss 0.067248\n",
      "batch 3950: loss 0.021519\n",
      "batch 3951: loss 0.030753\n",
      "batch 3952: loss 0.065400\n",
      "batch 3953: loss 0.072371\n",
      "batch 3954: loss 0.046388\n",
      "batch 3955: loss 0.165927\n",
      "batch 3956: loss 0.053092\n",
      "batch 3957: loss 0.139950\n",
      "batch 3958: loss 0.036829\n",
      "batch 3959: loss 0.036878\n",
      "batch 3960: loss 0.032762\n",
      "batch 3961: loss 0.045846\n",
      "batch 3962: loss 0.031681\n",
      "batch 3963: loss 0.028574\n",
      "batch 3964: loss 0.033293\n",
      "batch 3965: loss 0.030878\n",
      "batch 3966: loss 0.229875\n",
      "batch 3967: loss 0.038787\n",
      "batch 3968: loss 0.037257\n",
      "batch 3969: loss 0.014998\n",
      "batch 3970: loss 0.035828\n",
      "batch 3971: loss 0.152213\n",
      "batch 3972: loss 0.035395\n",
      "batch 3973: loss 0.217392\n",
      "batch 3974: loss 0.153049\n",
      "batch 3975: loss 0.083217\n",
      "batch 3976: loss 0.046567\n",
      "batch 3977: loss 0.031866\n",
      "batch 3978: loss 0.136302\n",
      "batch 3979: loss 0.108439\n",
      "batch 3980: loss 0.038171\n",
      "batch 3981: loss 0.051728\n",
      "batch 3982: loss 0.100875\n",
      "batch 3983: loss 0.115825\n",
      "batch 3984: loss 0.170585\n",
      "batch 3985: loss 0.129787\n",
      "batch 3986: loss 0.073581\n",
      "batch 3987: loss 0.072598\n",
      "batch 3988: loss 0.105815\n",
      "batch 3989: loss 0.132158\n",
      "batch 3990: loss 0.122804\n",
      "batch 3991: loss 0.041777\n",
      "batch 3992: loss 0.178098\n",
      "batch 3993: loss 0.146127\n",
      "batch 3994: loss 0.051193\n",
      "batch 3995: loss 0.043663\n",
      "batch 3996: loss 0.056926\n",
      "batch 3997: loss 0.062685\n",
      "batch 3998: loss 0.093591\n",
      "batch 3999: loss 0.055781\n",
      "batch 4000: loss 0.058809\n",
      "batch 4001: loss 0.091620\n",
      "batch 4002: loss 0.198072\n",
      "batch 4003: loss 0.055815\n",
      "batch 4004: loss 0.098221\n",
      "batch 4005: loss 0.240458\n",
      "batch 4006: loss 0.076798\n",
      "batch 4007: loss 0.022236\n",
      "batch 4008: loss 0.054326\n",
      "batch 4009: loss 0.156122\n",
      "batch 4010: loss 0.051779\n",
      "batch 4011: loss 0.123513\n",
      "batch 4012: loss 0.029779\n",
      "batch 4013: loss 0.013097\n",
      "batch 4014: loss 0.027069\n",
      "batch 4015: loss 0.320374\n",
      "batch 4016: loss 0.086372\n",
      "batch 4017: loss 0.118075\n",
      "batch 4018: loss 0.009257\n",
      "batch 4019: loss 0.035748\n",
      "batch 4020: loss 0.026054\n",
      "batch 4021: loss 0.075988\n",
      "batch 4022: loss 0.028101\n",
      "batch 4023: loss 0.029018\n",
      "batch 4024: loss 0.069174\n",
      "batch 4025: loss 0.123319\n",
      "batch 4026: loss 0.036224\n",
      "batch 4027: loss 0.139697\n",
      "batch 4028: loss 0.041050\n",
      "batch 4029: loss 0.045081\n",
      "batch 4030: loss 0.005525\n",
      "batch 4031: loss 0.090947\n",
      "batch 4032: loss 0.036209\n",
      "batch 4033: loss 0.225227\n",
      "batch 4034: loss 0.053816\n",
      "batch 4035: loss 0.088859\n",
      "batch 4036: loss 0.056210\n",
      "batch 4037: loss 0.032018\n",
      "batch 4038: loss 0.089141\n",
      "batch 4039: loss 0.160511\n",
      "batch 4040: loss 0.041042\n",
      "batch 4041: loss 0.035906\n",
      "batch 4042: loss 0.190289\n",
      "batch 4043: loss 0.107592\n",
      "batch 4044: loss 0.040748\n",
      "batch 4045: loss 0.037139\n",
      "batch 4046: loss 0.136338\n",
      "batch 4047: loss 0.031020\n",
      "batch 4048: loss 0.080340\n",
      "batch 4049: loss 0.014170\n",
      "batch 4050: loss 0.152872\n",
      "batch 4051: loss 0.060344\n",
      "batch 4052: loss 0.113373\n",
      "batch 4053: loss 0.093015\n",
      "batch 4054: loss 0.082458\n",
      "batch 4055: loss 0.059802\n",
      "batch 4056: loss 0.065231\n",
      "batch 4057: loss 0.027682\n",
      "batch 4058: loss 0.154408\n",
      "batch 4059: loss 0.019031\n",
      "batch 4060: loss 0.042625\n",
      "batch 4061: loss 0.079265\n",
      "batch 4062: loss 0.065746\n",
      "batch 4063: loss 0.069505\n",
      "batch 4064: loss 0.063125\n",
      "batch 4065: loss 0.057875\n",
      "batch 4066: loss 0.057368\n",
      "batch 4067: loss 0.229753\n",
      "batch 4068: loss 0.071390\n",
      "batch 4069: loss 0.054382\n",
      "batch 4070: loss 0.018245\n",
      "batch 4071: loss 0.057879\n",
      "batch 4072: loss 0.112429\n",
      "batch 4073: loss 0.096025\n",
      "batch 4074: loss 0.027988\n",
      "batch 4075: loss 0.051461\n",
      "batch 4076: loss 0.032215\n",
      "batch 4077: loss 0.026511\n",
      "batch 4078: loss 0.014076\n",
      "batch 4079: loss 0.061989\n",
      "batch 4080: loss 0.094685\n",
      "batch 4081: loss 0.101108\n",
      "batch 4082: loss 0.044029\n",
      "batch 4083: loss 0.167119\n",
      "batch 4084: loss 0.089355\n",
      "batch 4085: loss 0.058887\n",
      "batch 4086: loss 0.032072\n",
      "batch 4087: loss 0.019368\n",
      "batch 4088: loss 0.035612\n",
      "batch 4089: loss 0.008733\n",
      "batch 4090: loss 0.092976\n",
      "batch 4091: loss 0.052170\n",
      "batch 4092: loss 0.026462\n",
      "batch 4093: loss 0.039342\n",
      "batch 4094: loss 0.007538\n",
      "batch 4095: loss 0.057350\n",
      "batch 4096: loss 0.053867\n",
      "batch 4097: loss 0.067448\n",
      "batch 4098: loss 0.025316\n",
      "batch 4099: loss 0.025646\n",
      "batch 4100: loss 0.063579\n",
      "batch 4101: loss 0.046000\n",
      "batch 4102: loss 0.159453\n",
      "batch 4103: loss 0.049803\n",
      "batch 4104: loss 0.019741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4105: loss 0.117547\n",
      "batch 4106: loss 0.107104\n",
      "batch 4107: loss 0.128382\n",
      "batch 4108: loss 0.009036\n",
      "batch 4109: loss 0.181404\n",
      "batch 4110: loss 0.052840\n",
      "batch 4111: loss 0.079171\n",
      "batch 4112: loss 0.087171\n",
      "batch 4113: loss 0.029150\n",
      "batch 4114: loss 0.010346\n",
      "batch 4115: loss 0.052717\n",
      "batch 4116: loss 0.122202\n",
      "batch 4117: loss 0.018406\n",
      "batch 4118: loss 0.099968\n",
      "batch 4119: loss 0.128224\n",
      "batch 4120: loss 0.292793\n",
      "batch 4121: loss 0.014528\n",
      "batch 4122: loss 0.039510\n",
      "batch 4123: loss 0.103933\n",
      "batch 4124: loss 0.051691\n",
      "batch 4125: loss 0.013796\n",
      "batch 4126: loss 0.111367\n",
      "batch 4127: loss 0.032550\n",
      "batch 4128: loss 0.046131\n",
      "batch 4129: loss 0.039387\n",
      "batch 4130: loss 0.075228\n",
      "batch 4131: loss 0.025649\n",
      "batch 4132: loss 0.120380\n",
      "batch 4133: loss 0.141569\n",
      "batch 4134: loss 0.015871\n",
      "batch 4135: loss 0.015195\n",
      "batch 4136: loss 0.033848\n",
      "batch 4137: loss 0.076257\n",
      "batch 4138: loss 0.077486\n",
      "batch 4139: loss 0.024437\n",
      "batch 4140: loss 0.069400\n",
      "batch 4141: loss 0.014867\n",
      "batch 4142: loss 0.014484\n",
      "batch 4143: loss 0.030332\n",
      "batch 4144: loss 0.085326\n",
      "batch 4145: loss 0.103084\n",
      "batch 4146: loss 0.241913\n",
      "batch 4147: loss 0.046190\n",
      "batch 4148: loss 0.044283\n",
      "batch 4149: loss 0.117619\n",
      "batch 4150: loss 0.020478\n",
      "batch 4151: loss 0.105840\n",
      "batch 4152: loss 0.038528\n",
      "batch 4153: loss 0.060697\n",
      "batch 4154: loss 0.085040\n",
      "batch 4155: loss 0.050152\n",
      "batch 4156: loss 0.011864\n",
      "batch 4157: loss 0.038305\n",
      "batch 4158: loss 0.028437\n",
      "batch 4159: loss 0.029149\n",
      "batch 4160: loss 0.034922\n",
      "batch 4161: loss 0.065911\n",
      "batch 4162: loss 0.078739\n",
      "batch 4163: loss 0.060644\n",
      "batch 4164: loss 0.020436\n",
      "batch 4165: loss 0.151999\n",
      "batch 4166: loss 0.011511\n",
      "batch 4167: loss 0.110602\n",
      "batch 4168: loss 0.028901\n",
      "batch 4169: loss 0.053512\n",
      "batch 4170: loss 0.027916\n",
      "batch 4171: loss 0.045910\n",
      "batch 4172: loss 0.050560\n",
      "batch 4173: loss 0.056504\n",
      "batch 4174: loss 0.023256\n",
      "batch 4175: loss 0.029413\n",
      "batch 4176: loss 0.105599\n",
      "batch 4177: loss 0.118810\n",
      "batch 4178: loss 0.033942\n",
      "batch 4179: loss 0.040105\n",
      "batch 4180: loss 0.059926\n",
      "batch 4181: loss 0.008274\n",
      "batch 4182: loss 0.024810\n",
      "batch 4183: loss 0.113731\n",
      "batch 4184: loss 0.047582\n",
      "batch 4185: loss 0.034830\n",
      "batch 4186: loss 0.045881\n",
      "batch 4187: loss 0.036429\n",
      "batch 4188: loss 0.012248\n",
      "batch 4189: loss 0.066084\n",
      "batch 4190: loss 0.045134\n",
      "batch 4191: loss 0.374265\n",
      "batch 4192: loss 0.054954\n",
      "batch 4193: loss 0.068806\n",
      "batch 4194: loss 0.031734\n",
      "batch 4195: loss 0.106439\n",
      "batch 4196: loss 0.070709\n",
      "batch 4197: loss 0.059874\n",
      "batch 4198: loss 0.267314\n",
      "batch 4199: loss 0.072863\n",
      "batch 4200: loss 0.066854\n",
      "batch 4201: loss 0.025316\n",
      "batch 4202: loss 0.025879\n",
      "batch 4203: loss 0.037276\n",
      "batch 4204: loss 0.022540\n",
      "batch 4205: loss 0.048459\n",
      "batch 4206: loss 0.119882\n",
      "batch 4207: loss 0.036779\n",
      "batch 4208: loss 0.083171\n",
      "batch 4209: loss 0.047609\n",
      "batch 4210: loss 0.297532\n",
      "batch 4211: loss 0.077920\n",
      "batch 4212: loss 0.270326\n",
      "batch 4213: loss 0.122062\n",
      "batch 4214: loss 0.069095\n",
      "batch 4215: loss 0.047186\n",
      "batch 4216: loss 0.109373\n",
      "batch 4217: loss 0.081050\n",
      "batch 4218: loss 0.023825\n",
      "batch 4219: loss 0.144664\n",
      "batch 4220: loss 0.168865\n",
      "batch 4221: loss 0.044076\n",
      "batch 4222: loss 0.034164\n",
      "batch 4223: loss 0.019639\n",
      "batch 4224: loss 0.059073\n",
      "batch 4225: loss 0.026963\n",
      "batch 4226: loss 0.042647\n",
      "batch 4227: loss 0.061611\n",
      "batch 4228: loss 0.036774\n",
      "batch 4229: loss 0.048108\n",
      "batch 4230: loss 0.210591\n",
      "batch 4231: loss 0.037099\n",
      "batch 4232: loss 0.026696\n",
      "batch 4233: loss 0.088919\n",
      "batch 4234: loss 0.044760\n",
      "batch 4235: loss 0.044480\n",
      "batch 4236: loss 0.034250\n",
      "batch 4237: loss 0.010147\n",
      "batch 4238: loss 0.015827\n",
      "batch 4239: loss 0.112287\n",
      "batch 4240: loss 0.049942\n",
      "batch 4241: loss 0.157823\n",
      "batch 4242: loss 0.018081\n",
      "batch 4243: loss 0.011228\n",
      "batch 4244: loss 0.072078\n",
      "batch 4245: loss 0.004116\n",
      "batch 4246: loss 0.051155\n",
      "batch 4247: loss 0.121518\n",
      "batch 4248: loss 0.096442\n",
      "batch 4249: loss 0.061794\n",
      "batch 4250: loss 0.055136\n",
      "batch 4251: loss 0.020712\n",
      "batch 4252: loss 0.090516\n",
      "batch 4253: loss 0.087016\n",
      "batch 4254: loss 0.052145\n",
      "batch 4255: loss 0.135218\n",
      "batch 4256: loss 0.021647\n",
      "batch 4257: loss 0.087895\n",
      "batch 4258: loss 0.075228\n",
      "batch 4259: loss 0.076192\n",
      "batch 4260: loss 0.071143\n",
      "batch 4261: loss 0.016543\n",
      "batch 4262: loss 0.052904\n",
      "batch 4263: loss 0.099335\n",
      "batch 4264: loss 0.061716\n",
      "batch 4265: loss 0.071841\n",
      "batch 4266: loss 0.022957\n",
      "batch 4267: loss 0.073954\n",
      "batch 4268: loss 0.076155\n",
      "batch 4269: loss 0.143021\n",
      "batch 4270: loss 0.054542\n",
      "batch 4271: loss 0.070662\n",
      "batch 4272: loss 0.071019\n",
      "batch 4273: loss 0.067647\n",
      "batch 4274: loss 0.080960\n",
      "batch 4275: loss 0.032246\n",
      "batch 4276: loss 0.165509\n",
      "batch 4277: loss 0.045300\n",
      "batch 4278: loss 0.180499\n",
      "batch 4279: loss 0.071729\n",
      "batch 4280: loss 0.037281\n",
      "batch 4281: loss 0.059526\n",
      "batch 4282: loss 0.016754\n",
      "batch 4283: loss 0.014614\n",
      "batch 4284: loss 0.054952\n",
      "batch 4285: loss 0.035420\n",
      "batch 4286: loss 0.080903\n",
      "batch 4287: loss 0.149909\n",
      "batch 4288: loss 0.028943\n",
      "batch 4289: loss 0.092468\n",
      "batch 4290: loss 0.098328\n",
      "batch 4291: loss 0.023732\n",
      "batch 4292: loss 0.041733\n",
      "batch 4293: loss 0.034235\n",
      "batch 4294: loss 0.206022\n",
      "batch 4295: loss 0.022927\n",
      "batch 4296: loss 0.092838\n",
      "batch 4297: loss 0.032704\n",
      "batch 4298: loss 0.046091\n",
      "batch 4299: loss 0.041534\n",
      "batch 4300: loss 0.018385\n",
      "batch 4301: loss 0.031972\n",
      "batch 4302: loss 0.076469\n",
      "batch 4303: loss 0.024136\n",
      "batch 4304: loss 0.021743\n",
      "batch 4305: loss 0.144900\n",
      "batch 4306: loss 0.035870\n",
      "batch 4307: loss 0.046681\n",
      "batch 4308: loss 0.029712\n",
      "batch 4309: loss 0.145647\n",
      "batch 4310: loss 0.044116\n",
      "batch 4311: loss 0.112331\n",
      "batch 4312: loss 0.090645\n",
      "batch 4313: loss 0.041802\n",
      "batch 4314: loss 0.012881\n",
      "batch 4315: loss 0.054198\n",
      "batch 4316: loss 0.108961\n",
      "batch 4317: loss 0.129131\n",
      "batch 4318: loss 0.092284\n",
      "batch 4319: loss 0.128903\n",
      "batch 4320: loss 0.023207\n",
      "batch 4321: loss 0.131701\n",
      "batch 4322: loss 0.062057\n",
      "batch 4323: loss 0.075563\n",
      "batch 4324: loss 0.033128\n",
      "batch 4325: loss 0.019807\n",
      "batch 4326: loss 0.121278\n",
      "batch 4327: loss 0.133605\n",
      "batch 4328: loss 0.011932\n",
      "batch 4329: loss 0.037690\n",
      "batch 4330: loss 0.037601\n",
      "batch 4331: loss 0.028314\n",
      "batch 4332: loss 0.059030\n",
      "batch 4333: loss 0.009509\n",
      "batch 4334: loss 0.032117\n",
      "batch 4335: loss 0.030965\n",
      "batch 4336: loss 0.030182\n",
      "batch 4337: loss 0.057420\n",
      "batch 4338: loss 0.120126\n",
      "batch 4339: loss 0.119076\n",
      "batch 4340: loss 0.052348\n",
      "batch 4341: loss 0.020926\n",
      "batch 4342: loss 0.017470\n",
      "batch 4343: loss 0.058797\n",
      "batch 4344: loss 0.057076\n",
      "batch 4345: loss 0.021016\n",
      "batch 4346: loss 0.050643\n",
      "batch 4347: loss 0.113306\n",
      "batch 4348: loss 0.038236\n",
      "batch 4349: loss 0.038331\n",
      "batch 4350: loss 0.037977\n",
      "batch 4351: loss 0.054880\n",
      "batch 4352: loss 0.027747\n",
      "batch 4353: loss 0.186628\n",
      "batch 4354: loss 0.083566\n",
      "batch 4355: loss 0.025826\n",
      "batch 4356: loss 0.078945\n",
      "batch 4357: loss 0.198495\n",
      "batch 4358: loss 0.039631\n",
      "batch 4359: loss 0.072446\n",
      "batch 4360: loss 0.019997\n",
      "batch 4361: loss 0.008138\n",
      "batch 4362: loss 0.055604\n",
      "batch 4363: loss 0.039123\n",
      "batch 4364: loss 0.025234\n",
      "batch 4365: loss 0.057133\n",
      "batch 4366: loss 0.029478\n",
      "batch 4367: loss 0.078845\n",
      "batch 4368: loss 0.044034\n",
      "batch 4369: loss 0.055662\n",
      "batch 4370: loss 0.042941\n",
      "batch 4371: loss 0.054266\n",
      "batch 4372: loss 0.025472\n",
      "batch 4373: loss 0.085193\n",
      "batch 4374: loss 0.115294\n",
      "batch 4375: loss 0.020163\n",
      "batch 4376: loss 0.054552\n",
      "batch 4377: loss 0.106192\n",
      "batch 4378: loss 0.140872\n",
      "batch 4379: loss 0.104467\n",
      "batch 4380: loss 0.012672\n",
      "batch 4381: loss 0.027327\n",
      "batch 4382: loss 0.068771\n",
      "batch 4383: loss 0.055480\n",
      "batch 4384: loss 0.201296\n",
      "batch 4385: loss 0.047273\n",
      "batch 4386: loss 0.090413\n",
      "batch 4387: loss 0.044596\n",
      "batch 4388: loss 0.019438\n",
      "batch 4389: loss 0.039586\n",
      "batch 4390: loss 0.020389\n",
      "batch 4391: loss 0.022606\n",
      "batch 4392: loss 0.026434\n",
      "batch 4393: loss 0.122300\n",
      "batch 4394: loss 0.103003\n",
      "batch 4395: loss 0.074740\n",
      "batch 4396: loss 0.047584\n",
      "batch 4397: loss 0.066744\n",
      "batch 4398: loss 0.242350\n",
      "batch 4399: loss 0.046689\n",
      "batch 4400: loss 0.117520\n",
      "batch 4401: loss 0.090838\n",
      "batch 4402: loss 0.018487\n",
      "batch 4403: loss 0.032261\n",
      "batch 4404: loss 0.084682\n",
      "batch 4405: loss 0.009680\n",
      "batch 4406: loss 0.015906\n",
      "batch 4407: loss 0.037865\n",
      "batch 4408: loss 0.224044\n",
      "batch 4409: loss 0.027459\n",
      "batch 4410: loss 0.017770\n",
      "batch 4411: loss 0.032753\n",
      "batch 4412: loss 0.137025\n",
      "batch 4413: loss 0.037362\n",
      "batch 4414: loss 0.089839\n",
      "batch 4415: loss 0.072685\n",
      "batch 4416: loss 0.032643\n",
      "batch 4417: loss 0.018620\n",
      "batch 4418: loss 0.020284\n",
      "batch 4419: loss 0.035595\n",
      "batch 4420: loss 0.065373\n",
      "batch 4421: loss 0.034733\n",
      "batch 4422: loss 0.076015\n",
      "batch 4423: loss 0.041222\n",
      "batch 4424: loss 0.129472\n",
      "batch 4425: loss 0.091903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4426: loss 0.018541\n",
      "batch 4427: loss 0.028238\n",
      "batch 4428: loss 0.065711\n",
      "batch 4429: loss 0.039669\n",
      "batch 4430: loss 0.034379\n",
      "batch 4431: loss 0.110205\n",
      "batch 4432: loss 0.031028\n",
      "batch 4433: loss 0.034371\n",
      "batch 4434: loss 0.011023\n",
      "batch 4435: loss 0.198900\n",
      "batch 4436: loss 0.025627\n",
      "batch 4437: loss 0.017472\n",
      "batch 4438: loss 0.030237\n",
      "batch 4439: loss 0.044060\n",
      "batch 4440: loss 0.065467\n",
      "batch 4441: loss 0.014894\n",
      "batch 4442: loss 0.028347\n",
      "batch 4443: loss 0.038167\n",
      "batch 4444: loss 0.174668\n",
      "batch 4445: loss 0.101232\n",
      "batch 4446: loss 0.020385\n",
      "batch 4447: loss 0.061920\n",
      "batch 4448: loss 0.029190\n",
      "batch 4449: loss 0.028631\n",
      "batch 4450: loss 0.078042\n",
      "batch 4451: loss 0.030350\n",
      "batch 4452: loss 0.193013\n",
      "batch 4453: loss 0.015187\n",
      "batch 4454: loss 0.091566\n",
      "batch 4455: loss 0.057570\n",
      "batch 4456: loss 0.013041\n",
      "batch 4457: loss 0.304340\n",
      "batch 4458: loss 0.022265\n",
      "batch 4459: loss 0.087147\n",
      "batch 4460: loss 0.056082\n",
      "batch 4461: loss 0.169133\n",
      "batch 4462: loss 0.022505\n",
      "batch 4463: loss 0.019409\n",
      "batch 4464: loss 0.130214\n",
      "batch 4465: loss 0.164540\n",
      "batch 4466: loss 0.054605\n",
      "batch 4467: loss 0.076986\n",
      "batch 4468: loss 0.073551\n",
      "batch 4469: loss 0.047002\n",
      "batch 4470: loss 0.036377\n",
      "batch 4471: loss 0.106649\n",
      "batch 4472: loss 0.024493\n",
      "batch 4473: loss 0.025362\n",
      "batch 4474: loss 0.034721\n",
      "batch 4475: loss 0.088576\n",
      "batch 4476: loss 0.084135\n",
      "batch 4477: loss 0.034947\n",
      "batch 4478: loss 0.047122\n",
      "batch 4479: loss 0.045229\n",
      "batch 4480: loss 0.049249\n",
      "batch 4481: loss 0.017589\n",
      "batch 4482: loss 0.066836\n",
      "batch 4483: loss 0.211574\n",
      "batch 4484: loss 0.019452\n",
      "batch 4485: loss 0.035003\n",
      "batch 4486: loss 0.120370\n",
      "batch 4487: loss 0.071177\n",
      "batch 4488: loss 0.042709\n",
      "batch 4489: loss 0.030904\n",
      "batch 4490: loss 0.141291\n",
      "batch 4491: loss 0.027872\n",
      "batch 4492: loss 0.020929\n",
      "batch 4493: loss 0.049958\n",
      "batch 4494: loss 0.071673\n",
      "batch 4495: loss 0.353152\n",
      "batch 4496: loss 0.086941\n",
      "batch 4497: loss 0.106066\n",
      "batch 4498: loss 0.026959\n",
      "batch 4499: loss 0.032198\n",
      "batch 4500: loss 0.116863\n",
      "batch 4501: loss 0.043491\n",
      "batch 4502: loss 0.109866\n",
      "batch 4503: loss 0.042476\n",
      "batch 4504: loss 0.029843\n",
      "batch 4505: loss 0.070274\n",
      "batch 4506: loss 0.046601\n",
      "batch 4507: loss 0.126850\n",
      "batch 4508: loss 0.120297\n",
      "batch 4509: loss 0.081864\n",
      "batch 4510: loss 0.064761\n",
      "batch 4511: loss 0.030848\n",
      "batch 4512: loss 0.197086\n",
      "batch 4513: loss 0.085003\n",
      "batch 4514: loss 0.055500\n",
      "batch 4515: loss 0.062861\n",
      "batch 4516: loss 0.088717\n",
      "batch 4517: loss 0.022883\n",
      "batch 4518: loss 0.040211\n",
      "batch 4519: loss 0.032922\n",
      "batch 4520: loss 0.133392\n",
      "batch 4521: loss 0.128520\n",
      "batch 4522: loss 0.043046\n",
      "batch 4523: loss 0.082598\n",
      "batch 4524: loss 0.253237\n",
      "batch 4525: loss 0.024713\n",
      "batch 4526: loss 0.012813\n",
      "batch 4527: loss 0.018937\n",
      "batch 4528: loss 0.093383\n",
      "batch 4529: loss 0.018685\n",
      "batch 4530: loss 0.009323\n",
      "batch 4531: loss 0.186692\n",
      "batch 4532: loss 0.032534\n",
      "batch 4533: loss 0.015987\n",
      "batch 4534: loss 0.087442\n",
      "batch 4535: loss 0.049594\n",
      "batch 4536: loss 0.035432\n",
      "batch 4537: loss 0.025648\n",
      "batch 4538: loss 0.037808\n",
      "batch 4539: loss 0.181498\n",
      "batch 4540: loss 0.048601\n",
      "batch 4541: loss 0.098556\n",
      "batch 4542: loss 0.023499\n",
      "batch 4543: loss 0.025317\n",
      "batch 4544: loss 0.063207\n",
      "batch 4545: loss 0.038853\n",
      "batch 4546: loss 0.019926\n",
      "batch 4547: loss 0.017393\n",
      "batch 4548: loss 0.010709\n",
      "batch 4549: loss 0.010014\n",
      "batch 4550: loss 0.043185\n",
      "batch 4551: loss 0.007180\n",
      "batch 4552: loss 0.017567\n",
      "batch 4553: loss 0.098432\n",
      "batch 4554: loss 0.031140\n",
      "batch 4555: loss 0.015177\n",
      "batch 4556: loss 0.155587\n",
      "batch 4557: loss 0.034207\n",
      "batch 4558: loss 0.017015\n",
      "batch 4559: loss 0.061238\n",
      "batch 4560: loss 0.028750\n",
      "batch 4561: loss 0.036340\n",
      "batch 4562: loss 0.019572\n",
      "batch 4563: loss 0.045043\n",
      "batch 4564: loss 0.065109\n",
      "batch 4565: loss 0.033454\n",
      "batch 4566: loss 0.012716\n",
      "batch 4567: loss 0.036096\n",
      "batch 4568: loss 0.129867\n",
      "batch 4569: loss 0.057011\n",
      "batch 4570: loss 0.015348\n",
      "batch 4571: loss 0.027566\n",
      "batch 4572: loss 0.021737\n",
      "batch 4573: loss 0.009992\n",
      "batch 4574: loss 0.110052\n",
      "batch 4575: loss 0.029415\n",
      "batch 4576: loss 0.130296\n",
      "batch 4577: loss 0.029308\n",
      "batch 4578: loss 0.024344\n",
      "batch 4579: loss 0.120520\n",
      "batch 4580: loss 0.015481\n",
      "batch 4581: loss 0.025873\n",
      "batch 4582: loss 0.023865\n",
      "batch 4583: loss 0.036631\n",
      "batch 4584: loss 0.278174\n",
      "batch 4585: loss 0.030513\n",
      "batch 4586: loss 0.017761\n",
      "batch 4587: loss 0.060280\n",
      "batch 4588: loss 0.049446\n",
      "batch 4589: loss 0.012765\n",
      "batch 4590: loss 0.051713\n",
      "batch 4591: loss 0.044910\n",
      "batch 4592: loss 0.107008\n",
      "batch 4593: loss 0.090429\n",
      "batch 4594: loss 0.018763\n",
      "batch 4595: loss 0.032502\n",
      "batch 4596: loss 0.021924\n",
      "batch 4597: loss 0.022547\n",
      "batch 4598: loss 0.013199\n",
      "batch 4599: loss 0.046305\n",
      "batch 4600: loss 0.039522\n",
      "batch 4601: loss 0.164339\n",
      "batch 4602: loss 0.017851\n",
      "batch 4603: loss 0.125798\n",
      "batch 4604: loss 0.036258\n",
      "batch 4605: loss 0.142131\n",
      "batch 4606: loss 0.069053\n",
      "batch 4607: loss 0.009847\n",
      "batch 4608: loss 0.090078\n",
      "batch 4609: loss 0.012799\n",
      "batch 4610: loss 0.075494\n",
      "batch 4611: loss 0.015193\n",
      "batch 4612: loss 0.042891\n",
      "batch 4613: loss 0.024774\n",
      "batch 4614: loss 0.042927\n",
      "batch 4615: loss 0.175141\n",
      "batch 4616: loss 0.045837\n",
      "batch 4617: loss 0.110338\n",
      "batch 4618: loss 0.015400\n",
      "batch 4619: loss 0.020601\n",
      "batch 4620: loss 0.023570\n",
      "batch 4621: loss 0.049102\n",
      "batch 4622: loss 0.068155\n",
      "batch 4623: loss 0.021165\n",
      "batch 4624: loss 0.020497\n",
      "batch 4625: loss 0.038016\n",
      "batch 4626: loss 0.028637\n",
      "batch 4627: loss 0.020324\n",
      "batch 4628: loss 0.199724\n",
      "batch 4629: loss 0.027731\n",
      "batch 4630: loss 0.111779\n",
      "batch 4631: loss 0.023658\n",
      "batch 4632: loss 0.054159\n",
      "batch 4633: loss 0.072222\n",
      "batch 4634: loss 0.025949\n",
      "batch 4635: loss 0.018489\n",
      "batch 4636: loss 0.088292\n",
      "batch 4637: loss 0.080811\n",
      "batch 4638: loss 0.013110\n",
      "batch 4639: loss 0.033463\n",
      "batch 4640: loss 0.098303\n",
      "batch 4641: loss 0.015353\n",
      "batch 4642: loss 0.081393\n",
      "batch 4643: loss 0.103225\n",
      "batch 4644: loss 0.009512\n",
      "batch 4645: loss 0.009482\n",
      "batch 4646: loss 0.039358\n",
      "batch 4647: loss 0.103396\n",
      "batch 4648: loss 0.058461\n",
      "batch 4649: loss 0.125883\n",
      "batch 4650: loss 0.032293\n",
      "batch 4651: loss 0.105769\n",
      "batch 4652: loss 0.010760\n",
      "batch 4653: loss 0.023362\n",
      "batch 4654: loss 0.099548\n",
      "batch 4655: loss 0.068465\n",
      "batch 4656: loss 0.072535\n",
      "batch 4657: loss 0.069293\n",
      "batch 4658: loss 0.007717\n",
      "batch 4659: loss 0.025217\n",
      "batch 4660: loss 0.021848\n",
      "batch 4661: loss 0.029010\n",
      "batch 4662: loss 0.071232\n",
      "batch 4663: loss 0.042030\n",
      "batch 4664: loss 0.117433\n",
      "batch 4665: loss 0.054536\n",
      "batch 4666: loss 0.064130\n",
      "batch 4667: loss 0.027003\n",
      "batch 4668: loss 0.050284\n",
      "batch 4669: loss 0.037464\n",
      "batch 4670: loss 0.072405\n",
      "batch 4671: loss 0.049858\n",
      "batch 4672: loss 0.130133\n",
      "batch 4673: loss 0.045389\n",
      "batch 4674: loss 0.086101\n",
      "batch 4675: loss 0.072426\n",
      "batch 4676: loss 0.018439\n",
      "batch 4677: loss 0.110573\n",
      "batch 4678: loss 0.006972\n",
      "batch 4679: loss 0.096040\n",
      "batch 4680: loss 0.040248\n",
      "batch 4681: loss 0.015618\n",
      "batch 4682: loss 0.009258\n",
      "batch 4683: loss 0.034647\n",
      "batch 4684: loss 0.063126\n",
      "batch 4685: loss 0.035983\n",
      "batch 4686: loss 0.047864\n",
      "batch 4687: loss 0.028067\n",
      "batch 4688: loss 0.095049\n",
      "batch 4689: loss 0.022930\n",
      "batch 4690: loss 0.030762\n",
      "batch 4691: loss 0.069280\n",
      "batch 4692: loss 0.024913\n",
      "batch 4693: loss 0.022868\n",
      "batch 4694: loss 0.124709\n",
      "batch 4695: loss 0.012899\n",
      "batch 4696: loss 0.015540\n",
      "batch 4697: loss 0.102027\n",
      "batch 4698: loss 0.096610\n",
      "batch 4699: loss 0.086034\n",
      "batch 4700: loss 0.038322\n",
      "batch 4701: loss 0.070757\n",
      "batch 4702: loss 0.078947\n",
      "batch 4703: loss 0.196997\n",
      "batch 4704: loss 0.017216\n",
      "batch 4705: loss 0.018960\n",
      "batch 4706: loss 0.023590\n",
      "batch 4707: loss 0.220240\n",
      "batch 4708: loss 0.058306\n",
      "batch 4709: loss 0.087501\n",
      "batch 4710: loss 0.029409\n",
      "batch 4711: loss 0.064982\n",
      "batch 4712: loss 0.034733\n",
      "batch 4713: loss 0.023402\n",
      "batch 4714: loss 0.021239\n",
      "batch 4715: loss 0.060890\n",
      "batch 4716: loss 0.064931\n",
      "batch 4717: loss 0.016387\n",
      "batch 4718: loss 0.010701\n",
      "batch 4719: loss 0.051881\n",
      "batch 4720: loss 0.070491\n",
      "batch 4721: loss 0.058519\n",
      "batch 4722: loss 0.081125\n",
      "batch 4723: loss 0.051329\n",
      "batch 4724: loss 0.041753\n",
      "batch 4725: loss 0.021234\n",
      "batch 4726: loss 0.087641\n",
      "batch 4727: loss 0.055260\n",
      "batch 4728: loss 0.071950\n",
      "batch 4729: loss 0.017517\n",
      "batch 4730: loss 0.013537\n",
      "batch 4731: loss 0.033357\n",
      "batch 4732: loss 0.031811\n",
      "batch 4733: loss 0.068398\n",
      "batch 4734: loss 0.082799\n",
      "batch 4735: loss 0.019850\n",
      "batch 4736: loss 0.049251\n",
      "batch 4737: loss 0.086203\n",
      "batch 4738: loss 0.193753\n",
      "batch 4739: loss 0.021027\n",
      "batch 4740: loss 0.058394\n",
      "batch 4741: loss 0.052541\n",
      "batch 4742: loss 0.075498\n",
      "batch 4743: loss 0.086609\n",
      "batch 4744: loss 0.038426\n",
      "batch 4745: loss 0.128336\n",
      "batch 4746: loss 0.035105\n",
      "batch 4747: loss 0.068552\n",
      "batch 4748: loss 0.138301\n",
      "batch 4749: loss 0.070543\n",
      "batch 4750: loss 0.072134\n",
      "batch 4751: loss 0.012651\n",
      "batch 4752: loss 0.010663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4753: loss 0.020653\n",
      "batch 4754: loss 0.083181\n",
      "batch 4755: loss 0.010909\n",
      "batch 4756: loss 0.043500\n",
      "batch 4757: loss 0.066022\n",
      "batch 4758: loss 0.022486\n",
      "batch 4759: loss 0.267442\n",
      "batch 4760: loss 0.102723\n",
      "batch 4761: loss 0.145377\n",
      "batch 4762: loss 0.053585\n",
      "batch 4763: loss 0.029102\n",
      "batch 4764: loss 0.090092\n",
      "batch 4765: loss 0.049719\n",
      "batch 4766: loss 0.022748\n",
      "batch 4767: loss 0.039745\n",
      "batch 4768: loss 0.059553\n",
      "batch 4769: loss 0.025978\n",
      "batch 4770: loss 0.023191\n",
      "batch 4771: loss 0.079260\n",
      "batch 4772: loss 0.062416\n",
      "batch 4773: loss 0.012795\n",
      "batch 4774: loss 0.018660\n",
      "batch 4775: loss 0.311953\n",
      "batch 4776: loss 0.056313\n",
      "batch 4777: loss 0.032010\n",
      "batch 4778: loss 0.081163\n",
      "batch 4779: loss 0.092005\n",
      "batch 4780: loss 0.018106\n",
      "batch 4781: loss 0.011809\n",
      "batch 4782: loss 0.044611\n",
      "batch 4783: loss 0.083163\n",
      "batch 4784: loss 0.008340\n",
      "batch 4785: loss 0.083408\n",
      "batch 4786: loss 0.126669\n",
      "batch 4787: loss 0.012945\n",
      "batch 4788: loss 0.052477\n",
      "batch 4789: loss 0.031824\n",
      "batch 4790: loss 0.011815\n",
      "batch 4791: loss 0.111811\n",
      "batch 4792: loss 0.014766\n",
      "batch 4793: loss 0.080081\n",
      "batch 4794: loss 0.065081\n",
      "batch 4795: loss 0.005535\n",
      "batch 4796: loss 0.013805\n",
      "batch 4797: loss 0.177538\n",
      "batch 4798: loss 0.011560\n",
      "batch 4799: loss 0.064851\n",
      "batch 4800: loss 0.098013\n",
      "batch 4801: loss 0.184246\n",
      "batch 4802: loss 0.046821\n",
      "batch 4803: loss 0.112678\n",
      "batch 4804: loss 0.033466\n",
      "batch 4805: loss 0.111908\n",
      "batch 4806: loss 0.017878\n",
      "batch 4807: loss 0.008040\n",
      "batch 4808: loss 0.027447\n",
      "batch 4809: loss 0.260707\n",
      "batch 4810: loss 0.032785\n",
      "batch 4811: loss 0.089212\n",
      "batch 4812: loss 0.078091\n",
      "batch 4813: loss 0.188806\n",
      "batch 4814: loss 0.108730\n",
      "batch 4815: loss 0.034626\n",
      "batch 4816: loss 0.121930\n",
      "batch 4817: loss 0.070509\n",
      "batch 4818: loss 0.083824\n",
      "batch 4819: loss 0.039651\n",
      "batch 4820: loss 0.152731\n",
      "batch 4821: loss 0.049347\n",
      "batch 4822: loss 0.192940\n",
      "batch 4823: loss 0.129111\n",
      "batch 4824: loss 0.022421\n",
      "batch 4825: loss 0.056464\n",
      "batch 4826: loss 0.116731\n",
      "batch 4827: loss 0.020311\n",
      "batch 4828: loss 0.025804\n",
      "batch 4829: loss 0.049518\n",
      "batch 4830: loss 0.030443\n",
      "batch 4831: loss 0.020434\n",
      "batch 4832: loss 0.027843\n",
      "batch 4833: loss 0.040307\n",
      "batch 4834: loss 0.066554\n",
      "batch 4835: loss 0.023637\n",
      "batch 4836: loss 0.043901\n",
      "batch 4837: loss 0.062349\n",
      "batch 4838: loss 0.020351\n",
      "batch 4839: loss 0.025852\n",
      "batch 4840: loss 0.026459\n",
      "batch 4841: loss 0.042198\n",
      "batch 4842: loss 0.019319\n",
      "batch 4843: loss 0.032167\n",
      "batch 4844: loss 0.068358\n",
      "batch 4845: loss 0.025427\n",
      "batch 4846: loss 0.024654\n",
      "batch 4847: loss 0.086406\n",
      "batch 4848: loss 0.039133\n",
      "batch 4849: loss 0.076274\n",
      "batch 4850: loss 0.026079\n",
      "batch 4851: loss 0.089706\n",
      "batch 4852: loss 0.026583\n",
      "batch 4853: loss 0.054695\n",
      "batch 4854: loss 0.183019\n",
      "batch 4855: loss 0.122460\n",
      "batch 4856: loss 0.016175\n",
      "batch 4857: loss 0.052825\n",
      "batch 4858: loss 0.021379\n",
      "batch 4859: loss 0.044664\n",
      "batch 4860: loss 0.063690\n",
      "batch 4861: loss 0.044583\n",
      "batch 4862: loss 0.032366\n",
      "batch 4863: loss 0.053626\n",
      "batch 4864: loss 0.029753\n",
      "batch 4865: loss 0.060787\n",
      "batch 4866: loss 0.213124\n",
      "batch 4867: loss 0.036795\n",
      "batch 4868: loss 0.011612\n",
      "batch 4869: loss 0.036693\n",
      "batch 4870: loss 0.104912\n",
      "batch 4871: loss 0.089148\n",
      "batch 4872: loss 0.043330\n",
      "batch 4873: loss 0.045424\n",
      "batch 4874: loss 0.049003\n",
      "batch 4875: loss 0.081376\n",
      "batch 4876: loss 0.014025\n",
      "batch 4877: loss 0.026170\n",
      "batch 4878: loss 0.017624\n",
      "batch 4879: loss 0.075692\n",
      "batch 4880: loss 0.048751\n",
      "batch 4881: loss 0.015619\n",
      "batch 4882: loss 0.105455\n",
      "batch 4883: loss 0.043759\n",
      "batch 4884: loss 0.035360\n",
      "batch 4885: loss 0.081400\n",
      "batch 4886: loss 0.034195\n",
      "batch 4887: loss 0.041577\n",
      "batch 4888: loss 0.078678\n",
      "batch 4889: loss 0.009757\n",
      "batch 4890: loss 0.094440\n",
      "batch 4891: loss 0.078633\n",
      "batch 4892: loss 0.067606\n",
      "batch 4893: loss 0.036267\n",
      "batch 4894: loss 0.171418\n",
      "batch 4895: loss 0.033580\n",
      "batch 4896: loss 0.017285\n",
      "batch 4897: loss 0.071218\n",
      "batch 4898: loss 0.054625\n",
      "batch 4899: loss 0.044714\n",
      "batch 4900: loss 0.097736\n",
      "batch 4901: loss 0.141629\n",
      "batch 4902: loss 0.214370\n",
      "batch 4903: loss 0.111616\n",
      "batch 4904: loss 0.043394\n",
      "batch 4905: loss 0.033280\n",
      "batch 4906: loss 0.033378\n",
      "batch 4907: loss 0.040461\n",
      "batch 4908: loss 0.061599\n",
      "batch 4909: loss 0.056457\n",
      "batch 4910: loss 0.048355\n",
      "batch 4911: loss 0.018768\n",
      "batch 4912: loss 0.019366\n",
      "batch 4913: loss 0.029093\n",
      "batch 4914: loss 0.048805\n",
      "batch 4915: loss 0.031705\n",
      "batch 4916: loss 0.103446\n",
      "batch 4917: loss 0.114648\n",
      "batch 4918: loss 0.017962\n",
      "batch 4919: loss 0.028246\n",
      "batch 4920: loss 0.057705\n",
      "batch 4921: loss 0.008023\n",
      "batch 4922: loss 0.045280\n",
      "batch 4923: loss 0.065594\n",
      "batch 4924: loss 0.020993\n",
      "batch 4925: loss 0.198240\n",
      "batch 4926: loss 0.027588\n",
      "batch 4927: loss 0.040567\n",
      "batch 4928: loss 0.077607\n",
      "batch 4929: loss 0.008290\n",
      "batch 4930: loss 0.037298\n",
      "batch 4931: loss 0.280752\n",
      "batch 4932: loss 0.047753\n",
      "batch 4933: loss 0.076058\n",
      "batch 4934: loss 0.089667\n",
      "batch 4935: loss 0.051489\n",
      "batch 4936: loss 0.046139\n",
      "batch 4937: loss 0.088248\n",
      "batch 4938: loss 0.050510\n",
      "batch 4939: loss 0.008830\n",
      "batch 4940: loss 0.040484\n",
      "batch 4941: loss 0.131521\n",
      "batch 4942: loss 0.128814\n",
      "batch 4943: loss 0.045504\n",
      "batch 4944: loss 0.081603\n",
      "batch 4945: loss 0.048637\n",
      "batch 4946: loss 0.029362\n",
      "batch 4947: loss 0.052689\n",
      "batch 4948: loss 0.058513\n",
      "batch 4949: loss 0.066657\n",
      "batch 4950: loss 0.028234\n",
      "batch 4951: loss 0.076674\n",
      "batch 4952: loss 0.045192\n",
      "batch 4953: loss 0.025044\n",
      "batch 4954: loss 0.044954\n",
      "batch 4955: loss 0.173230\n",
      "batch 4956: loss 0.030176\n",
      "batch 4957: loss 0.030276\n",
      "batch 4958: loss 0.030082\n",
      "batch 4959: loss 0.029230\n",
      "batch 4960: loss 0.072823\n",
      "batch 4961: loss 0.020853\n",
      "batch 4962: loss 0.019981\n",
      "batch 4963: loss 0.024124\n",
      "batch 4964: loss 0.054209\n",
      "batch 4965: loss 0.008267\n",
      "batch 4966: loss 0.032156\n",
      "batch 4967: loss 0.029470\n",
      "batch 4968: loss 0.011987\n",
      "batch 4969: loss 0.018855\n",
      "batch 4970: loss 0.035194\n",
      "batch 4971: loss 0.033847\n",
      "batch 4972: loss 0.058905\n",
      "batch 4973: loss 0.110222\n",
      "batch 4974: loss 0.043827\n",
      "batch 4975: loss 0.047159\n",
      "batch 4976: loss 0.033065\n",
      "batch 4977: loss 0.024297\n",
      "batch 4978: loss 0.121741\n",
      "batch 4979: loss 0.043998\n",
      "batch 4980: loss 0.040308\n",
      "batch 4981: loss 0.125982\n",
      "batch 4982: loss 0.048251\n",
      "batch 4983: loss 0.091841\n",
      "batch 4984: loss 0.072201\n",
      "batch 4985: loss 0.051154\n",
      "batch 4986: loss 0.025110\n",
      "batch 4987: loss 0.057727\n",
      "batch 4988: loss 0.019964\n",
      "batch 4989: loss 0.020486\n",
      "batch 4990: loss 0.041633\n",
      "batch 4991: loss 0.099192\n",
      "batch 4992: loss 0.067547\n",
      "batch 4993: loss 0.024060\n",
      "batch 4994: loss 0.009667\n",
      "batch 4995: loss 0.016938\n",
      "batch 4996: loss 0.083431\n",
      "batch 4997: loss 0.075232\n",
      "batch 4998: loss 0.070370\n",
      "batch 4999: loss 0.030610\n",
      "batch 5000: loss 0.039052\n",
      "batch 5001: loss 0.081586\n",
      "batch 5002: loss 0.013546\n",
      "batch 5003: loss 0.011925\n",
      "batch 5004: loss 0.086574\n",
      "batch 5005: loss 0.021632\n",
      "batch 5006: loss 0.247521\n",
      "batch 5007: loss 0.222948\n",
      "batch 5008: loss 0.176938\n",
      "batch 5009: loss 0.031411\n",
      "batch 5010: loss 0.018836\n",
      "batch 5011: loss 0.008600\n",
      "batch 5012: loss 0.025205\n",
      "batch 5013: loss 0.039424\n",
      "batch 5014: loss 0.092341\n",
      "batch 5015: loss 0.054414\n",
      "batch 5016: loss 0.165151\n",
      "batch 5017: loss 0.050632\n",
      "batch 5018: loss 0.061856\n",
      "batch 5019: loss 0.063508\n",
      "batch 5020: loss 0.102771\n",
      "batch 5021: loss 0.041045\n",
      "batch 5022: loss 0.008243\n",
      "batch 5023: loss 0.065576\n",
      "batch 5024: loss 0.081659\n",
      "batch 5025: loss 0.134603\n",
      "batch 5026: loss 0.052254\n",
      "batch 5027: loss 0.210862\n",
      "batch 5028: loss 0.053927\n",
      "batch 5029: loss 0.019766\n",
      "batch 5030: loss 0.014426\n",
      "batch 5031: loss 0.011879\n",
      "batch 5032: loss 0.041630\n",
      "batch 5033: loss 0.205024\n",
      "batch 5034: loss 0.062184\n",
      "batch 5035: loss 0.023094\n",
      "batch 5036: loss 0.022901\n",
      "batch 5037: loss 0.043874\n",
      "batch 5038: loss 0.072399\n",
      "batch 5039: loss 0.067309\n",
      "batch 5040: loss 0.155935\n",
      "batch 5041: loss 0.051092\n",
      "batch 5042: loss 0.073382\n",
      "batch 5043: loss 0.090475\n",
      "batch 5044: loss 0.104390\n",
      "batch 5045: loss 0.017206\n",
      "batch 5046: loss 0.013962\n",
      "batch 5047: loss 0.122756\n",
      "batch 5048: loss 0.223362\n",
      "batch 5049: loss 0.082122\n",
      "batch 5050: loss 0.038323\n",
      "batch 5051: loss 0.024777\n",
      "batch 5052: loss 0.014691\n",
      "batch 5053: loss 0.028548\n",
      "batch 5054: loss 0.022261\n",
      "batch 5055: loss 0.045426\n",
      "batch 5056: loss 0.096135\n",
      "batch 5057: loss 0.015175\n",
      "batch 5058: loss 0.103701\n",
      "batch 5059: loss 0.043812\n",
      "batch 5060: loss 0.012346\n",
      "batch 5061: loss 0.022414\n",
      "batch 5062: loss 0.023861\n",
      "batch 5063: loss 0.028279\n",
      "batch 5064: loss 0.018556\n",
      "batch 5065: loss 0.009163\n",
      "batch 5066: loss 0.016067\n",
      "batch 5067: loss 0.032430\n",
      "batch 5068: loss 0.021066\n",
      "batch 5069: loss 0.033953\n",
      "batch 5070: loss 0.021553\n",
      "batch 5071: loss 0.051089\n",
      "batch 5072: loss 0.205102\n",
      "batch 5073: loss 0.078810\n",
      "batch 5074: loss 0.081701\n",
      "batch 5075: loss 0.012844\n",
      "batch 5076: loss 0.163992\n",
      "batch 5077: loss 0.036344\n",
      "batch 5078: loss 0.056509\n",
      "batch 5079: loss 0.056410\n",
      "batch 5080: loss 0.017058\n",
      "batch 5081: loss 0.044882\n",
      "batch 5082: loss 0.018113\n",
      "batch 5083: loss 0.165324\n",
      "batch 5084: loss 0.042398\n",
      "batch 5085: loss 0.037268\n",
      "batch 5086: loss 0.014418\n",
      "batch 5087: loss 0.032006\n",
      "batch 5088: loss 0.029308\n",
      "batch 5089: loss 0.064571\n",
      "batch 5090: loss 0.056720\n",
      "batch 5091: loss 0.025069\n",
      "batch 5092: loss 0.148724\n",
      "batch 5093: loss 0.032188\n",
      "batch 5094: loss 0.046112\n",
      "batch 5095: loss 0.032518\n",
      "batch 5096: loss 0.013757\n",
      "batch 5097: loss 0.008436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5098: loss 0.068797\n",
      "batch 5099: loss 0.044556\n",
      "batch 5100: loss 0.150175\n",
      "batch 5101: loss 0.033028\n",
      "batch 5102: loss 0.057800\n",
      "batch 5103: loss 0.040591\n",
      "batch 5104: loss 0.033216\n",
      "batch 5105: loss 0.064875\n",
      "batch 5106: loss 0.023333\n",
      "batch 5107: loss 0.024097\n",
      "batch 5108: loss 0.041645\n",
      "batch 5109: loss 0.015214\n",
      "batch 5110: loss 0.023347\n",
      "batch 5111: loss 0.015303\n",
      "batch 5112: loss 0.040533\n",
      "batch 5113: loss 0.073789\n",
      "batch 5114: loss 0.027309\n",
      "batch 5115: loss 0.028070\n",
      "batch 5116: loss 0.178180\n",
      "batch 5117: loss 0.019393\n",
      "batch 5118: loss 0.016328\n",
      "batch 5119: loss 0.026753\n",
      "batch 5120: loss 0.016176\n",
      "batch 5121: loss 0.019594\n",
      "batch 5122: loss 0.086949\n",
      "batch 5123: loss 0.014113\n",
      "batch 5124: loss 0.106383\n",
      "batch 5125: loss 0.024899\n",
      "batch 5126: loss 0.063764\n",
      "batch 5127: loss 0.015309\n",
      "batch 5128: loss 0.020666\n",
      "batch 5129: loss 0.051349\n",
      "batch 5130: loss 0.006751\n",
      "batch 5131: loss 0.142402\n",
      "batch 5132: loss 0.032188\n",
      "batch 5133: loss 0.022839\n",
      "batch 5134: loss 0.029800\n",
      "batch 5135: loss 0.070172\n",
      "batch 5136: loss 0.147172\n",
      "batch 5137: loss 0.068820\n",
      "batch 5138: loss 0.008406\n",
      "batch 5139: loss 0.238786\n",
      "batch 5140: loss 0.013119\n",
      "batch 5141: loss 0.064997\n",
      "batch 5142: loss 0.048706\n",
      "batch 5143: loss 0.053171\n",
      "batch 5144: loss 0.065500\n",
      "batch 5145: loss 0.126463\n",
      "batch 5146: loss 0.010615\n",
      "batch 5147: loss 0.076846\n",
      "batch 5148: loss 0.016979\n",
      "batch 5149: loss 0.098638\n",
      "batch 5150: loss 0.083929\n",
      "batch 5151: loss 0.093926\n",
      "batch 5152: loss 0.012399\n",
      "batch 5153: loss 0.010332\n",
      "batch 5154: loss 0.093045\n",
      "batch 5155: loss 0.056517\n",
      "batch 5156: loss 0.030289\n",
      "batch 5157: loss 0.017887\n",
      "batch 5158: loss 0.040998\n",
      "batch 5159: loss 0.022752\n",
      "batch 5160: loss 0.091926\n",
      "batch 5161: loss 0.006452\n",
      "batch 5162: loss 0.008130\n",
      "batch 5163: loss 0.039825\n",
      "batch 5164: loss 0.018124\n",
      "batch 5165: loss 0.048368\n",
      "batch 5166: loss 0.030624\n",
      "batch 5167: loss 0.022555\n",
      "batch 5168: loss 0.040804\n",
      "batch 5169: loss 0.061258\n",
      "batch 5170: loss 0.034694\n",
      "batch 5171: loss 0.026589\n",
      "batch 5172: loss 0.018564\n",
      "batch 5173: loss 0.025846\n",
      "batch 5174: loss 0.021216\n",
      "batch 5175: loss 0.036524\n",
      "batch 5176: loss 0.047807\n",
      "batch 5177: loss 0.029484\n",
      "batch 5178: loss 0.034246\n",
      "batch 5179: loss 0.066987\n",
      "batch 5180: loss 0.070275\n",
      "batch 5181: loss 0.073547\n",
      "batch 5182: loss 0.094254\n",
      "batch 5183: loss 0.055507\n",
      "batch 5184: loss 0.011136\n",
      "batch 5185: loss 0.025964\n",
      "batch 5186: loss 0.024444\n",
      "batch 5187: loss 0.151001\n",
      "batch 5188: loss 0.072291\n",
      "batch 5189: loss 0.076515\n",
      "batch 5190: loss 0.025103\n",
      "batch 5191: loss 0.142343\n",
      "batch 5192: loss 0.019054\n",
      "batch 5193: loss 0.046026\n",
      "batch 5194: loss 0.088375\n",
      "batch 5195: loss 0.025711\n",
      "batch 5196: loss 0.020247\n",
      "batch 5197: loss 0.047042\n",
      "batch 5198: loss 0.060678\n",
      "batch 5199: loss 0.083152\n",
      "batch 5200: loss 0.028506\n",
      "batch 5201: loss 0.059768\n",
      "batch 5202: loss 0.015389\n",
      "batch 5203: loss 0.068315\n",
      "batch 5204: loss 0.153949\n",
      "batch 5205: loss 0.037582\n",
      "batch 5206: loss 0.030150\n",
      "batch 5207: loss 0.046730\n",
      "batch 5208: loss 0.113101\n",
      "batch 5209: loss 0.103541\n",
      "batch 5210: loss 0.037110\n",
      "batch 5211: loss 0.023370\n",
      "batch 5212: loss 0.063180\n",
      "batch 5213: loss 0.022336\n",
      "batch 5214: loss 0.039941\n",
      "batch 5215: loss 0.037284\n",
      "batch 5216: loss 0.030281\n",
      "batch 5217: loss 0.044344\n",
      "batch 5218: loss 0.085505\n",
      "batch 5219: loss 0.083812\n",
      "batch 5220: loss 0.053565\n",
      "batch 5221: loss 0.040835\n",
      "batch 5222: loss 0.086450\n",
      "batch 5223: loss 0.033447\n",
      "batch 5224: loss 0.022344\n",
      "batch 5225: loss 0.050341\n",
      "batch 5226: loss 0.024342\n",
      "batch 5227: loss 0.010698\n",
      "batch 5228: loss 0.073565\n",
      "batch 5229: loss 0.038582\n",
      "batch 5230: loss 0.101615\n",
      "batch 5231: loss 0.071531\n",
      "batch 5232: loss 0.035674\n",
      "batch 5233: loss 0.030301\n",
      "batch 5234: loss 0.025975\n",
      "batch 5235: loss 0.027906\n",
      "batch 5236: loss 0.022945\n",
      "batch 5237: loss 0.036877\n",
      "batch 5238: loss 0.029472\n",
      "batch 5239: loss 0.034224\n",
      "batch 5240: loss 0.021661\n",
      "batch 5241: loss 0.070714\n",
      "batch 5242: loss 0.033516\n",
      "batch 5243: loss 0.010124\n",
      "batch 5244: loss 0.040798\n",
      "batch 5245: loss 0.029082\n",
      "batch 5246: loss 0.010420\n",
      "batch 5247: loss 0.049235\n",
      "batch 5248: loss 0.042784\n",
      "batch 5249: loss 0.086119\n",
      "batch 5250: loss 0.018717\n",
      "batch 5251: loss 0.017512\n",
      "batch 5252: loss 0.010639\n",
      "batch 5253: loss 0.136295\n",
      "batch 5254: loss 0.116075\n",
      "batch 5255: loss 0.021133\n",
      "batch 5256: loss 0.036483\n",
      "batch 5257: loss 0.116188\n",
      "batch 5258: loss 0.010696\n",
      "batch 5259: loss 0.006405\n",
      "batch 5260: loss 0.009466\n",
      "batch 5261: loss 0.012349\n",
      "batch 5262: loss 0.042646\n",
      "batch 5263: loss 0.092741\n",
      "batch 5264: loss 0.120314\n",
      "batch 5265: loss 0.149911\n",
      "batch 5266: loss 0.050753\n",
      "batch 5267: loss 0.098095\n",
      "batch 5268: loss 0.102377\n",
      "batch 5269: loss 0.027187\n",
      "batch 5270: loss 0.130113\n",
      "batch 5271: loss 0.009153\n",
      "batch 5272: loss 0.035377\n",
      "batch 5273: loss 0.182388\n",
      "batch 5274: loss 0.051111\n",
      "batch 5275: loss 0.006398\n",
      "batch 5276: loss 0.016939\n",
      "batch 5277: loss 0.039402\n",
      "batch 5278: loss 0.110570\n",
      "batch 5279: loss 0.165229\n",
      "batch 5280: loss 0.023092\n",
      "batch 5281: loss 0.117990\n",
      "batch 5282: loss 0.137791\n",
      "batch 5283: loss 0.119041\n",
      "batch 5284: loss 0.028353\n",
      "batch 5285: loss 0.054652\n",
      "batch 5286: loss 0.076650\n",
      "batch 5287: loss 0.191443\n",
      "batch 5288: loss 0.084194\n",
      "batch 5289: loss 0.145631\n",
      "batch 5290: loss 0.005269\n",
      "batch 5291: loss 0.072057\n",
      "batch 5292: loss 0.023363\n",
      "batch 5293: loss 0.122510\n",
      "batch 5294: loss 0.014229\n",
      "batch 5295: loss 0.020642\n",
      "batch 5296: loss 0.018270\n",
      "batch 5297: loss 0.055661\n",
      "batch 5298: loss 0.073868\n",
      "batch 5299: loss 0.016278\n",
      "batch 5300: loss 0.063052\n",
      "batch 5301: loss 0.135050\n",
      "batch 5302: loss 0.010943\n",
      "batch 5303: loss 0.050283\n",
      "batch 5304: loss 0.026025\n",
      "batch 5305: loss 0.042987\n",
      "batch 5306: loss 0.021729\n",
      "batch 5307: loss 0.078594\n",
      "batch 5308: loss 0.031997\n",
      "batch 5309: loss 0.040215\n",
      "batch 5310: loss 0.048403\n",
      "batch 5311: loss 0.065996\n",
      "batch 5312: loss 0.043742\n",
      "batch 5313: loss 0.008876\n",
      "batch 5314: loss 0.017719\n",
      "batch 5315: loss 0.028237\n",
      "batch 5316: loss 0.089706\n",
      "batch 5317: loss 0.100472\n",
      "batch 5318: loss 0.057022\n",
      "batch 5319: loss 0.103165\n",
      "batch 5320: loss 0.149431\n",
      "batch 5321: loss 0.006926\n",
      "batch 5322: loss 0.044366\n",
      "batch 5323: loss 0.050083\n",
      "batch 5324: loss 0.029337\n",
      "batch 5325: loss 0.021721\n",
      "batch 5326: loss 0.006409\n",
      "batch 5327: loss 0.029375\n",
      "batch 5328: loss 0.033760\n",
      "batch 5329: loss 0.049238\n",
      "batch 5330: loss 0.015219\n",
      "batch 5331: loss 0.144501\n",
      "batch 5332: loss 0.068503\n",
      "batch 5333: loss 0.193110\n",
      "batch 5334: loss 0.059179\n",
      "batch 5335: loss 0.044185\n",
      "batch 5336: loss 0.035219\n",
      "batch 5337: loss 0.036011\n",
      "batch 5338: loss 0.020639\n",
      "batch 5339: loss 0.014224\n",
      "batch 5340: loss 0.050520\n",
      "batch 5341: loss 0.046419\n",
      "batch 5342: loss 0.019239\n",
      "batch 5343: loss 0.196947\n",
      "batch 5344: loss 0.031602\n",
      "batch 5345: loss 0.072585\n",
      "batch 5346: loss 0.039356\n",
      "batch 5347: loss 0.083250\n",
      "batch 5348: loss 0.111393\n",
      "batch 5349: loss 0.020413\n",
      "batch 5350: loss 0.151929\n",
      "batch 5351: loss 0.016124\n",
      "batch 5352: loss 0.014266\n",
      "batch 5353: loss 0.080419\n",
      "batch 5354: loss 0.030691\n",
      "batch 5355: loss 0.031635\n",
      "batch 5356: loss 0.104330\n",
      "batch 5357: loss 0.135465\n",
      "batch 5358: loss 0.024643\n",
      "batch 5359: loss 0.024658\n",
      "batch 5360: loss 0.113412\n",
      "batch 5361: loss 0.055622\n",
      "batch 5362: loss 0.050693\n",
      "batch 5363: loss 0.084972\n",
      "batch 5364: loss 0.119654\n",
      "batch 5365: loss 0.042832\n",
      "batch 5366: loss 0.071360\n",
      "batch 5367: loss 0.014860\n",
      "batch 5368: loss 0.063192\n",
      "batch 5369: loss 0.075630\n",
      "batch 5370: loss 0.035909\n",
      "batch 5371: loss 0.023287\n",
      "batch 5372: loss 0.116059\n",
      "batch 5373: loss 0.074726\n",
      "batch 5374: loss 0.031934\n",
      "batch 5375: loss 0.120444\n",
      "batch 5376: loss 0.011955\n",
      "batch 5377: loss 0.014186\n",
      "batch 5378: loss 0.099741\n",
      "batch 5379: loss 0.004897\n",
      "batch 5380: loss 0.010641\n",
      "batch 5381: loss 0.014107\n",
      "batch 5382: loss 0.028715\n",
      "batch 5383: loss 0.014615\n",
      "batch 5384: loss 0.029234\n",
      "batch 5385: loss 0.019023\n",
      "batch 5386: loss 0.055487\n",
      "batch 5387: loss 0.019036\n",
      "batch 5388: loss 0.074054\n",
      "batch 5389: loss 0.042711\n",
      "batch 5390: loss 0.039963\n",
      "batch 5391: loss 0.025399\n",
      "batch 5392: loss 0.110025\n",
      "batch 5393: loss 0.023159\n",
      "batch 5394: loss 0.118104\n",
      "batch 5395: loss 0.012929\n",
      "batch 5396: loss 0.011603\n",
      "batch 5397: loss 0.077447\n",
      "batch 5398: loss 0.006754\n",
      "batch 5399: loss 0.046225\n",
      "batch 5400: loss 0.117934\n",
      "batch 5401: loss 0.015840\n",
      "batch 5402: loss 0.062638\n",
      "batch 5403: loss 0.104446\n",
      "batch 5404: loss 0.007471\n",
      "batch 5405: loss 0.048079\n",
      "batch 5406: loss 0.012944\n",
      "batch 5407: loss 0.042761\n",
      "batch 5408: loss 0.033536\n",
      "batch 5409: loss 0.115931\n",
      "batch 5410: loss 0.065972\n",
      "batch 5411: loss 0.056849\n",
      "batch 5412: loss 0.089963\n",
      "batch 5413: loss 0.063138\n",
      "batch 5414: loss 0.228390\n",
      "batch 5415: loss 0.188224\n",
      "batch 5416: loss 0.090242\n",
      "batch 5417: loss 0.122834\n",
      "batch 5418: loss 0.040758\n",
      "batch 5419: loss 0.033286\n",
      "batch 5420: loss 0.007738\n",
      "batch 5421: loss 0.027252\n",
      "batch 5422: loss 0.021579\n",
      "batch 5423: loss 0.080928\n",
      "batch 5424: loss 0.110241\n",
      "batch 5425: loss 0.062937\n",
      "batch 5426: loss 0.012008\n",
      "batch 5427: loss 0.031971\n",
      "batch 5428: loss 0.022754\n",
      "batch 5429: loss 0.139663\n",
      "batch 5430: loss 0.032217\n",
      "batch 5431: loss 0.049515\n",
      "batch 5432: loss 0.026959\n",
      "batch 5433: loss 0.031154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5434: loss 0.005399\n",
      "batch 5435: loss 0.140274\n",
      "batch 5436: loss 0.040588\n",
      "batch 5437: loss 0.130424\n",
      "batch 5438: loss 0.020335\n",
      "batch 5439: loss 0.005546\n",
      "batch 5440: loss 0.026691\n",
      "batch 5441: loss 0.028415\n",
      "batch 5442: loss 0.072437\n",
      "batch 5443: loss 0.042467\n",
      "batch 5444: loss 0.036198\n",
      "batch 5445: loss 0.015582\n",
      "batch 5446: loss 0.016076\n",
      "batch 5447: loss 0.038996\n",
      "batch 5448: loss 0.075612\n",
      "batch 5449: loss 0.032558\n",
      "batch 5450: loss 0.036463\n",
      "batch 5451: loss 0.081481\n",
      "batch 5452: loss 0.023301\n",
      "batch 5453: loss 0.024229\n",
      "batch 5454: loss 0.122796\n",
      "batch 5455: loss 0.009106\n",
      "batch 5456: loss 0.033761\n",
      "batch 5457: loss 0.119223\n",
      "batch 5458: loss 0.012178\n",
      "batch 5459: loss 0.049769\n",
      "batch 5460: loss 0.022803\n",
      "batch 5461: loss 0.049132\n",
      "batch 5462: loss 0.037522\n",
      "batch 5463: loss 0.034101\n",
      "batch 5464: loss 0.060980\n",
      "batch 5465: loss 0.098946\n",
      "batch 5466: loss 0.083074\n",
      "batch 5467: loss 0.015809\n",
      "batch 5468: loss 0.166464\n",
      "batch 5469: loss 0.258132\n",
      "batch 5470: loss 0.048449\n",
      "batch 5471: loss 0.249369\n",
      "batch 5472: loss 0.068150\n",
      "batch 5473: loss 0.006408\n",
      "batch 5474: loss 0.041037\n",
      "batch 5475: loss 0.100025\n",
      "batch 5476: loss 0.012125\n",
      "batch 5477: loss 0.078238\n",
      "batch 5478: loss 0.025795\n",
      "batch 5479: loss 0.027435\n",
      "batch 5480: loss 0.026708\n",
      "batch 5481: loss 0.043075\n",
      "batch 5482: loss 0.078540\n",
      "batch 5483: loss 0.008036\n",
      "batch 5484: loss 0.069309\n",
      "batch 5485: loss 0.050098\n",
      "batch 5486: loss 0.028572\n",
      "batch 5487: loss 0.024285\n",
      "batch 5488: loss 0.036337\n",
      "batch 5489: loss 0.072325\n",
      "batch 5490: loss 0.086866\n",
      "batch 5491: loss 0.046079\n",
      "batch 5492: loss 0.084956\n",
      "batch 5493: loss 0.029513\n",
      "batch 5494: loss 0.031698\n",
      "batch 5495: loss 0.054186\n",
      "batch 5496: loss 0.042815\n",
      "batch 5497: loss 0.044789\n",
      "batch 5498: loss 0.078948\n",
      "batch 5499: loss 0.062055\n",
      "batch 5500: loss 0.016266\n",
      "batch 5501: loss 0.017741\n",
      "batch 5502: loss 0.016037\n",
      "batch 5503: loss 0.058916\n",
      "batch 5504: loss 0.098606\n",
      "batch 5505: loss 0.016369\n",
      "batch 5506: loss 0.018761\n",
      "batch 5507: loss 0.052635\n",
      "batch 5508: loss 0.030067\n",
      "batch 5509: loss 0.028282\n",
      "batch 5510: loss 0.033887\n",
      "batch 5511: loss 0.041688\n",
      "batch 5512: loss 0.047645\n",
      "batch 5513: loss 0.031716\n",
      "batch 5514: loss 0.008573\n",
      "batch 5515: loss 0.028340\n",
      "batch 5516: loss 0.106300\n",
      "batch 5517: loss 0.025216\n",
      "batch 5518: loss 0.046782\n",
      "batch 5519: loss 0.026209\n",
      "batch 5520: loss 0.026595\n",
      "batch 5521: loss 0.053916\n",
      "batch 5522: loss 0.066398\n",
      "batch 5523: loss 0.018467\n",
      "batch 5524: loss 0.032397\n",
      "batch 5525: loss 0.010694\n",
      "batch 5526: loss 0.009075\n",
      "batch 5527: loss 0.084730\n",
      "batch 5528: loss 0.057308\n",
      "batch 5529: loss 0.077403\n",
      "batch 5530: loss 0.069425\n",
      "batch 5531: loss 0.040863\n",
      "batch 5532: loss 0.018793\n",
      "batch 5533: loss 0.011694\n",
      "batch 5534: loss 0.096401\n",
      "batch 5535: loss 0.019291\n",
      "batch 5536: loss 0.088850\n",
      "batch 5537: loss 0.019267\n",
      "batch 5538: loss 0.021088\n",
      "batch 5539: loss 0.011405\n",
      "batch 5540: loss 0.007621\n",
      "batch 5541: loss 0.037914\n",
      "batch 5542: loss 0.028446\n",
      "batch 5543: loss 0.060744\n",
      "batch 5544: loss 0.015883\n",
      "batch 5545: loss 0.015508\n",
      "batch 5546: loss 0.022458\n",
      "batch 5547: loss 0.093512\n",
      "batch 5548: loss 0.005931\n",
      "batch 5549: loss 0.061132\n",
      "batch 5550: loss 0.032107\n",
      "batch 5551: loss 0.047860\n",
      "batch 5552: loss 0.069561\n",
      "batch 5553: loss 0.094381\n",
      "batch 5554: loss 0.130422\n",
      "batch 5555: loss 0.004085\n",
      "batch 5556: loss 0.037488\n",
      "batch 5557: loss 0.028123\n",
      "batch 5558: loss 0.024274\n",
      "batch 5559: loss 0.056039\n",
      "batch 5560: loss 0.050221\n",
      "batch 5561: loss 0.071039\n",
      "batch 5562: loss 0.017876\n",
      "batch 5563: loss 0.030638\n",
      "batch 5564: loss 0.010191\n",
      "batch 5565: loss 0.022607\n",
      "batch 5566: loss 0.004947\n",
      "batch 5567: loss 0.022130\n",
      "batch 5568: loss 0.050953\n",
      "batch 5569: loss 0.097155\n",
      "batch 5570: loss 0.027029\n",
      "batch 5571: loss 0.016621\n",
      "batch 5572: loss 0.132372\n",
      "batch 5573: loss 0.016403\n",
      "batch 5574: loss 0.024480\n",
      "batch 5575: loss 0.027890\n",
      "batch 5576: loss 0.025059\n",
      "batch 5577: loss 0.007583\n",
      "batch 5578: loss 0.097097\n",
      "batch 5579: loss 0.025264\n",
      "batch 5580: loss 0.008347\n",
      "batch 5581: loss 0.043806\n",
      "batch 5582: loss 0.051284\n",
      "batch 5583: loss 0.008373\n",
      "batch 5584: loss 0.024947\n",
      "batch 5585: loss 0.016110\n",
      "batch 5586: loss 0.004922\n",
      "batch 5587: loss 0.020653\n",
      "batch 5588: loss 0.013452\n",
      "batch 5589: loss 0.106705\n",
      "batch 5590: loss 0.012290\n",
      "batch 5591: loss 0.010766\n",
      "batch 5592: loss 0.051578\n",
      "batch 5593: loss 0.027008\n",
      "batch 5594: loss 0.007353\n",
      "batch 5595: loss 0.004627\n",
      "batch 5596: loss 0.044255\n",
      "batch 5597: loss 0.047941\n",
      "batch 5598: loss 0.017026\n",
      "batch 5599: loss 0.055377\n",
      "batch 5600: loss 0.039314\n",
      "batch 5601: loss 0.019506\n",
      "batch 5602: loss 0.162943\n",
      "batch 5603: loss 0.109005\n",
      "batch 5604: loss 0.079022\n",
      "batch 5605: loss 0.018707\n",
      "batch 5606: loss 0.017371\n",
      "batch 5607: loss 0.062624\n",
      "batch 5608: loss 0.009089\n",
      "batch 5609: loss 0.004691\n",
      "batch 5610: loss 0.097667\n",
      "batch 5611: loss 0.006197\n",
      "batch 5612: loss 0.021521\n",
      "batch 5613: loss 0.071255\n",
      "batch 5614: loss 0.020145\n",
      "batch 5615: loss 0.019663\n",
      "batch 5616: loss 0.124262\n",
      "batch 5617: loss 0.038113\n",
      "batch 5618: loss 0.054435\n",
      "batch 5619: loss 0.038656\n",
      "batch 5620: loss 0.126213\n",
      "batch 5621: loss 0.030609\n",
      "batch 5622: loss 0.009934\n",
      "batch 5623: loss 0.067605\n",
      "batch 5624: loss 0.034242\n",
      "batch 5625: loss 0.093976\n",
      "batch 5626: loss 0.029989\n",
      "batch 5627: loss 0.040151\n",
      "batch 5628: loss 0.022249\n",
      "batch 5629: loss 0.044641\n",
      "batch 5630: loss 0.043396\n",
      "batch 5631: loss 0.013080\n",
      "batch 5632: loss 0.009605\n",
      "batch 5633: loss 0.022337\n",
      "batch 5634: loss 0.045420\n",
      "batch 5635: loss 0.024144\n",
      "batch 5636: loss 0.020029\n",
      "batch 5637: loss 0.042156\n",
      "batch 5638: loss 0.139611\n",
      "batch 5639: loss 0.005683\n",
      "batch 5640: loss 0.260715\n",
      "batch 5641: loss 0.036163\n",
      "batch 5642: loss 0.060070\n",
      "batch 5643: loss 0.017256\n",
      "batch 5644: loss 0.021069\n",
      "batch 5645: loss 0.033567\n",
      "batch 5646: loss 0.013850\n",
      "batch 5647: loss 0.012845\n",
      "batch 5648: loss 0.006994\n",
      "batch 5649: loss 0.012930\n",
      "batch 5650: loss 0.007783\n",
      "batch 5651: loss 0.171614\n",
      "batch 5652: loss 0.052908\n",
      "batch 5653: loss 0.010096\n",
      "batch 5654: loss 0.119206\n",
      "batch 5655: loss 0.031369\n",
      "batch 5656: loss 0.010756\n",
      "batch 5657: loss 0.013014\n",
      "batch 5658: loss 0.018086\n",
      "batch 5659: loss 0.040459\n",
      "batch 5660: loss 0.007571\n",
      "batch 5661: loss 0.008662\n",
      "batch 5662: loss 0.017692\n",
      "batch 5663: loss 0.017417\n",
      "batch 5664: loss 0.035623\n",
      "batch 5665: loss 0.093580\n",
      "batch 5666: loss 0.012622\n",
      "batch 5667: loss 0.063341\n",
      "batch 5668: loss 0.047747\n",
      "batch 5669: loss 0.009718\n",
      "batch 5670: loss 0.008368\n",
      "batch 5671: loss 0.060675\n",
      "batch 5672: loss 0.075378\n",
      "batch 5673: loss 0.103423\n",
      "batch 5674: loss 0.024382\n",
      "batch 5675: loss 0.087578\n",
      "batch 5676: loss 0.037275\n",
      "batch 5677: loss 0.119436\n",
      "batch 5678: loss 0.006388\n",
      "batch 5679: loss 0.072728\n",
      "batch 5680: loss 0.102873\n",
      "batch 5681: loss 0.221834\n",
      "batch 5682: loss 0.052115\n",
      "batch 5683: loss 0.038862\n",
      "batch 5684: loss 0.061871\n",
      "batch 5685: loss 0.036044\n",
      "batch 5686: loss 0.014537\n",
      "batch 5687: loss 0.044655\n",
      "batch 5688: loss 0.011895\n",
      "batch 5689: loss 0.071768\n",
      "batch 5690: loss 0.009638\n",
      "batch 5691: loss 0.037959\n",
      "batch 5692: loss 0.033388\n",
      "batch 5693: loss 0.022380\n",
      "batch 5694: loss 0.046564\n",
      "batch 5695: loss 0.074523\n",
      "batch 5696: loss 0.177185\n",
      "batch 5697: loss 0.030800\n",
      "batch 5698: loss 0.051170\n",
      "batch 5699: loss 0.079978\n",
      "batch 5700: loss 0.013987\n",
      "batch 5701: loss 0.088716\n",
      "batch 5702: loss 0.010126\n",
      "batch 5703: loss 0.014855\n",
      "batch 5704: loss 0.066380\n",
      "batch 5705: loss 0.021055\n",
      "batch 5706: loss 0.171798\n",
      "batch 5707: loss 0.129275\n",
      "batch 5708: loss 0.032459\n",
      "batch 5709: loss 0.026996\n",
      "batch 5710: loss 0.064226\n",
      "batch 5711: loss 0.046155\n",
      "batch 5712: loss 0.087512\n",
      "batch 5713: loss 0.025878\n",
      "batch 5714: loss 0.093018\n",
      "batch 5715: loss 0.032394\n",
      "batch 5716: loss 0.007067\n",
      "batch 5717: loss 0.042463\n",
      "batch 5718: loss 0.012545\n",
      "batch 5719: loss 0.178329\n",
      "batch 5720: loss 0.059658\n",
      "batch 5721: loss 0.027380\n",
      "batch 5722: loss 0.064705\n",
      "batch 5723: loss 0.061337\n",
      "batch 5724: loss 0.033354\n",
      "batch 5725: loss 0.056159\n",
      "batch 5726: loss 0.134973\n",
      "batch 5727: loss 0.009624\n",
      "batch 5728: loss 0.151740\n",
      "batch 5729: loss 0.035352\n",
      "batch 5730: loss 0.036092\n",
      "batch 5731: loss 0.049823\n",
      "batch 5732: loss 0.184388\n",
      "batch 5733: loss 0.060783\n",
      "batch 5734: loss 0.106930\n",
      "batch 5735: loss 0.052667\n",
      "batch 5736: loss 0.016736\n",
      "batch 5737: loss 0.015165\n",
      "batch 5738: loss 0.009469\n",
      "batch 5739: loss 0.125137\n",
      "batch 5740: loss 0.006011\n",
      "batch 5741: loss 0.043026\n",
      "batch 5742: loss 0.014218\n",
      "batch 5743: loss 0.081074\n",
      "batch 5744: loss 0.041899\n",
      "batch 5745: loss 0.136304\n",
      "batch 5746: loss 0.050498\n",
      "batch 5747: loss 0.051257\n",
      "batch 5748: loss 0.034382\n",
      "batch 5749: loss 0.010967\n",
      "batch 5750: loss 0.036295\n",
      "batch 5751: loss 0.135059\n",
      "batch 5752: loss 0.030383\n",
      "batch 5753: loss 0.017425\n",
      "batch 5754: loss 0.025335\n",
      "batch 5755: loss 0.097497\n",
      "batch 5756: loss 0.139470\n",
      "batch 5757: loss 0.050160\n",
      "batch 5758: loss 0.022631\n",
      "batch 5759: loss 0.033909\n",
      "batch 5760: loss 0.021862\n",
      "batch 5761: loss 0.048538\n",
      "batch 5762: loss 0.030531\n",
      "batch 5763: loss 0.135206\n",
      "batch 5764: loss 0.027458\n",
      "batch 5765: loss 0.030129\n",
      "batch 5766: loss 0.102356\n",
      "batch 5767: loss 0.134453\n",
      "batch 5768: loss 0.011702\n",
      "batch 5769: loss 0.023042\n",
      "batch 5770: loss 0.029860\n",
      "batch 5771: loss 0.088296\n",
      "batch 5772: loss 0.031956\n",
      "batch 5773: loss 0.021335\n",
      "batch 5774: loss 0.013800\n",
      "batch 5775: loss 0.040668\n",
      "batch 5776: loss 0.091231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5777: loss 0.104267\n",
      "batch 5778: loss 0.119306\n",
      "batch 5779: loss 0.129082\n",
      "batch 5780: loss 0.028040\n",
      "batch 5781: loss 0.145208\n",
      "batch 5782: loss 0.037131\n",
      "batch 5783: loss 0.073354\n",
      "batch 5784: loss 0.078038\n",
      "batch 5785: loss 0.065277\n",
      "batch 5786: loss 0.071798\n",
      "batch 5787: loss 0.083576\n",
      "batch 5788: loss 0.069607\n",
      "batch 5789: loss 0.031996\n",
      "batch 5790: loss 0.032126\n",
      "batch 5791: loss 0.084191\n",
      "batch 5792: loss 0.016723\n",
      "batch 5793: loss 0.038692\n",
      "batch 5794: loss 0.026809\n",
      "batch 5795: loss 0.070674\n",
      "batch 5796: loss 0.010927\n",
      "batch 5797: loss 0.033127\n",
      "batch 5798: loss 0.011371\n",
      "batch 5799: loss 0.013495\n",
      "batch 5800: loss 0.027510\n",
      "batch 5801: loss 0.064980\n",
      "batch 5802: loss 0.028854\n",
      "batch 5803: loss 0.031969\n",
      "batch 5804: loss 0.029882\n",
      "batch 5805: loss 0.009789\n",
      "batch 5806: loss 0.047843\n",
      "batch 5807: loss 0.071522\n",
      "batch 5808: loss 0.025624\n",
      "batch 5809: loss 0.035372\n",
      "batch 5810: loss 0.016835\n",
      "batch 5811: loss 0.049969\n",
      "batch 5812: loss 0.017545\n",
      "batch 5813: loss 0.033383\n",
      "batch 5814: loss 0.209501\n",
      "batch 5815: loss 0.032177\n",
      "batch 5816: loss 0.084891\n",
      "batch 5817: loss 0.017185\n",
      "batch 5818: loss 0.066531\n",
      "batch 5819: loss 0.041121\n",
      "batch 5820: loss 0.011832\n",
      "batch 5821: loss 0.053503\n",
      "batch 5822: loss 0.053958\n",
      "batch 5823: loss 0.007897\n",
      "batch 5824: loss 0.069307\n",
      "batch 5825: loss 0.034334\n",
      "batch 5826: loss 0.013949\n",
      "batch 5827: loss 0.031798\n",
      "batch 5828: loss 0.029750\n",
      "batch 5829: loss 0.042269\n",
      "batch 5830: loss 0.006097\n",
      "batch 5831: loss 0.072813\n",
      "batch 5832: loss 0.067032\n",
      "batch 5833: loss 0.042312\n",
      "batch 5834: loss 0.016861\n",
      "batch 5835: loss 0.041228\n",
      "batch 5836: loss 0.325086\n",
      "batch 5837: loss 0.051885\n",
      "batch 5838: loss 0.006480\n",
      "batch 5839: loss 0.002388\n",
      "batch 5840: loss 0.096476\n",
      "batch 5841: loss 0.029651\n",
      "batch 5842: loss 0.090278\n",
      "batch 5843: loss 0.018295\n",
      "batch 5844: loss 0.016001\n",
      "batch 5845: loss 0.023846\n",
      "batch 5846: loss 0.031253\n",
      "batch 5847: loss 0.030183\n",
      "batch 5848: loss 0.034523\n",
      "batch 5849: loss 0.111710\n",
      "batch 5850: loss 0.130675\n",
      "batch 5851: loss 0.003311\n",
      "batch 5852: loss 0.055261\n",
      "batch 5853: loss 0.015876\n",
      "batch 5854: loss 0.036093\n",
      "batch 5855: loss 0.006008\n",
      "batch 5856: loss 0.011545\n",
      "batch 5857: loss 0.003986\n",
      "batch 5858: loss 0.027954\n",
      "batch 5859: loss 0.054942\n",
      "batch 5860: loss 0.024651\n",
      "batch 5861: loss 0.033160\n",
      "batch 5862: loss 0.160352\n",
      "batch 5863: loss 0.082523\n",
      "batch 5864: loss 0.045956\n",
      "batch 5865: loss 0.014795\n",
      "batch 5866: loss 0.026628\n",
      "batch 5867: loss 0.129820\n",
      "batch 5868: loss 0.053823\n",
      "batch 5869: loss 0.055491\n",
      "batch 5870: loss 0.021246\n",
      "batch 5871: loss 0.025549\n",
      "batch 5872: loss 0.098679\n",
      "batch 5873: loss 0.161890\n",
      "batch 5874: loss 0.026496\n",
      "batch 5875: loss 0.198603\n",
      "batch 5876: loss 0.022765\n",
      "batch 5877: loss 0.077319\n",
      "batch 5878: loss 0.029982\n",
      "batch 5879: loss 0.035036\n",
      "batch 5880: loss 0.012781\n",
      "batch 5881: loss 0.024423\n",
      "batch 5882: loss 0.100255\n",
      "batch 5883: loss 0.038094\n",
      "batch 5884: loss 0.125369\n",
      "batch 5885: loss 0.020786\n",
      "batch 5886: loss 0.025445\n",
      "batch 5887: loss 0.036806\n",
      "batch 5888: loss 0.072845\n",
      "batch 5889: loss 0.013740\n",
      "batch 5890: loss 0.009755\n",
      "batch 5891: loss 0.022244\n",
      "batch 5892: loss 0.040071\n",
      "batch 5893: loss 0.067621\n",
      "batch 5894: loss 0.016720\n",
      "batch 5895: loss 0.074857\n",
      "batch 5896: loss 0.060262\n",
      "batch 5897: loss 0.030528\n",
      "batch 5898: loss 0.052703\n",
      "batch 5899: loss 0.010866\n",
      "batch 5900: loss 0.018398\n",
      "batch 5901: loss 0.049168\n",
      "batch 5902: loss 0.057150\n",
      "batch 5903: loss 0.035200\n",
      "batch 5904: loss 0.022892\n",
      "batch 5905: loss 0.026875\n",
      "batch 5906: loss 0.013704\n",
      "batch 5907: loss 0.072464\n",
      "batch 5908: loss 0.028752\n",
      "batch 5909: loss 0.052799\n",
      "batch 5910: loss 0.019009\n",
      "batch 5911: loss 0.012539\n",
      "batch 5912: loss 0.029621\n",
      "batch 5913: loss 0.112438\n",
      "batch 5914: loss 0.211546\n",
      "batch 5915: loss 0.057980\n",
      "batch 5916: loss 0.049926\n",
      "batch 5917: loss 0.023009\n",
      "batch 5918: loss 0.016927\n",
      "batch 5919: loss 0.016040\n",
      "batch 5920: loss 0.007091\n",
      "batch 5921: loss 0.068683\n",
      "batch 5922: loss 0.004011\n",
      "batch 5923: loss 0.027995\n",
      "batch 5924: loss 0.080094\n",
      "batch 5925: loss 0.008226\n",
      "batch 5926: loss 0.094384\n",
      "batch 5927: loss 0.014932\n",
      "batch 5928: loss 0.030764\n",
      "batch 5929: loss 0.019127\n",
      "batch 5930: loss 0.075165\n",
      "batch 5931: loss 0.030992\n",
      "batch 5932: loss 0.142513\n",
      "batch 5933: loss 0.068991\n",
      "batch 5934: loss 0.012148\n",
      "batch 5935: loss 0.075386\n",
      "batch 5936: loss 0.009635\n",
      "batch 5937: loss 0.111574\n",
      "batch 5938: loss 0.105889\n",
      "batch 5939: loss 0.092912\n",
      "batch 5940: loss 0.024704\n",
      "batch 5941: loss 0.181045\n",
      "batch 5942: loss 0.025712\n",
      "batch 5943: loss 0.084548\n",
      "batch 5944: loss 0.122203\n",
      "batch 5945: loss 0.010300\n",
      "batch 5946: loss 0.063238\n",
      "batch 5947: loss 0.028999\n",
      "batch 5948: loss 0.082935\n",
      "batch 5949: loss 0.023578\n",
      "batch 5950: loss 0.025014\n",
      "batch 5951: loss 0.039594\n",
      "batch 5952: loss 0.131440\n",
      "batch 5953: loss 0.026170\n",
      "batch 5954: loss 0.074196\n",
      "batch 5955: loss 0.032907\n",
      "batch 5956: loss 0.026549\n",
      "batch 5957: loss 0.039334\n",
      "batch 5958: loss 0.116926\n",
      "batch 5959: loss 0.027771\n",
      "batch 5960: loss 0.007349\n",
      "batch 5961: loss 0.040462\n",
      "batch 5962: loss 0.022648\n",
      "batch 5963: loss 0.052029\n",
      "batch 5964: loss 0.069582\n",
      "batch 5965: loss 0.016692\n",
      "batch 5966: loss 0.077419\n",
      "batch 5967: loss 0.013488\n",
      "batch 5968: loss 0.044979\n",
      "batch 5969: loss 0.012979\n",
      "batch 5970: loss 0.084278\n",
      "batch 5971: loss 0.009005\n",
      "batch 5972: loss 0.073622\n",
      "batch 5973: loss 0.055542\n",
      "batch 5974: loss 0.088729\n",
      "batch 5975: loss 0.020893\n",
      "batch 5976: loss 0.012050\n",
      "batch 5977: loss 0.079281\n",
      "batch 5978: loss 0.109306\n",
      "batch 5979: loss 0.024978\n",
      "batch 5980: loss 0.083228\n",
      "batch 5981: loss 0.113672\n",
      "batch 5982: loss 0.122051\n",
      "batch 5983: loss 0.126540\n",
      "batch 5984: loss 0.011215\n",
      "batch 5985: loss 0.017730\n",
      "batch 5986: loss 0.079757\n",
      "batch 5987: loss 0.060469\n",
      "batch 5988: loss 0.107954\n",
      "batch 5989: loss 0.013342\n",
      "batch 5990: loss 0.036553\n",
      "batch 5991: loss 0.105831\n",
      "batch 5992: loss 0.032174\n",
      "batch 5993: loss 0.063141\n",
      "batch 5994: loss 0.017080\n",
      "batch 5995: loss 0.045057\n",
      "batch 5996: loss 0.053986\n",
      "batch 5997: loss 0.015046\n",
      "batch 5998: loss 0.017398\n",
      "batch 5999: loss 0.036781\n",
      "test accuracy: 0.973000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    \n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.L1(0.01))\n",
    "#        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.L2(0.01))\n",
    "\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上正則向(keras版本)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)   (60000,)\n",
      "(10000, 784)   (10000,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "165/165 - 1s - loss: 12.2978 - accuracy: 0.7207 - val_loss: 8.7443 - val_accuracy: 0.8365\n",
      "Epoch 2/100\n",
      "165/165 - 0s - loss: 7.6594 - accuracy: 0.8703 - val_loss: 6.6998 - val_accuracy: 0.8843\n",
      "Epoch 3/100\n",
      "165/165 - 0s - loss: 5.8720 - accuracy: 0.9081 - val_loss: 5.2062 - val_accuracy: 0.9046\n",
      "Epoch 4/100\n",
      "165/165 - 0s - loss: 4.5586 - accuracy: 0.9265 - val_loss: 4.0508 - val_accuracy: 0.9223\n",
      "Epoch 5/100\n",
      "165/165 - 0s - loss: 3.5584 - accuracy: 0.9383 - val_loss: 3.1832 - val_accuracy: 0.9312\n",
      "Epoch 6/100\n",
      "165/165 - 0s - loss: 2.8015 - accuracy: 0.9464 - val_loss: 2.5239 - val_accuracy: 0.9357\n",
      "Epoch 7/100\n",
      "165/165 - 0s - loss: 2.2219 - accuracy: 0.9496 - val_loss: 2.0048 - val_accuracy: 0.9428\n",
      "Epoch 8/100\n",
      "165/165 - 0s - loss: 1.7787 - accuracy: 0.9540 - val_loss: 1.6279 - val_accuracy: 0.9443\n",
      "Epoch 9/100\n",
      "165/165 - 0s - loss: 1.4565 - accuracy: 0.9545 - val_loss: 1.3506 - val_accuracy: 0.9443\n",
      "Epoch 10/100\n",
      "165/165 - 0s - loss: 1.2055 - accuracy: 0.9572 - val_loss: 1.1298 - val_accuracy: 0.9477\n",
      "Epoch 11/100\n",
      "165/165 - 0s - loss: 1.0117 - accuracy: 0.9599 - val_loss: 0.9549 - val_accuracy: 0.9518\n",
      "Epoch 12/100\n",
      "165/165 - 0s - loss: 0.8632 - accuracy: 0.9606 - val_loss: 0.8330 - val_accuracy: 0.9501\n",
      "Epoch 13/100\n",
      "165/165 - 0s - loss: 0.7491 - accuracy: 0.9620 - val_loss: 0.7312 - val_accuracy: 0.9506\n",
      "Epoch 14/100\n",
      "165/165 - 0s - loss: 0.6549 - accuracy: 0.9630 - val_loss: 0.6464 - val_accuracy: 0.9526\n",
      "Epoch 15/100\n",
      "165/165 - 0s - loss: 0.5811 - accuracy: 0.9644 - val_loss: 0.5861 - val_accuracy: 0.9514\n",
      "Epoch 16/100\n",
      "165/165 - 0s - loss: 0.5270 - accuracy: 0.9646 - val_loss: 0.5364 - val_accuracy: 0.9549\n",
      "Epoch 17/100\n",
      "165/165 - 0s - loss: 0.4844 - accuracy: 0.9670 - val_loss: 0.4952 - val_accuracy: 0.9551\n",
      "Epoch 18/100\n",
      "165/165 - 0s - loss: 0.4452 - accuracy: 0.9675 - val_loss: 0.4638 - val_accuracy: 0.9558\n",
      "Epoch 19/100\n",
      "165/165 - 0s - loss: 0.4170 - accuracy: 0.9676 - val_loss: 0.4460 - val_accuracy: 0.9542\n",
      "Epoch 20/100\n",
      "165/165 - 0s - loss: 0.3939 - accuracy: 0.9691 - val_loss: 0.4160 - val_accuracy: 0.9576\n",
      "Epoch 21/100\n",
      "165/165 - 0s - loss: 0.3739 - accuracy: 0.9695 - val_loss: 0.3992 - val_accuracy: 0.9582\n",
      "Epoch 22/100\n",
      "165/165 - 0s - loss: 0.3562 - accuracy: 0.9717 - val_loss: 0.3864 - val_accuracy: 0.9589\n",
      "Epoch 23/100\n",
      "165/165 - 0s - loss: 0.3399 - accuracy: 0.9727 - val_loss: 0.3767 - val_accuracy: 0.9583\n",
      "Epoch 24/100\n",
      "165/165 - 0s - loss: 0.3284 - accuracy: 0.9726 - val_loss: 0.3626 - val_accuracy: 0.9589\n",
      "Epoch 25/100\n",
      "165/165 - 0s - loss: 0.3158 - accuracy: 0.9740 - val_loss: 0.3645 - val_accuracy: 0.9560\n",
      "Epoch 26/100\n",
      "165/165 - 0s - loss: 0.3073 - accuracy: 0.9735 - val_loss: 0.3428 - val_accuracy: 0.9618\n",
      "Epoch 27/100\n",
      "165/165 - 0s - loss: 0.2948 - accuracy: 0.9760 - val_loss: 0.3377 - val_accuracy: 0.9608\n",
      "Epoch 28/100\n",
      "165/165 - 0s - loss: 0.2855 - accuracy: 0.9761 - val_loss: 0.3230 - val_accuracy: 0.9628\n",
      "Epoch 29/100\n",
      "165/165 - 0s - loss: 0.2786 - accuracy: 0.9760 - val_loss: 0.3200 - val_accuracy: 0.9614\n",
      "Epoch 30/100\n",
      "165/165 - 0s - loss: 0.2732 - accuracy: 0.9766 - val_loss: 0.3294 - val_accuracy: 0.9557\n",
      "Epoch 31/100\n",
      "165/165 - 0s - loss: 0.2654 - accuracy: 0.9776 - val_loss: 0.3270 - val_accuracy: 0.9564\n",
      "Epoch 32/100\n",
      "165/165 - 0s - loss: 0.2587 - accuracy: 0.9777 - val_loss: 0.3062 - val_accuracy: 0.9615\n",
      "Epoch 33/100\n",
      "165/165 - 0s - loss: 0.2516 - accuracy: 0.9787 - val_loss: 0.3104 - val_accuracy: 0.9573\n",
      "Epoch 34/100\n",
      "165/165 - 0s - loss: 0.2467 - accuracy: 0.9789 - val_loss: 0.3067 - val_accuracy: 0.9587\n",
      "Epoch 35/100\n",
      "165/165 - 0s - loss: 0.2407 - accuracy: 0.9796 - val_loss: 0.2972 - val_accuracy: 0.9603\n",
      "Epoch 36/100\n",
      "165/165 - 0s - loss: 0.2361 - accuracy: 0.9800 - val_loss: 0.2927 - val_accuracy: 0.9626\n",
      "Epoch 37/100\n",
      "165/165 - 0s - loss: 0.2320 - accuracy: 0.9800 - val_loss: 0.2918 - val_accuracy: 0.9602\n",
      "Epoch 38/100\n",
      "165/165 - 0s - loss: 0.2281 - accuracy: 0.9804 - val_loss: 0.2853 - val_accuracy: 0.9614\n",
      "Epoch 39/100\n",
      "165/165 - 0s - loss: 0.2249 - accuracy: 0.9805 - val_loss: 0.2826 - val_accuracy: 0.9623\n",
      "Epoch 40/100\n",
      "165/165 - 0s - loss: 0.2144 - accuracy: 0.9822 - val_loss: 0.2748 - val_accuracy: 0.9630\n",
      "Epoch 41/100\n",
      "165/165 - 0s - loss: 0.2103 - accuracy: 0.9823 - val_loss: 0.2719 - val_accuracy: 0.9629\n",
      "Epoch 42/100\n",
      "165/165 - 0s - loss: 0.2075 - accuracy: 0.9823 - val_loss: 0.2755 - val_accuracy: 0.9626\n",
      "Epoch 43/100\n",
      "165/165 - 0s - loss: 0.2043 - accuracy: 0.9830 - val_loss: 0.2736 - val_accuracy: 0.9612\n",
      "Epoch 44/100\n",
      "165/165 - 0s - loss: 0.2003 - accuracy: 0.9840 - val_loss: 0.2719 - val_accuracy: 0.9625\n",
      "Epoch 45/100\n",
      "165/165 - 0s - loss: 0.1965 - accuracy: 0.9853 - val_loss: 0.2680 - val_accuracy: 0.9618\n",
      "Epoch 46/100\n",
      "165/165 - 0s - loss: 0.1951 - accuracy: 0.9847 - val_loss: 0.2702 - val_accuracy: 0.9630\n",
      "Epoch 47/100\n",
      "165/165 - 0s - loss: 0.1895 - accuracy: 0.9854 - val_loss: 0.2689 - val_accuracy: 0.9603\n",
      "Epoch 48/100\n",
      "165/165 - 0s - loss: 0.1927 - accuracy: 0.9839 - val_loss: 0.2596 - val_accuracy: 0.9638\n",
      "Epoch 49/100\n",
      "165/165 - 0s - loss: 0.1853 - accuracy: 0.9860 - val_loss: 0.2653 - val_accuracy: 0.9622\n",
      "Epoch 50/100\n",
      "165/165 - 0s - loss: 0.1830 - accuracy: 0.9867 - val_loss: 0.2622 - val_accuracy: 0.9636\n",
      "Epoch 51/100\n",
      "165/165 - 0s - loss: 0.1816 - accuracy: 0.9868 - val_loss: 0.2593 - val_accuracy: 0.9637\n",
      "Epoch 52/100\n",
      "165/165 - 0s - loss: 0.1789 - accuracy: 0.9878 - val_loss: 0.2638 - val_accuracy: 0.9631\n",
      "Epoch 53/100\n",
      "165/165 - 0s - loss: 0.1762 - accuracy: 0.9880 - val_loss: 0.2626 - val_accuracy: 0.9614\n",
      "Epoch 54/100\n",
      "165/165 - 0s - loss: 0.1762 - accuracy: 0.9878 - val_loss: 0.2611 - val_accuracy: 0.9632\n",
      "Epoch 55/100\n",
      "165/165 - 0s - loss: 0.1754 - accuracy: 0.9884 - val_loss: 0.2574 - val_accuracy: 0.9636\n",
      "Epoch 56/100\n",
      "165/165 - 0s - loss: 0.1712 - accuracy: 0.9893 - val_loss: 0.2594 - val_accuracy: 0.9624\n",
      "Epoch 57/100\n",
      "165/165 - 0s - loss: 0.1710 - accuracy: 0.9890 - val_loss: 0.2608 - val_accuracy: 0.9623\n",
      "Epoch 58/100\n",
      "165/165 - 0s - loss: 0.1697 - accuracy: 0.9883 - val_loss: 0.2566 - val_accuracy: 0.9623\n",
      "Epoch 59/100\n",
      "165/165 - 0s - loss: 0.1668 - accuracy: 0.9890 - val_loss: 0.2534 - val_accuracy: 0.9632\n",
      "Epoch 60/100\n",
      "165/165 - 0s - loss: 0.1666 - accuracy: 0.9889 - val_loss: 0.2610 - val_accuracy: 0.9618\n",
      "Epoch 61/100\n",
      "165/165 - 0s - loss: 0.1656 - accuracy: 0.9892 - val_loss: 0.2524 - val_accuracy: 0.9644\n",
      "Epoch 62/100\n",
      "165/165 - 0s - loss: 0.1605 - accuracy: 0.9915 - val_loss: 0.2540 - val_accuracy: 0.9635\n",
      "Epoch 63/100\n",
      "165/165 - 0s - loss: 0.1599 - accuracy: 0.9908 - val_loss: 0.2545 - val_accuracy: 0.9631\n",
      "Epoch 64/100\n",
      "165/165 - 0s - loss: 0.1593 - accuracy: 0.9914 - val_loss: 0.2546 - val_accuracy: 0.9632\n",
      "Epoch 65/100\n",
      "165/165 - 0s - loss: 0.1587 - accuracy: 0.9911 - val_loss: 0.2613 - val_accuracy: 0.9619\n",
      "Epoch 66/100\n",
      "165/165 - 0s - loss: 0.1578 - accuracy: 0.9913 - val_loss: 0.2594 - val_accuracy: 0.9627\n",
      "Epoch 67/100\n",
      "165/165 - 0s - loss: 0.1579 - accuracy: 0.9913 - val_loss: 0.2614 - val_accuracy: 0.9619\n",
      "Epoch 68/100\n",
      "165/165 - 0s - loss: 0.1553 - accuracy: 0.9919 - val_loss: 0.2547 - val_accuracy: 0.9646\n",
      "Epoch 69/100\n",
      "165/165 - 0s - loss: 0.1548 - accuracy: 0.9921 - val_loss: 0.2542 - val_accuracy: 0.9634\n",
      "Epoch 70/100\n",
      "165/165 - 0s - loss: 0.1563 - accuracy: 0.9915 - val_loss: 0.2683 - val_accuracy: 0.9612\n",
      "Epoch 71/100\n",
      "165/165 - 0s - loss: 0.1533 - accuracy: 0.9923 - val_loss: 0.2516 - val_accuracy: 0.9635\n",
      "Epoch 72/100\n",
      "165/165 - 0s - loss: 0.1522 - accuracy: 0.9925 - val_loss: 0.2588 - val_accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "165/165 - 0s - loss: 0.1534 - accuracy: 0.9919 - val_loss: 0.2581 - val_accuracy: 0.9629\n",
      "Epoch 74/100\n",
      "165/165 - 0s - loss: 0.1551 - accuracy: 0.9912 - val_loss: 0.2589 - val_accuracy: 0.9622\n",
      "Epoch 75/100\n",
      "165/165 - 0s - loss: 0.1487 - accuracy: 0.9934 - val_loss: 0.2579 - val_accuracy: 0.9615\n",
      "Epoch 76/100\n",
      "165/165 - 0s - loss: 0.1489 - accuracy: 0.9935 - val_loss: 0.2527 - val_accuracy: 0.9641\n",
      "Epoch 77/100\n",
      "165/165 - 0s - loss: 0.1484 - accuracy: 0.9932 - val_loss: 0.2556 - val_accuracy: 0.9634\n",
      "Epoch 78/100\n",
      "165/165 - 0s - loss: 0.1466 - accuracy: 0.9936 - val_loss: 0.2589 - val_accuracy: 0.9614\n",
      "Epoch 79/100\n",
      "165/165 - 0s - loss: 0.1480 - accuracy: 0.9930 - val_loss: 0.2570 - val_accuracy: 0.9638\n",
      "Epoch 80/100\n",
      "165/165 - 0s - loss: 0.1487 - accuracy: 0.9932 - val_loss: 0.2500 - val_accuracy: 0.9647\n",
      "Epoch 81/100\n",
      "165/165 - 0s - loss: 0.1444 - accuracy: 0.9940 - val_loss: 0.2581 - val_accuracy: 0.9612\n",
      "Epoch 82/100\n",
      "165/165 - 0s - loss: 0.1447 - accuracy: 0.9945 - val_loss: 0.2508 - val_accuracy: 0.9647\n",
      "Epoch 83/100\n",
      "165/165 - 0s - loss: 0.1425 - accuracy: 0.9943 - val_loss: 0.2539 - val_accuracy: 0.9631\n",
      "Epoch 84/100\n",
      "165/165 - 0s - loss: 0.1444 - accuracy: 0.9938 - val_loss: 0.2545 - val_accuracy: 0.9628\n",
      "Epoch 85/100\n",
      "165/165 - 0s - loss: 0.1420 - accuracy: 0.9947 - val_loss: 0.2766 - val_accuracy: 0.9559\n",
      "Epoch 86/100\n",
      "165/165 - 0s - loss: 0.1483 - accuracy: 0.9925 - val_loss: 0.2507 - val_accuracy: 0.9637\n",
      "Epoch 87/100\n",
      "165/165 - 0s - loss: 0.1403 - accuracy: 0.9951 - val_loss: 0.2467 - val_accuracy: 0.9644\n",
      "Epoch 88/100\n",
      "165/165 - 0s - loss: 0.1397 - accuracy: 0.9950 - val_loss: 0.2577 - val_accuracy: 0.9609\n",
      "Epoch 89/100\n",
      "165/165 - 0s - loss: 0.1410 - accuracy: 0.9946 - val_loss: 0.2484 - val_accuracy: 0.9637\n",
      "Epoch 90/100\n",
      "165/165 - 0s - loss: 0.1394 - accuracy: 0.9947 - val_loss: 0.2577 - val_accuracy: 0.9617\n",
      "Epoch 91/100\n",
      "165/165 - 0s - loss: 0.1402 - accuracy: 0.9948 - val_loss: 0.2540 - val_accuracy: 0.9636\n",
      "Epoch 92/100\n",
      "165/165 - 0s - loss: 0.1382 - accuracy: 0.9953 - val_loss: 0.2552 - val_accuracy: 0.9629\n",
      "Epoch 93/100\n",
      "165/165 - 0s - loss: 0.1364 - accuracy: 0.9956 - val_loss: 0.2486 - val_accuracy: 0.9638\n",
      "Epoch 94/100\n",
      "165/165 - 0s - loss: 0.1400 - accuracy: 0.9945 - val_loss: 0.2512 - val_accuracy: 0.9642\n",
      "Epoch 95/100\n",
      "165/165 - 0s - loss: 0.1362 - accuracy: 0.9956 - val_loss: 0.2583 - val_accuracy: 0.9612\n",
      "Epoch 96/100\n",
      "165/165 - 0s - loss: 0.1362 - accuracy: 0.9955 - val_loss: 0.2532 - val_accuracy: 0.9636\n",
      "Epoch 97/100\n",
      "165/165 - 0s - loss: 0.1352 - accuracy: 0.9957 - val_loss: 0.2545 - val_accuracy: 0.9630\n",
      "Epoch 98/100\n",
      "165/165 - 0s - loss: 0.1402 - accuracy: 0.9940 - val_loss: 0.2546 - val_accuracy: 0.9629\n",
      "Epoch 99/100\n",
      "165/165 - 0s - loss: 0.1342 - accuracy: 0.9957 - val_loss: 0.2592 - val_accuracy: 0.9619\n",
      "Epoch 100/100\n",
      "165/165 - 0s - loss: 0.1358 - accuracy: 0.9952 - val_loss: 0.2518 - val_accuracy: 0.9630\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])\n",
    "print(x_train.shape, ' ', y_train.shape)\n",
    "print(x_test.shape, ' ', y_test.shape)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "    layers.Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(0.01))\n",
    "])\n",
    "\n",
    "\n",
    "#keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#keras.optimizers.Adam(learning_rate=0.01)\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# provide labels as one_hot representation => tf.keras.losses.CategoricalCrossentropy\n",
    "# provide labels as integers => tf.keras.losses.SparseCategoricalCrossentropy \n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=256, epochs=100, validation_split=0.3, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 623us/step - loss: 0.2380 - accuracy: 0.9659\n",
      "[0.23797090351581573, 0.9659000039100647]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.367347\n",
      "batch 1: loss 2.235757\n",
      "batch 2: loss 2.258137\n",
      "batch 3: loss 2.050302\n",
      "batch 4: loss 2.106163\n",
      "batch 5: loss 2.065310\n",
      "batch 6: loss 1.832214\n",
      "batch 7: loss 1.883654\n",
      "batch 8: loss 1.806508\n",
      "batch 9: loss 1.793744\n",
      "batch 10: loss 1.744891\n",
      "batch 11: loss 1.690027\n",
      "batch 12: loss 1.577990\n",
      "batch 13: loss 1.499764\n",
      "batch 14: loss 1.444115\n",
      "batch 15: loss 1.528492\n",
      "batch 16: loss 1.424717\n",
      "batch 17: loss 1.420692\n",
      "batch 18: loss 1.421820\n",
      "batch 19: loss 1.254521\n",
      "batch 20: loss 1.337875\n",
      "batch 21: loss 1.259990\n",
      "batch 22: loss 1.125584\n",
      "batch 23: loss 1.330357\n",
      "batch 24: loss 1.311557\n",
      "batch 25: loss 1.256429\n",
      "batch 26: loss 1.117525\n",
      "batch 27: loss 1.083125\n",
      "batch 28: loss 0.993340\n",
      "batch 29: loss 1.251084\n",
      "batch 30: loss 1.143280\n",
      "batch 31: loss 1.135569\n",
      "batch 32: loss 1.241614\n",
      "batch 33: loss 1.013816\n",
      "batch 34: loss 0.996322\n",
      "batch 35: loss 0.930064\n",
      "batch 36: loss 1.034250\n",
      "batch 37: loss 1.075321\n",
      "batch 38: loss 0.914486\n",
      "batch 39: loss 0.969876\n",
      "batch 40: loss 1.065566\n",
      "batch 41: loss 0.937700\n",
      "batch 42: loss 0.958039\n",
      "batch 43: loss 1.015204\n",
      "batch 44: loss 0.809940\n",
      "batch 45: loss 0.916777\n",
      "batch 46: loss 0.982714\n",
      "batch 47: loss 0.793594\n",
      "batch 48: loss 0.924985\n",
      "batch 49: loss 0.936285\n",
      "batch 50: loss 0.783825\n",
      "batch 51: loss 0.779913\n",
      "batch 52: loss 0.734812\n",
      "batch 53: loss 1.056764\n",
      "batch 54: loss 0.847782\n",
      "batch 55: loss 0.897634\n",
      "batch 56: loss 0.714479\n",
      "batch 57: loss 0.942330\n",
      "batch 58: loss 1.003862\n",
      "batch 59: loss 0.769691\n",
      "batch 60: loss 0.904132\n",
      "batch 61: loss 0.871015\n",
      "batch 62: loss 0.944826\n",
      "batch 63: loss 0.922721\n",
      "batch 64: loss 0.868610\n",
      "batch 65: loss 0.769204\n",
      "batch 66: loss 0.742825\n",
      "batch 67: loss 0.849198\n",
      "batch 68: loss 0.796597\n",
      "batch 69: loss 0.903088\n",
      "batch 70: loss 0.761601\n",
      "batch 71: loss 0.725264\n",
      "batch 72: loss 0.777247\n",
      "batch 73: loss 0.882746\n",
      "batch 74: loss 0.845494\n",
      "batch 75: loss 0.797565\n",
      "batch 76: loss 0.773108\n",
      "batch 77: loss 0.801172\n",
      "batch 78: loss 0.693785\n",
      "batch 79: loss 0.909651\n",
      "batch 80: loss 0.842596\n",
      "batch 81: loss 0.771511\n",
      "batch 82: loss 0.827654\n",
      "batch 83: loss 0.719842\n",
      "batch 84: loss 0.821441\n",
      "batch 85: loss 0.691604\n",
      "batch 86: loss 0.600422\n",
      "batch 87: loss 0.736599\n",
      "batch 88: loss 0.706782\n",
      "batch 89: loss 0.645496\n",
      "batch 90: loss 0.745875\n",
      "batch 91: loss 0.569214\n",
      "batch 92: loss 0.728996\n",
      "batch 93: loss 0.766075\n",
      "batch 94: loss 0.644121\n",
      "batch 95: loss 0.733361\n",
      "batch 96: loss 0.714129\n",
      "batch 97: loss 0.689924\n",
      "batch 98: loss 0.740516\n",
      "batch 99: loss 0.833023\n",
      "batch 100: loss 0.747751\n",
      "batch 101: loss 0.790777\n",
      "batch 102: loss 0.655241\n",
      "batch 103: loss 0.786559\n",
      "batch 104: loss 0.825084\n",
      "batch 105: loss 0.680195\n",
      "batch 106: loss 0.730614\n",
      "batch 107: loss 0.656770\n",
      "batch 108: loss 0.779891\n",
      "batch 109: loss 0.729811\n",
      "batch 110: loss 0.803995\n",
      "batch 111: loss 0.832512\n",
      "batch 112: loss 0.806721\n",
      "batch 113: loss 0.737284\n",
      "batch 114: loss 0.726792\n",
      "batch 115: loss 0.616739\n",
      "batch 116: loss 0.837408\n",
      "batch 117: loss 0.749200\n",
      "batch 118: loss 0.599532\n",
      "batch 119: loss 0.724724\n",
      "batch 120: loss 0.801050\n",
      "batch 121: loss 0.613551\n",
      "batch 122: loss 0.733807\n",
      "batch 123: loss 0.739388\n",
      "batch 124: loss 0.696298\n",
      "batch 125: loss 0.768932\n",
      "batch 126: loss 0.695011\n",
      "batch 127: loss 0.697634\n",
      "batch 128: loss 0.707633\n",
      "batch 129: loss 0.545531\n",
      "batch 130: loss 0.663213\n",
      "batch 131: loss 0.722488\n",
      "batch 132: loss 0.536340\n",
      "batch 133: loss 0.673260\n",
      "batch 134: loss 0.599141\n",
      "batch 135: loss 0.715519\n",
      "batch 136: loss 0.608456\n",
      "batch 137: loss 0.680048\n",
      "batch 138: loss 0.678046\n",
      "batch 139: loss 0.593436\n",
      "batch 140: loss 0.617639\n",
      "batch 141: loss 0.798619\n",
      "batch 142: loss 0.835566\n",
      "batch 143: loss 0.764551\n",
      "batch 144: loss 0.593352\n",
      "batch 145: loss 0.663625\n",
      "batch 146: loss 0.556756\n",
      "batch 147: loss 0.728655\n",
      "batch 148: loss 0.675382\n",
      "batch 149: loss 0.747605\n",
      "batch 150: loss 0.694676\n",
      "batch 151: loss 0.686828\n",
      "batch 152: loss 0.625464\n",
      "batch 153: loss 0.715259\n",
      "batch 154: loss 0.709345\n",
      "batch 155: loss 0.771483\n",
      "batch 156: loss 0.756962\n",
      "batch 157: loss 0.611577\n",
      "batch 158: loss 0.502993\n",
      "batch 159: loss 0.664919\n",
      "batch 160: loss 0.587451\n",
      "batch 161: loss 0.602469\n",
      "batch 162: loss 0.609685\n",
      "batch 163: loss 0.638992\n",
      "batch 164: loss 0.770831\n",
      "batch 165: loss 0.624151\n",
      "batch 166: loss 0.574809\n",
      "batch 167: loss 0.801541\n",
      "batch 168: loss 0.652199\n",
      "batch 169: loss 0.707712\n",
      "batch 170: loss 0.519173\n",
      "batch 171: loss 0.604648\n",
      "batch 172: loss 0.733650\n",
      "batch 173: loss 0.569576\n",
      "batch 174: loss 0.563516\n",
      "batch 175: loss 0.725854\n",
      "batch 176: loss 0.782053\n",
      "batch 177: loss 0.614499\n",
      "batch 178: loss 0.779588\n",
      "batch 179: loss 0.819500\n",
      "batch 180: loss 0.504721\n",
      "batch 181: loss 0.636624\n",
      "batch 182: loss 0.681528\n",
      "batch 183: loss 0.590548\n",
      "batch 184: loss 0.809591\n",
      "batch 185: loss 0.612836\n",
      "batch 186: loss 0.629737\n",
      "batch 187: loss 0.612484\n",
      "batch 188: loss 0.590153\n",
      "batch 189: loss 0.627500\n",
      "batch 190: loss 0.650852\n",
      "batch 191: loss 0.662396\n",
      "batch 192: loss 0.564572\n",
      "batch 193: loss 0.714621\n",
      "batch 194: loss 0.589098\n",
      "batch 195: loss 0.533262\n",
      "batch 196: loss 0.556124\n",
      "batch 197: loss 0.527669\n",
      "batch 198: loss 0.647362\n",
      "batch 199: loss 0.577172\n",
      "batch 200: loss 0.541410\n",
      "batch 201: loss 0.640340\n",
      "batch 202: loss 0.592732\n",
      "batch 203: loss 0.498321\n",
      "batch 204: loss 0.593983\n",
      "batch 205: loss 0.531052\n",
      "batch 206: loss 0.592561\n",
      "batch 207: loss 0.713103\n",
      "batch 208: loss 0.615027\n",
      "batch 209: loss 0.619114\n",
      "batch 210: loss 0.604853\n",
      "batch 211: loss 0.537523\n",
      "batch 212: loss 0.710843\n",
      "batch 213: loss 0.676400\n",
      "batch 214: loss 0.656848\n",
      "batch 215: loss 0.629992\n",
      "batch 216: loss 0.736390\n",
      "batch 217: loss 0.489978\n",
      "batch 218: loss 0.693570\n",
      "batch 219: loss 0.516260\n",
      "batch 220: loss 0.608330\n",
      "batch 221: loss 0.608545\n",
      "batch 222: loss 0.496185\n",
      "batch 223: loss 0.724561\n",
      "batch 224: loss 0.585447\n",
      "batch 225: loss 0.719990\n",
      "batch 226: loss 0.497617\n",
      "batch 227: loss 0.584190\n",
      "batch 228: loss 0.629508\n",
      "batch 229: loss 0.554269\n",
      "batch 230: loss 0.592649\n",
      "batch 231: loss 0.560742\n",
      "batch 232: loss 0.721298\n",
      "batch 233: loss 0.672409\n",
      "batch 234: loss 0.504133\n",
      "batch 235: loss 0.500719\n",
      "batch 236: loss 0.486762\n",
      "batch 237: loss 0.603798\n",
      "batch 238: loss 0.645539\n",
      "batch 239: loss 0.670391\n",
      "batch 240: loss 0.471524\n",
      "batch 241: loss 0.479203\n",
      "batch 242: loss 0.819844\n",
      "batch 243: loss 0.626503\n",
      "batch 244: loss 0.625443\n",
      "batch 245: loss 0.542802\n",
      "batch 246: loss 0.633461\n",
      "batch 247: loss 0.635030\n",
      "batch 248: loss 0.459781\n",
      "batch 249: loss 0.554175\n",
      "batch 250: loss 0.560164\n",
      "batch 251: loss 0.593475\n",
      "batch 252: loss 0.478243\n",
      "batch 253: loss 0.667688\n",
      "batch 254: loss 0.578058\n",
      "batch 255: loss 0.628347\n",
      "batch 256: loss 0.556260\n",
      "batch 257: loss 0.577656\n",
      "batch 258: loss 0.590642\n",
      "batch 259: loss 0.584806\n",
      "batch 260: loss 0.554774\n",
      "batch 261: loss 0.464668\n",
      "batch 262: loss 0.541723\n",
      "batch 263: loss 0.596454\n",
      "batch 264: loss 0.566583\n",
      "batch 265: loss 0.549948\n",
      "batch 266: loss 0.644845\n",
      "batch 267: loss 0.625194\n",
      "batch 268: loss 0.540200\n",
      "batch 269: loss 0.669886\n",
      "batch 270: loss 0.543867\n",
      "batch 271: loss 0.805022\n",
      "batch 272: loss 0.670034\n",
      "batch 273: loss 0.743965\n",
      "batch 274: loss 0.561272\n",
      "batch 275: loss 0.705971\n",
      "batch 276: loss 0.570859\n",
      "batch 277: loss 0.438041\n",
      "batch 278: loss 0.462106\n",
      "batch 279: loss 0.561425\n",
      "batch 280: loss 0.483893\n",
      "batch 281: loss 0.625636\n",
      "batch 282: loss 0.545172\n",
      "batch 283: loss 0.655657\n",
      "batch 284: loss 0.574939\n",
      "batch 285: loss 0.551039\n",
      "batch 286: loss 0.685149\n",
      "batch 287: loss 0.547319\n",
      "batch 288: loss 0.635963\n",
      "batch 289: loss 0.536179\n",
      "batch 290: loss 0.602541\n",
      "batch 291: loss 0.561294\n",
      "batch 292: loss 0.702650\n",
      "batch 293: loss 0.594239\n",
      "batch 294: loss 0.774738\n",
      "batch 295: loss 0.463802\n",
      "batch 296: loss 0.612726\n",
      "batch 297: loss 0.570858\n",
      "batch 298: loss 0.454100\n",
      "batch 299: loss 0.481308\n",
      "batch 300: loss 0.519977\n",
      "batch 301: loss 0.598980\n",
      "batch 302: loss 0.459780\n",
      "batch 303: loss 0.539668\n",
      "batch 304: loss 0.486374\n",
      "batch 305: loss 0.576762\n",
      "batch 306: loss 0.539597\n",
      "batch 307: loss 0.592896\n",
      "batch 308: loss 0.529873\n",
      "batch 309: loss 0.642820\n",
      "batch 310: loss 0.531312\n",
      "batch 311: loss 0.455351\n",
      "batch 312: loss 0.582514\n",
      "batch 313: loss 0.553169\n",
      "batch 314: loss 0.484182\n",
      "batch 315: loss 0.525561\n",
      "batch 316: loss 0.351977\n",
      "batch 317: loss 0.601021\n",
      "batch 318: loss 0.492553\n",
      "batch 319: loss 0.580087\n",
      "batch 320: loss 0.578819\n",
      "batch 321: loss 0.441436\n",
      "batch 322: loss 0.521405\n",
      "batch 323: loss 0.654329\n",
      "batch 324: loss 0.494205\n",
      "batch 325: loss 0.489647\n",
      "batch 326: loss 0.689304\n",
      "batch 327: loss 0.557271\n",
      "batch 328: loss 0.603025\n",
      "batch 329: loss 0.452131\n",
      "batch 330: loss 0.559075\n",
      "batch 331: loss 0.533256\n",
      "batch 332: loss 0.541191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 333: loss 0.751615\n",
      "batch 334: loss 0.528809\n",
      "batch 335: loss 0.606010\n",
      "batch 336: loss 0.421455\n",
      "batch 337: loss 0.479264\n",
      "batch 338: loss 0.646639\n",
      "batch 339: loss 0.443535\n",
      "batch 340: loss 0.584036\n",
      "batch 341: loss 0.534135\n",
      "batch 342: loss 0.391141\n",
      "batch 343: loss 0.575087\n",
      "batch 344: loss 0.563329\n",
      "batch 345: loss 0.412160\n",
      "batch 346: loss 0.609208\n",
      "batch 347: loss 0.622252\n",
      "batch 348: loss 0.448896\n",
      "batch 349: loss 0.482667\n",
      "batch 350: loss 0.446319\n",
      "batch 351: loss 0.509667\n",
      "batch 352: loss 0.566356\n",
      "batch 353: loss 0.522359\n",
      "batch 354: loss 0.470543\n",
      "batch 355: loss 0.540084\n",
      "batch 356: loss 0.656268\n",
      "batch 357: loss 0.408096\n",
      "batch 358: loss 0.601206\n",
      "batch 359: loss 0.513891\n",
      "batch 360: loss 0.424106\n",
      "batch 361: loss 0.661901\n",
      "batch 362: loss 0.638705\n",
      "batch 363: loss 0.489160\n",
      "batch 364: loss 0.481764\n",
      "batch 365: loss 0.364336\n",
      "batch 366: loss 0.451484\n",
      "batch 367: loss 0.534145\n",
      "batch 368: loss 0.612004\n",
      "batch 369: loss 0.428625\n",
      "batch 370: loss 0.509094\n",
      "batch 371: loss 0.659495\n",
      "batch 372: loss 0.606181\n",
      "batch 373: loss 0.511843\n",
      "batch 374: loss 0.444852\n",
      "batch 375: loss 0.525042\n",
      "batch 376: loss 0.597524\n",
      "batch 377: loss 0.550607\n",
      "batch 378: loss 0.770588\n",
      "batch 379: loss 0.567529\n",
      "batch 380: loss 0.612918\n",
      "batch 381: loss 0.524631\n",
      "batch 382: loss 0.536819\n",
      "batch 383: loss 0.854632\n",
      "batch 384: loss 0.440185\n",
      "batch 385: loss 0.620700\n",
      "batch 386: loss 0.592621\n",
      "batch 387: loss 0.421555\n",
      "batch 388: loss 0.560001\n",
      "batch 389: loss 0.502456\n",
      "batch 390: loss 0.393938\n",
      "batch 391: loss 0.572734\n",
      "batch 392: loss 0.457328\n",
      "batch 393: loss 0.429187\n",
      "batch 394: loss 0.495515\n",
      "batch 395: loss 0.602533\n",
      "batch 396: loss 0.577141\n",
      "batch 397: loss 0.585798\n",
      "batch 398: loss 0.423980\n",
      "batch 399: loss 0.414383\n",
      "batch 400: loss 0.499680\n",
      "batch 401: loss 0.368894\n",
      "batch 402: loss 0.461500\n",
      "batch 403: loss 0.529174\n",
      "batch 404: loss 0.495210\n",
      "batch 405: loss 0.490564\n",
      "batch 406: loss 0.419566\n",
      "batch 407: loss 0.437464\n",
      "batch 408: loss 0.577311\n",
      "batch 409: loss 0.324980\n",
      "batch 410: loss 0.465010\n",
      "batch 411: loss 0.566805\n",
      "batch 412: loss 0.587716\n",
      "batch 413: loss 0.488454\n",
      "batch 414: loss 0.443860\n",
      "batch 415: loss 0.525606\n",
      "batch 416: loss 0.608513\n",
      "batch 417: loss 0.442669\n",
      "batch 418: loss 0.493302\n",
      "batch 419: loss 0.544050\n",
      "batch 420: loss 0.576921\n",
      "batch 421: loss 0.557225\n",
      "batch 422: loss 0.484327\n",
      "batch 423: loss 0.431323\n",
      "batch 424: loss 0.452279\n",
      "batch 425: loss 0.469242\n",
      "batch 426: loss 0.503501\n",
      "batch 427: loss 0.572909\n",
      "batch 428: loss 0.377763\n",
      "batch 429: loss 0.376841\n",
      "batch 430: loss 0.413291\n",
      "batch 431: loss 0.488697\n",
      "batch 432: loss 0.533558\n",
      "batch 433: loss 0.624425\n",
      "batch 434: loss 0.553328\n",
      "batch 435: loss 0.468138\n",
      "batch 436: loss 0.423116\n",
      "batch 437: loss 0.464080\n",
      "batch 438: loss 0.405779\n",
      "batch 439: loss 0.532551\n",
      "batch 440: loss 0.521798\n",
      "batch 441: loss 0.420466\n",
      "batch 442: loss 0.495607\n",
      "batch 443: loss 0.455477\n",
      "batch 444: loss 0.499599\n",
      "batch 445: loss 0.464106\n",
      "batch 446: loss 0.503358\n",
      "batch 447: loss 0.433264\n",
      "batch 448: loss 0.537337\n",
      "batch 449: loss 0.605663\n",
      "batch 450: loss 0.550733\n",
      "batch 451: loss 0.521824\n",
      "batch 452: loss 0.502037\n",
      "batch 453: loss 0.601375\n",
      "batch 454: loss 0.541397\n",
      "batch 455: loss 0.436449\n",
      "batch 456: loss 0.420542\n",
      "batch 457: loss 0.465645\n",
      "batch 458: loss 0.506999\n",
      "batch 459: loss 0.467250\n",
      "batch 460: loss 0.465969\n",
      "batch 461: loss 0.503736\n",
      "batch 462: loss 0.441555\n",
      "batch 463: loss 0.470520\n",
      "batch 464: loss 0.545714\n",
      "batch 465: loss 0.471161\n",
      "batch 466: loss 0.573011\n",
      "batch 467: loss 0.610606\n",
      "batch 468: loss 0.403284\n",
      "batch 469: loss 0.385857\n",
      "batch 470: loss 0.499537\n",
      "batch 471: loss 0.610734\n",
      "batch 472: loss 0.633416\n",
      "batch 473: loss 0.479411\n",
      "batch 474: loss 0.483352\n",
      "batch 475: loss 0.554526\n",
      "batch 476: loss 0.626072\n",
      "batch 477: loss 0.526225\n",
      "batch 478: loss 0.415182\n",
      "batch 479: loss 0.533134\n",
      "batch 480: loss 0.483558\n",
      "batch 481: loss 0.414146\n",
      "batch 482: loss 0.356406\n",
      "batch 483: loss 0.515441\n",
      "batch 484: loss 0.434442\n",
      "batch 485: loss 0.654964\n",
      "batch 486: loss 0.517655\n",
      "batch 487: loss 0.406147\n",
      "batch 488: loss 0.489369\n",
      "batch 489: loss 0.489090\n",
      "batch 490: loss 0.549226\n",
      "batch 491: loss 0.408197\n",
      "batch 492: loss 0.492475\n",
      "batch 493: loss 0.514305\n",
      "batch 494: loss 0.456984\n",
      "batch 495: loss 0.563380\n",
      "batch 496: loss 0.356807\n",
      "batch 497: loss 0.512922\n",
      "batch 498: loss 0.400125\n",
      "batch 499: loss 0.498192\n",
      "batch 500: loss 0.470916\n",
      "batch 501: loss 0.388365\n",
      "batch 502: loss 0.456909\n",
      "batch 503: loss 0.436733\n",
      "batch 504: loss 0.471133\n",
      "batch 505: loss 0.518225\n",
      "batch 506: loss 0.419018\n",
      "batch 507: loss 0.520574\n",
      "batch 508: loss 0.443618\n",
      "batch 509: loss 0.386156\n",
      "batch 510: loss 0.431236\n",
      "batch 511: loss 0.696020\n",
      "batch 512: loss 0.446915\n",
      "batch 513: loss 0.532288\n",
      "batch 514: loss 0.562908\n",
      "batch 515: loss 0.521878\n",
      "batch 516: loss 0.488426\n",
      "batch 517: loss 0.492403\n",
      "batch 518: loss 0.507630\n",
      "batch 519: loss 0.572802\n",
      "batch 520: loss 0.560517\n",
      "batch 521: loss 0.660180\n",
      "batch 522: loss 0.502936\n",
      "batch 523: loss 0.522522\n",
      "batch 524: loss 0.380607\n",
      "batch 525: loss 0.473330\n",
      "batch 526: loss 0.461054\n",
      "batch 527: loss 0.551483\n",
      "batch 528: loss 0.475340\n",
      "batch 529: loss 0.389005\n",
      "batch 530: loss 0.565650\n",
      "batch 531: loss 0.388250\n",
      "batch 532: loss 0.346117\n",
      "batch 533: loss 0.438018\n",
      "batch 534: loss 0.514985\n",
      "batch 535: loss 0.550353\n",
      "batch 536: loss 0.383209\n",
      "batch 537: loss 0.456848\n",
      "batch 538: loss 0.491757\n",
      "batch 539: loss 0.510885\n",
      "batch 540: loss 0.522741\n",
      "batch 541: loss 0.479199\n",
      "batch 542: loss 0.461789\n",
      "batch 543: loss 0.427394\n",
      "batch 544: loss 0.498847\n",
      "batch 545: loss 0.405490\n",
      "batch 546: loss 0.456746\n",
      "batch 547: loss 0.447771\n",
      "batch 548: loss 0.485141\n",
      "batch 549: loss 0.453603\n",
      "batch 550: loss 0.576293\n",
      "batch 551: loss 0.487798\n",
      "batch 552: loss 0.387678\n",
      "batch 553: loss 0.649998\n",
      "batch 554: loss 0.519870\n",
      "batch 555: loss 0.409409\n",
      "batch 556: loss 0.409720\n",
      "batch 557: loss 0.442053\n",
      "batch 558: loss 0.492904\n",
      "batch 559: loss 0.455498\n",
      "batch 560: loss 0.491489\n",
      "batch 561: loss 0.548231\n",
      "batch 562: loss 0.445961\n",
      "batch 563: loss 0.488299\n",
      "batch 564: loss 0.512391\n",
      "batch 565: loss 0.402270\n",
      "batch 566: loss 0.438361\n",
      "batch 567: loss 0.390014\n",
      "batch 568: loss 0.347550\n",
      "batch 569: loss 0.472421\n",
      "batch 570: loss 0.565582\n",
      "batch 571: loss 0.370645\n",
      "batch 572: loss 0.523537\n",
      "batch 573: loss 0.580351\n",
      "batch 574: loss 0.510446\n",
      "batch 575: loss 0.655887\n",
      "batch 576: loss 0.342210\n",
      "batch 577: loss 0.450777\n",
      "batch 578: loss 0.507342\n",
      "batch 579: loss 0.362915\n",
      "batch 580: loss 0.462105\n",
      "batch 581: loss 0.396703\n",
      "batch 582: loss 0.464879\n",
      "batch 583: loss 0.478032\n",
      "batch 584: loss 0.402058\n",
      "batch 585: loss 0.441447\n",
      "batch 586: loss 0.551654\n",
      "batch 587: loss 0.537973\n",
      "batch 588: loss 0.469456\n",
      "batch 589: loss 0.536180\n",
      "batch 590: loss 0.369955\n",
      "batch 591: loss 0.432037\n",
      "batch 592: loss 0.488236\n",
      "batch 593: loss 0.428878\n",
      "batch 594: loss 0.436995\n",
      "batch 595: loss 0.495334\n",
      "batch 596: loss 0.493794\n",
      "batch 597: loss 0.490262\n",
      "batch 598: loss 0.409628\n",
      "batch 599: loss 0.432576\n",
      "batch 600: loss 0.448301\n",
      "batch 601: loss 0.524215\n",
      "batch 602: loss 0.449908\n",
      "batch 603: loss 0.328325\n",
      "batch 604: loss 0.689181\n",
      "batch 605: loss 0.484014\n",
      "batch 606: loss 0.410872\n",
      "batch 607: loss 0.454115\n",
      "batch 608: loss 0.441728\n",
      "batch 609: loss 0.446926\n",
      "batch 610: loss 0.627394\n",
      "batch 611: loss 0.391383\n",
      "batch 612: loss 0.349795\n",
      "batch 613: loss 0.406954\n",
      "batch 614: loss 0.387563\n",
      "batch 615: loss 0.364373\n",
      "batch 616: loss 0.438989\n",
      "batch 617: loss 0.451987\n",
      "batch 618: loss 0.532500\n",
      "batch 619: loss 0.432049\n",
      "batch 620: loss 0.424573\n",
      "batch 621: loss 0.417269\n",
      "batch 622: loss 0.452552\n",
      "batch 623: loss 0.512416\n",
      "batch 624: loss 0.398744\n",
      "batch 625: loss 0.545461\n",
      "batch 626: loss 0.514783\n",
      "batch 627: loss 0.467953\n",
      "batch 628: loss 0.450570\n",
      "batch 629: loss 0.456439\n",
      "batch 630: loss 0.456777\n",
      "batch 631: loss 0.533817\n",
      "batch 632: loss 0.431896\n",
      "batch 633: loss 0.454193\n",
      "batch 634: loss 0.477550\n",
      "batch 635: loss 0.402198\n",
      "batch 636: loss 0.346799\n",
      "batch 637: loss 0.437948\n",
      "batch 638: loss 0.409191\n",
      "batch 639: loss 0.418293\n",
      "batch 640: loss 0.458426\n",
      "batch 641: loss 0.540495\n",
      "batch 642: loss 0.496379\n",
      "batch 643: loss 0.560386\n",
      "batch 644: loss 0.598439\n",
      "batch 645: loss 0.518551\n",
      "batch 646: loss 0.492844\n",
      "batch 647: loss 0.324687\n",
      "batch 648: loss 0.398550\n",
      "batch 649: loss 0.334948\n",
      "batch 650: loss 0.374724\n",
      "batch 651: loss 0.521711\n",
      "batch 652: loss 0.430844\n",
      "batch 653: loss 0.413360\n",
      "batch 654: loss 0.412229\n",
      "batch 655: loss 0.459229\n",
      "batch 656: loss 0.478092\n",
      "batch 657: loss 0.435077\n",
      "batch 658: loss 0.356836\n",
      "batch 659: loss 0.442643\n",
      "batch 660: loss 0.416457\n",
      "batch 661: loss 0.573301\n",
      "batch 662: loss 0.514562\n",
      "batch 663: loss 0.525270\n",
      "batch 664: loss 0.458372\n",
      "batch 665: loss 0.421286\n",
      "batch 666: loss 0.589973\n",
      "batch 667: loss 0.534017\n",
      "batch 668: loss 0.575838\n",
      "batch 669: loss 0.457668\n",
      "batch 670: loss 0.522661\n",
      "batch 671: loss 0.550321\n",
      "batch 672: loss 0.479409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 673: loss 0.415457\n",
      "batch 674: loss 0.339096\n",
      "batch 675: loss 0.484356\n",
      "batch 676: loss 0.380083\n",
      "batch 677: loss 0.473980\n",
      "batch 678: loss 0.465999\n",
      "batch 679: loss 0.449253\n",
      "batch 680: loss 0.481968\n",
      "batch 681: loss 0.480602\n",
      "batch 682: loss 0.402605\n",
      "batch 683: loss 0.495898\n",
      "batch 684: loss 0.379711\n",
      "batch 685: loss 0.428430\n",
      "batch 686: loss 0.502772\n",
      "batch 687: loss 0.449904\n",
      "batch 688: loss 0.548461\n",
      "batch 689: loss 0.524300\n",
      "batch 690: loss 0.388234\n",
      "batch 691: loss 0.422841\n",
      "batch 692: loss 0.520586\n",
      "batch 693: loss 0.558591\n",
      "batch 694: loss 0.392037\n",
      "batch 695: loss 0.494464\n",
      "batch 696: loss 0.339246\n",
      "batch 697: loss 0.454695\n",
      "batch 698: loss 0.420836\n",
      "batch 699: loss 0.430890\n",
      "batch 700: loss 0.409817\n",
      "batch 701: loss 0.536183\n",
      "batch 702: loss 0.353927\n",
      "batch 703: loss 0.515872\n",
      "batch 704: loss 0.438408\n",
      "batch 705: loss 0.525377\n",
      "batch 706: loss 0.472788\n",
      "batch 707: loss 0.285509\n",
      "batch 708: loss 0.405789\n",
      "batch 709: loss 0.341320\n",
      "batch 710: loss 0.424771\n",
      "batch 711: loss 0.436876\n",
      "batch 712: loss 0.409344\n",
      "batch 713: loss 0.295481\n",
      "batch 714: loss 0.528846\n",
      "batch 715: loss 0.475900\n",
      "batch 716: loss 0.405104\n",
      "batch 717: loss 0.509614\n",
      "batch 718: loss 0.294138\n",
      "batch 719: loss 0.440584\n",
      "batch 720: loss 0.457820\n",
      "batch 721: loss 0.401871\n",
      "batch 722: loss 0.466223\n",
      "batch 723: loss 0.432679\n",
      "batch 724: loss 0.416916\n",
      "batch 725: loss 0.597292\n",
      "batch 726: loss 0.452588\n",
      "batch 727: loss 0.443130\n",
      "batch 728: loss 0.391559\n",
      "batch 729: loss 0.438280\n",
      "batch 730: loss 0.516749\n",
      "batch 731: loss 0.504532\n",
      "batch 732: loss 0.500899\n",
      "batch 733: loss 0.415097\n",
      "batch 734: loss 0.549329\n",
      "batch 735: loss 0.487445\n",
      "batch 736: loss 0.427765\n",
      "batch 737: loss 0.579026\n",
      "batch 738: loss 0.431956\n",
      "batch 739: loss 0.487011\n",
      "batch 740: loss 0.401137\n",
      "batch 741: loss 0.376384\n",
      "batch 742: loss 0.456734\n",
      "batch 743: loss 0.342753\n",
      "batch 744: loss 0.537191\n",
      "batch 745: loss 0.443479\n",
      "batch 746: loss 0.381959\n",
      "batch 747: loss 0.407303\n",
      "batch 748: loss 0.443616\n",
      "batch 749: loss 0.464974\n",
      "batch 750: loss 0.461351\n",
      "batch 751: loss 0.397984\n",
      "batch 752: loss 0.449806\n",
      "batch 753: loss 0.349342\n",
      "batch 754: loss 0.382658\n",
      "batch 755: loss 0.477614\n",
      "batch 756: loss 0.338497\n",
      "batch 757: loss 0.518497\n",
      "batch 758: loss 0.433642\n",
      "batch 759: loss 0.318905\n",
      "batch 760: loss 0.588311\n",
      "batch 761: loss 0.336064\n",
      "batch 762: loss 0.521060\n",
      "batch 763: loss 0.414265\n",
      "batch 764: loss 0.456784\n",
      "batch 765: loss 0.312495\n",
      "batch 766: loss 0.490967\n",
      "batch 767: loss 0.426617\n",
      "batch 768: loss 0.445886\n",
      "batch 769: loss 0.397145\n",
      "batch 770: loss 0.505289\n",
      "batch 771: loss 0.466405\n",
      "batch 772: loss 0.411011\n",
      "batch 773: loss 0.477805\n",
      "batch 774: loss 0.432174\n",
      "batch 775: loss 0.392587\n",
      "batch 776: loss 0.390955\n",
      "batch 777: loss 0.373306\n",
      "batch 778: loss 0.517635\n",
      "batch 779: loss 0.488242\n",
      "batch 780: loss 0.410146\n",
      "batch 781: loss 0.537207\n",
      "batch 782: loss 0.385555\n",
      "batch 783: loss 0.258839\n",
      "batch 784: loss 0.309779\n",
      "batch 785: loss 0.324644\n",
      "batch 786: loss 0.408885\n",
      "batch 787: loss 0.453268\n",
      "batch 788: loss 0.353960\n",
      "batch 789: loss 0.449078\n",
      "batch 790: loss 0.510356\n",
      "batch 791: loss 0.595874\n",
      "batch 792: loss 0.309853\n",
      "batch 793: loss 0.437563\n",
      "batch 794: loss 0.444977\n",
      "batch 795: loss 0.420016\n",
      "batch 796: loss 0.460062\n",
      "batch 797: loss 0.488923\n",
      "batch 798: loss 0.429396\n",
      "batch 799: loss 0.467384\n",
      "batch 800: loss 0.449149\n",
      "batch 801: loss 0.436526\n",
      "batch 802: loss 0.576137\n",
      "batch 803: loss 0.463761\n",
      "batch 804: loss 0.415852\n",
      "batch 805: loss 0.393452\n",
      "batch 806: loss 0.541785\n",
      "batch 807: loss 0.395753\n",
      "batch 808: loss 0.471702\n",
      "batch 809: loss 0.369181\n",
      "batch 810: loss 0.450015\n",
      "batch 811: loss 0.469088\n",
      "batch 812: loss 0.438023\n",
      "batch 813: loss 0.422669\n",
      "batch 814: loss 0.442196\n",
      "batch 815: loss 0.357928\n",
      "batch 816: loss 0.446803\n",
      "batch 817: loss 0.447191\n",
      "batch 818: loss 0.506343\n",
      "batch 819: loss 0.420925\n",
      "batch 820: loss 0.544814\n",
      "batch 821: loss 0.423886\n",
      "batch 822: loss 0.423901\n",
      "batch 823: loss 0.472340\n",
      "batch 824: loss 0.402414\n",
      "batch 825: loss 0.412705\n",
      "batch 826: loss 0.328083\n",
      "batch 827: loss 0.451861\n",
      "batch 828: loss 0.463361\n",
      "batch 829: loss 0.392904\n",
      "batch 830: loss 0.399383\n",
      "batch 831: loss 0.628604\n",
      "batch 832: loss 0.458584\n",
      "batch 833: loss 0.273416\n",
      "batch 834: loss 0.665752\n",
      "batch 835: loss 0.435612\n",
      "batch 836: loss 0.446079\n",
      "batch 837: loss 0.506032\n",
      "batch 838: loss 0.441869\n",
      "batch 839: loss 0.385832\n",
      "batch 840: loss 0.500532\n",
      "batch 841: loss 0.731330\n",
      "batch 842: loss 0.471084\n",
      "batch 843: loss 0.414255\n",
      "batch 844: loss 0.383110\n",
      "batch 845: loss 0.427983\n",
      "batch 846: loss 0.339793\n",
      "batch 847: loss 0.609468\n",
      "batch 848: loss 0.388463\n",
      "batch 849: loss 0.383811\n",
      "batch 850: loss 0.307604\n",
      "batch 851: loss 0.446280\n",
      "batch 852: loss 0.422100\n",
      "batch 853: loss 0.422824\n",
      "batch 854: loss 0.341949\n",
      "batch 855: loss 0.348163\n",
      "batch 856: loss 0.462677\n",
      "batch 857: loss 0.414348\n",
      "batch 858: loss 0.395533\n",
      "batch 859: loss 0.351713\n",
      "batch 860: loss 0.527222\n",
      "batch 861: loss 0.370264\n",
      "batch 862: loss 0.427248\n",
      "batch 863: loss 0.378126\n",
      "batch 864: loss 0.461452\n",
      "batch 865: loss 0.374722\n",
      "batch 866: loss 0.454421\n",
      "batch 867: loss 0.495227\n",
      "batch 868: loss 0.290804\n",
      "batch 869: loss 0.346062\n",
      "batch 870: loss 0.428301\n",
      "batch 871: loss 0.430231\n",
      "batch 872: loss 0.413404\n",
      "batch 873: loss 0.444214\n",
      "batch 874: loss 0.442178\n",
      "batch 875: loss 0.500495\n",
      "batch 876: loss 0.451412\n",
      "batch 877: loss 0.437484\n",
      "batch 878: loss 0.511617\n",
      "batch 879: loss 0.462791\n",
      "batch 880: loss 0.430778\n",
      "batch 881: loss 0.364177\n",
      "batch 882: loss 0.440757\n",
      "batch 883: loss 0.437648\n",
      "batch 884: loss 0.553665\n",
      "batch 885: loss 0.588224\n",
      "batch 886: loss 0.363988\n",
      "batch 887: loss 0.357189\n",
      "batch 888: loss 0.566219\n",
      "batch 889: loss 0.393563\n",
      "batch 890: loss 0.546639\n",
      "batch 891: loss 0.384782\n",
      "batch 892: loss 0.422495\n",
      "batch 893: loss 0.282961\n",
      "batch 894: loss 0.429081\n",
      "batch 895: loss 0.444753\n",
      "batch 896: loss 0.483097\n",
      "batch 897: loss 0.432945\n",
      "batch 898: loss 0.358281\n",
      "batch 899: loss 0.476374\n",
      "batch 900: loss 0.359916\n",
      "batch 901: loss 0.419986\n",
      "batch 902: loss 0.471697\n",
      "batch 903: loss 0.397148\n",
      "batch 904: loss 0.504585\n",
      "batch 905: loss 0.450528\n",
      "batch 906: loss 0.465178\n",
      "batch 907: loss 0.355766\n",
      "batch 908: loss 0.488457\n",
      "batch 909: loss 0.345345\n",
      "batch 910: loss 0.478819\n",
      "batch 911: loss 0.405226\n",
      "batch 912: loss 0.361181\n",
      "batch 913: loss 0.404363\n",
      "batch 914: loss 0.426272\n",
      "batch 915: loss 0.566719\n",
      "batch 916: loss 0.615529\n",
      "batch 917: loss 0.320596\n",
      "batch 918: loss 0.425717\n",
      "batch 919: loss 0.380749\n",
      "batch 920: loss 0.379131\n",
      "batch 921: loss 0.413151\n",
      "batch 922: loss 0.388996\n",
      "batch 923: loss 0.473998\n",
      "batch 924: loss 0.380874\n",
      "batch 925: loss 0.434032\n",
      "batch 926: loss 0.425163\n",
      "batch 927: loss 0.295819\n",
      "batch 928: loss 0.495196\n",
      "batch 929: loss 0.614884\n",
      "batch 930: loss 0.486789\n",
      "batch 931: loss 0.275789\n",
      "batch 932: loss 0.413384\n",
      "batch 933: loss 0.492940\n",
      "batch 934: loss 0.506557\n",
      "batch 935: loss 0.370631\n",
      "batch 936: loss 0.392659\n",
      "batch 937: loss 0.338936\n",
      "batch 938: loss 0.403582\n",
      "batch 939: loss 0.428047\n",
      "batch 940: loss 0.503473\n",
      "batch 941: loss 0.296421\n",
      "batch 942: loss 0.487477\n",
      "batch 943: loss 0.482814\n",
      "batch 944: loss 0.436813\n",
      "batch 945: loss 0.461122\n",
      "batch 946: loss 0.297019\n",
      "batch 947: loss 0.347415\n",
      "batch 948: loss 0.448305\n",
      "batch 949: loss 0.243680\n",
      "batch 950: loss 0.429214\n",
      "batch 951: loss 0.468520\n",
      "batch 952: loss 0.347370\n",
      "batch 953: loss 0.480094\n",
      "batch 954: loss 0.298683\n",
      "batch 955: loss 0.238036\n",
      "batch 956: loss 0.332834\n",
      "batch 957: loss 0.558677\n",
      "batch 958: loss 0.391535\n",
      "batch 959: loss 0.475420\n",
      "batch 960: loss 0.388960\n",
      "batch 961: loss 0.226553\n",
      "batch 962: loss 0.304619\n",
      "batch 963: loss 0.373963\n",
      "batch 964: loss 0.406210\n",
      "batch 965: loss 0.294669\n",
      "batch 966: loss 0.463206\n",
      "batch 967: loss 0.432678\n",
      "batch 968: loss 0.441641\n",
      "batch 969: loss 0.582459\n",
      "batch 970: loss 0.296871\n",
      "batch 971: loss 0.381345\n",
      "batch 972: loss 0.449121\n",
      "batch 973: loss 0.389018\n",
      "batch 974: loss 0.474831\n",
      "batch 975: loss 0.340283\n",
      "batch 976: loss 0.388229\n",
      "batch 977: loss 0.381370\n",
      "batch 978: loss 0.447489\n",
      "batch 979: loss 0.401311\n",
      "batch 980: loss 0.343717\n",
      "batch 981: loss 0.304953\n",
      "batch 982: loss 0.347068\n",
      "batch 983: loss 0.362216\n",
      "batch 984: loss 0.360672\n",
      "batch 985: loss 0.378870\n",
      "batch 986: loss 0.336729\n",
      "batch 987: loss 0.431157\n",
      "batch 988: loss 0.582100\n",
      "batch 989: loss 0.353697\n",
      "batch 990: loss 0.428220\n",
      "batch 991: loss 0.396456\n",
      "batch 992: loss 0.565962\n",
      "batch 993: loss 0.436552\n",
      "batch 994: loss 0.444004\n",
      "batch 995: loss 0.379177\n",
      "batch 996: loss 0.322546\n",
      "batch 997: loss 0.407576\n",
      "batch 998: loss 0.363181\n",
      "batch 999: loss 0.443866\n",
      "batch 1000: loss 0.396272\n",
      "batch 1001: loss 0.566058\n",
      "batch 1002: loss 0.380336\n",
      "batch 1003: loss 0.479110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1004: loss 0.461104\n",
      "batch 1005: loss 0.411058\n",
      "batch 1006: loss 0.406151\n",
      "batch 1007: loss 0.318702\n",
      "batch 1008: loss 0.387049\n",
      "batch 1009: loss 0.300489\n",
      "batch 1010: loss 0.482967\n",
      "batch 1011: loss 0.353073\n",
      "batch 1012: loss 0.334972\n",
      "batch 1013: loss 0.351002\n",
      "batch 1014: loss 0.410680\n",
      "batch 1015: loss 0.297117\n",
      "batch 1016: loss 0.421851\n",
      "batch 1017: loss 0.381679\n",
      "batch 1018: loss 0.423004\n",
      "batch 1019: loss 0.323045\n",
      "batch 1020: loss 0.405689\n",
      "batch 1021: loss 0.458315\n",
      "batch 1022: loss 0.270806\n",
      "batch 1023: loss 0.303467\n",
      "batch 1024: loss 0.492824\n",
      "batch 1025: loss 0.456655\n",
      "batch 1026: loss 0.347182\n",
      "batch 1027: loss 0.318217\n",
      "batch 1028: loss 0.604174\n",
      "batch 1029: loss 0.393159\n",
      "batch 1030: loss 0.459880\n",
      "batch 1031: loss 0.432685\n",
      "batch 1032: loss 0.442549\n",
      "batch 1033: loss 0.441185\n",
      "batch 1034: loss 0.344294\n",
      "batch 1035: loss 0.386401\n",
      "batch 1036: loss 0.385947\n",
      "batch 1037: loss 0.439399\n",
      "batch 1038: loss 0.399009\n",
      "batch 1039: loss 0.329435\n",
      "batch 1040: loss 0.398173\n",
      "batch 1041: loss 0.549100\n",
      "batch 1042: loss 0.412910\n",
      "batch 1043: loss 0.441246\n",
      "batch 1044: loss 0.442367\n",
      "batch 1045: loss 0.488719\n",
      "batch 1046: loss 0.436097\n",
      "batch 1047: loss 0.287585\n",
      "batch 1048: loss 0.294106\n",
      "batch 1049: loss 0.461472\n",
      "batch 1050: loss 0.370687\n",
      "batch 1051: loss 0.523298\n",
      "batch 1052: loss 0.309808\n",
      "batch 1053: loss 0.422426\n",
      "batch 1054: loss 0.398085\n",
      "batch 1055: loss 0.541364\n",
      "batch 1056: loss 0.340841\n",
      "batch 1057: loss 0.400509\n",
      "batch 1058: loss 0.444968\n",
      "batch 1059: loss 0.391167\n",
      "batch 1060: loss 0.460576\n",
      "batch 1061: loss 0.481546\n",
      "batch 1062: loss 0.481166\n",
      "batch 1063: loss 0.543743\n",
      "batch 1064: loss 0.408565\n",
      "batch 1065: loss 0.469173\n",
      "batch 1066: loss 0.466640\n",
      "batch 1067: loss 0.378781\n",
      "batch 1068: loss 0.431061\n",
      "batch 1069: loss 0.382818\n",
      "batch 1070: loss 0.436369\n",
      "batch 1071: loss 0.419468\n",
      "batch 1072: loss 0.448302\n",
      "batch 1073: loss 0.306714\n",
      "batch 1074: loss 0.439894\n",
      "batch 1075: loss 0.420176\n",
      "batch 1076: loss 0.423407\n",
      "batch 1077: loss 0.365128\n",
      "batch 1078: loss 0.399194\n",
      "batch 1079: loss 0.400519\n",
      "batch 1080: loss 0.290283\n",
      "batch 1081: loss 0.327103\n",
      "batch 1082: loss 0.402331\n",
      "batch 1083: loss 0.421263\n",
      "batch 1084: loss 0.449034\n",
      "batch 1085: loss 0.452973\n",
      "batch 1086: loss 0.357171\n",
      "batch 1087: loss 0.382211\n",
      "batch 1088: loss 0.459983\n",
      "batch 1089: loss 0.412222\n",
      "batch 1090: loss 0.355063\n",
      "batch 1091: loss 0.345917\n",
      "batch 1092: loss 0.494745\n",
      "batch 1093: loss 0.466614\n",
      "batch 1094: loss 0.463550\n",
      "batch 1095: loss 0.487702\n",
      "batch 1096: loss 0.337955\n",
      "batch 1097: loss 0.501478\n",
      "batch 1098: loss 0.388576\n",
      "batch 1099: loss 0.461513\n",
      "batch 1100: loss 0.517390\n",
      "batch 1101: loss 0.344327\n",
      "batch 1102: loss 0.482423\n",
      "batch 1103: loss 0.395718\n",
      "batch 1104: loss 0.385555\n",
      "batch 1105: loss 0.372612\n",
      "batch 1106: loss 0.548839\n",
      "batch 1107: loss 0.489372\n",
      "batch 1108: loss 0.338737\n",
      "batch 1109: loss 0.400506\n",
      "batch 1110: loss 0.414481\n",
      "batch 1111: loss 0.365700\n",
      "batch 1112: loss 0.386000\n",
      "batch 1113: loss 0.307529\n",
      "batch 1114: loss 0.410753\n",
      "batch 1115: loss 0.380195\n",
      "batch 1116: loss 0.369359\n",
      "batch 1117: loss 0.340063\n",
      "batch 1118: loss 0.402423\n",
      "batch 1119: loss 0.329170\n",
      "batch 1120: loss 0.428844\n",
      "batch 1121: loss 0.487601\n",
      "batch 1122: loss 0.469268\n",
      "batch 1123: loss 0.349544\n",
      "batch 1124: loss 0.365988\n",
      "batch 1125: loss 0.402978\n",
      "batch 1126: loss 0.423266\n",
      "batch 1127: loss 0.345764\n",
      "batch 1128: loss 0.372024\n",
      "batch 1129: loss 0.363751\n",
      "batch 1130: loss 0.377982\n",
      "batch 1131: loss 0.308115\n",
      "batch 1132: loss 0.504245\n",
      "batch 1133: loss 0.334910\n",
      "batch 1134: loss 0.380876\n",
      "batch 1135: loss 0.474017\n",
      "batch 1136: loss 0.360082\n",
      "batch 1137: loss 0.453410\n",
      "batch 1138: loss 0.356503\n",
      "batch 1139: loss 0.421545\n",
      "batch 1140: loss 0.345804\n",
      "batch 1141: loss 0.423137\n",
      "batch 1142: loss 0.302479\n",
      "batch 1143: loss 0.408703\n",
      "batch 1144: loss 0.354055\n",
      "batch 1145: loss 0.326055\n",
      "batch 1146: loss 0.319418\n",
      "batch 1147: loss 0.353960\n",
      "batch 1148: loss 0.271233\n",
      "batch 1149: loss 0.358399\n",
      "batch 1150: loss 0.276488\n",
      "batch 1151: loss 0.473109\n",
      "batch 1152: loss 0.452670\n",
      "batch 1153: loss 0.414086\n",
      "batch 1154: loss 0.461377\n",
      "batch 1155: loss 0.478598\n",
      "batch 1156: loss 0.331258\n",
      "batch 1157: loss 0.326601\n",
      "batch 1158: loss 0.577057\n",
      "batch 1159: loss 0.377953\n",
      "batch 1160: loss 0.450202\n",
      "batch 1161: loss 0.272458\n",
      "batch 1162: loss 0.581796\n",
      "batch 1163: loss 0.493554\n",
      "batch 1164: loss 0.388150\n",
      "batch 1165: loss 0.430023\n",
      "batch 1166: loss 0.366866\n",
      "batch 1167: loss 0.436954\n",
      "batch 1168: loss 0.421827\n",
      "batch 1169: loss 0.517498\n",
      "batch 1170: loss 0.347036\n",
      "batch 1171: loss 0.460062\n",
      "batch 1172: loss 0.294025\n",
      "batch 1173: loss 0.457990\n",
      "batch 1174: loss 0.500989\n",
      "batch 1175: loss 0.321878\n",
      "batch 1176: loss 0.374467\n",
      "batch 1177: loss 0.464471\n",
      "batch 1178: loss 0.403000\n",
      "batch 1179: loss 0.466845\n",
      "batch 1180: loss 0.273862\n",
      "batch 1181: loss 0.366412\n",
      "batch 1182: loss 0.337630\n",
      "batch 1183: loss 0.480849\n",
      "batch 1184: loss 0.408406\n",
      "batch 1185: loss 0.417022\n",
      "batch 1186: loss 0.278292\n",
      "batch 1187: loss 0.418808\n",
      "batch 1188: loss 0.453577\n",
      "batch 1189: loss 0.440386\n",
      "batch 1190: loss 0.369086\n",
      "batch 1191: loss 0.412247\n",
      "batch 1192: loss 0.500058\n",
      "batch 1193: loss 0.318934\n",
      "batch 1194: loss 0.376909\n",
      "batch 1195: loss 0.370472\n",
      "batch 1196: loss 0.382966\n",
      "batch 1197: loss 0.432295\n",
      "batch 1198: loss 0.310599\n",
      "batch 1199: loss 0.378445\n",
      "batch 1200: loss 0.427731\n",
      "batch 1201: loss 0.348257\n",
      "batch 1202: loss 0.388191\n",
      "batch 1203: loss 0.520906\n",
      "batch 1204: loss 0.301877\n",
      "batch 1205: loss 0.371652\n",
      "batch 1206: loss 0.408176\n",
      "batch 1207: loss 0.343621\n",
      "batch 1208: loss 0.269353\n",
      "batch 1209: loss 0.363308\n",
      "batch 1210: loss 0.487792\n",
      "batch 1211: loss 0.373810\n",
      "batch 1212: loss 0.438849\n",
      "batch 1213: loss 0.441107\n",
      "batch 1214: loss 0.375655\n",
      "batch 1215: loss 0.349612\n",
      "batch 1216: loss 0.294755\n",
      "batch 1217: loss 0.415377\n",
      "batch 1218: loss 0.396294\n",
      "batch 1219: loss 0.364069\n",
      "batch 1220: loss 0.310328\n",
      "batch 1221: loss 0.399298\n",
      "batch 1222: loss 0.455863\n",
      "batch 1223: loss 0.378043\n",
      "batch 1224: loss 0.371485\n",
      "batch 1225: loss 0.429932\n",
      "batch 1226: loss 0.328960\n",
      "batch 1227: loss 0.376179\n",
      "batch 1228: loss 0.469299\n",
      "batch 1229: loss 0.368161\n",
      "batch 1230: loss 0.370569\n",
      "batch 1231: loss 0.455612\n",
      "batch 1232: loss 0.309885\n",
      "batch 1233: loss 0.424562\n",
      "batch 1234: loss 0.450359\n",
      "batch 1235: loss 0.363768\n",
      "batch 1236: loss 0.453268\n",
      "batch 1237: loss 0.414298\n",
      "batch 1238: loss 0.393143\n",
      "batch 1239: loss 0.508216\n",
      "batch 1240: loss 0.381137\n",
      "batch 1241: loss 0.351432\n",
      "batch 1242: loss 0.463036\n",
      "batch 1243: loss 0.263757\n",
      "batch 1244: loss 0.434181\n",
      "batch 1245: loss 0.596709\n",
      "batch 1246: loss 0.294528\n",
      "batch 1247: loss 0.459763\n",
      "batch 1248: loss 0.343777\n",
      "batch 1249: loss 0.367900\n",
      "batch 1250: loss 0.379020\n",
      "batch 1251: loss 0.373572\n",
      "batch 1252: loss 0.435091\n",
      "batch 1253: loss 0.411329\n",
      "batch 1254: loss 0.413579\n",
      "batch 1255: loss 0.318623\n",
      "batch 1256: loss 0.226876\n",
      "batch 1257: loss 0.321252\n",
      "batch 1258: loss 0.298949\n",
      "batch 1259: loss 0.414691\n",
      "batch 1260: loss 0.376323\n",
      "batch 1261: loss 0.369435\n",
      "batch 1262: loss 0.416044\n",
      "batch 1263: loss 0.324912\n",
      "batch 1264: loss 0.487324\n",
      "batch 1265: loss 0.320700\n",
      "batch 1266: loss 0.380701\n",
      "batch 1267: loss 0.341255\n",
      "batch 1268: loss 0.324668\n",
      "batch 1269: loss 0.376327\n",
      "batch 1270: loss 0.466862\n",
      "batch 1271: loss 0.304929\n",
      "batch 1272: loss 0.258316\n",
      "batch 1273: loss 0.459047\n",
      "batch 1274: loss 0.361638\n",
      "batch 1275: loss 0.399393\n",
      "batch 1276: loss 0.354184\n",
      "batch 1277: loss 0.316974\n",
      "batch 1278: loss 0.297842\n",
      "batch 1279: loss 0.476097\n",
      "batch 1280: loss 0.474289\n",
      "batch 1281: loss 0.400836\n",
      "batch 1282: loss 0.345121\n",
      "batch 1283: loss 0.329007\n",
      "batch 1284: loss 0.385673\n",
      "batch 1285: loss 0.534954\n",
      "batch 1286: loss 0.378143\n",
      "batch 1287: loss 0.344512\n",
      "batch 1288: loss 0.411633\n",
      "batch 1289: loss 0.389479\n",
      "batch 1290: loss 0.399880\n",
      "batch 1291: loss 0.271515\n",
      "batch 1292: loss 0.371783\n",
      "batch 1293: loss 0.453913\n",
      "batch 1294: loss 0.574428\n",
      "batch 1295: loss 0.364350\n",
      "batch 1296: loss 0.349619\n",
      "batch 1297: loss 0.521112\n",
      "batch 1298: loss 0.289103\n",
      "batch 1299: loss 0.402963\n",
      "batch 1300: loss 0.306589\n",
      "batch 1301: loss 0.407219\n",
      "batch 1302: loss 0.310252\n",
      "batch 1303: loss 0.431099\n",
      "batch 1304: loss 0.368476\n",
      "batch 1305: loss 0.365138\n",
      "batch 1306: loss 0.407588\n",
      "batch 1307: loss 0.406059\n",
      "batch 1308: loss 0.396660\n",
      "batch 1309: loss 0.430007\n",
      "batch 1310: loss 0.383270\n",
      "batch 1311: loss 0.320675\n",
      "batch 1312: loss 0.476069\n",
      "batch 1313: loss 0.378213\n",
      "batch 1314: loss 0.415520\n",
      "batch 1315: loss 0.358402\n",
      "batch 1316: loss 0.368241\n",
      "batch 1317: loss 0.343529\n",
      "batch 1318: loss 0.338314\n",
      "batch 1319: loss 0.323589\n",
      "batch 1320: loss 0.423776\n",
      "batch 1321: loss 0.372176\n",
      "batch 1322: loss 0.320872\n",
      "batch 1323: loss 0.382310\n",
      "batch 1324: loss 0.286500\n",
      "batch 1325: loss 0.284076\n",
      "batch 1326: loss 0.381605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1327: loss 0.549511\n",
      "batch 1328: loss 0.424278\n",
      "batch 1329: loss 0.308777\n",
      "batch 1330: loss 0.333024\n",
      "batch 1331: loss 0.335625\n",
      "batch 1332: loss 0.284447\n",
      "batch 1333: loss 0.234141\n",
      "batch 1334: loss 0.340525\n",
      "batch 1335: loss 0.312562\n",
      "batch 1336: loss 0.348706\n",
      "batch 1337: loss 0.353475\n",
      "batch 1338: loss 0.266360\n",
      "batch 1339: loss 0.415471\n",
      "batch 1340: loss 0.348345\n",
      "batch 1341: loss 0.295475\n",
      "batch 1342: loss 0.450290\n",
      "batch 1343: loss 0.343909\n",
      "batch 1344: loss 0.319720\n",
      "batch 1345: loss 0.397838\n",
      "batch 1346: loss 0.339796\n",
      "batch 1347: loss 0.262226\n",
      "batch 1348: loss 0.293367\n",
      "batch 1349: loss 0.362927\n",
      "batch 1350: loss 0.368126\n",
      "batch 1351: loss 0.329738\n",
      "batch 1352: loss 0.410780\n",
      "batch 1353: loss 0.344920\n",
      "batch 1354: loss 0.439480\n",
      "batch 1355: loss 0.300559\n",
      "batch 1356: loss 0.327560\n",
      "batch 1357: loss 0.351913\n",
      "batch 1358: loss 0.362051\n",
      "batch 1359: loss 0.361778\n",
      "batch 1360: loss 0.364720\n",
      "batch 1361: loss 0.442710\n",
      "batch 1362: loss 0.337543\n",
      "batch 1363: loss 0.342436\n",
      "batch 1364: loss 0.362549\n",
      "batch 1365: loss 0.496380\n",
      "batch 1366: loss 0.363455\n",
      "batch 1367: loss 0.436989\n",
      "batch 1368: loss 0.422495\n",
      "batch 1369: loss 0.397829\n",
      "batch 1370: loss 0.302212\n",
      "batch 1371: loss 0.319160\n",
      "batch 1372: loss 0.361878\n",
      "batch 1373: loss 0.314366\n",
      "batch 1374: loss 0.477204\n",
      "batch 1375: loss 0.445626\n",
      "batch 1376: loss 0.496315\n",
      "batch 1377: loss 0.421287\n",
      "batch 1378: loss 0.393605\n",
      "batch 1379: loss 0.441309\n",
      "batch 1380: loss 0.363171\n",
      "batch 1381: loss 0.446313\n",
      "batch 1382: loss 0.303494\n",
      "batch 1383: loss 0.300648\n",
      "batch 1384: loss 0.527908\n",
      "batch 1385: loss 0.390037\n",
      "batch 1386: loss 0.334385\n",
      "batch 1387: loss 0.407070\n",
      "batch 1388: loss 0.322385\n",
      "batch 1389: loss 0.336039\n",
      "batch 1390: loss 0.302857\n",
      "batch 1391: loss 0.381597\n",
      "batch 1392: loss 0.502720\n",
      "batch 1393: loss 0.307821\n",
      "batch 1394: loss 0.355229\n",
      "batch 1395: loss 0.401645\n",
      "batch 1396: loss 0.381453\n",
      "batch 1397: loss 0.370156\n",
      "batch 1398: loss 0.320124\n",
      "batch 1399: loss 0.443401\n",
      "batch 1400: loss 0.423458\n",
      "batch 1401: loss 0.441116\n",
      "batch 1402: loss 0.524459\n",
      "batch 1403: loss 0.450087\n",
      "batch 1404: loss 0.410489\n",
      "batch 1405: loss 0.473092\n",
      "batch 1406: loss 0.417859\n",
      "batch 1407: loss 0.466638\n",
      "batch 1408: loss 0.394527\n",
      "batch 1409: loss 0.363727\n",
      "batch 1410: loss 0.274805\n",
      "batch 1411: loss 0.335778\n",
      "batch 1412: loss 0.448211\n",
      "batch 1413: loss 0.343402\n",
      "batch 1414: loss 0.346950\n",
      "batch 1415: loss 0.334133\n",
      "batch 1416: loss 0.509132\n",
      "batch 1417: loss 0.322176\n",
      "batch 1418: loss 0.430700\n",
      "batch 1419: loss 0.356715\n",
      "batch 1420: loss 0.275078\n",
      "batch 1421: loss 0.434982\n",
      "batch 1422: loss 0.393326\n",
      "batch 1423: loss 0.423182\n",
      "batch 1424: loss 0.373873\n",
      "batch 1425: loss 0.361751\n",
      "batch 1426: loss 0.483395\n",
      "batch 1427: loss 0.318599\n",
      "batch 1428: loss 0.358084\n",
      "batch 1429: loss 0.392791\n",
      "batch 1430: loss 0.288279\n",
      "batch 1431: loss 0.376522\n",
      "batch 1432: loss 0.416950\n",
      "batch 1433: loss 0.337665\n",
      "batch 1434: loss 0.328901\n",
      "batch 1435: loss 0.296904\n",
      "batch 1436: loss 0.446808\n",
      "batch 1437: loss 0.281611\n",
      "batch 1438: loss 0.431287\n",
      "batch 1439: loss 0.352030\n",
      "batch 1440: loss 0.345921\n",
      "batch 1441: loss 0.389027\n",
      "batch 1442: loss 0.356190\n",
      "batch 1443: loss 0.292881\n",
      "batch 1444: loss 0.378926\n",
      "batch 1445: loss 0.363462\n",
      "batch 1446: loss 0.327710\n",
      "batch 1447: loss 0.294324\n",
      "batch 1448: loss 0.492121\n",
      "batch 1449: loss 0.312050\n",
      "batch 1450: loss 0.381906\n",
      "batch 1451: loss 0.571781\n",
      "batch 1452: loss 0.409494\n",
      "batch 1453: loss 0.419636\n",
      "batch 1454: loss 0.279023\n",
      "batch 1455: loss 0.337160\n",
      "batch 1456: loss 0.390243\n",
      "batch 1457: loss 0.461178\n",
      "batch 1458: loss 0.405025\n",
      "batch 1459: loss 0.321291\n",
      "batch 1460: loss 0.268931\n",
      "batch 1461: loss 0.429995\n",
      "batch 1462: loss 0.313365\n",
      "batch 1463: loss 0.401946\n",
      "batch 1464: loss 0.390017\n",
      "batch 1465: loss 0.262392\n",
      "batch 1466: loss 0.390628\n",
      "batch 1467: loss 0.370270\n",
      "batch 1468: loss 0.324594\n",
      "batch 1469: loss 0.450666\n",
      "batch 1470: loss 0.258380\n",
      "batch 1471: loss 0.335960\n",
      "batch 1472: loss 0.428526\n",
      "batch 1473: loss 0.380316\n",
      "batch 1474: loss 0.340840\n",
      "batch 1475: loss 0.425819\n",
      "batch 1476: loss 0.463102\n",
      "batch 1477: loss 0.343681\n",
      "batch 1478: loss 0.424757\n",
      "batch 1479: loss 0.309790\n",
      "batch 1480: loss 0.320783\n",
      "batch 1481: loss 0.427270\n",
      "batch 1482: loss 0.303120\n",
      "batch 1483: loss 0.361631\n",
      "batch 1484: loss 0.340301\n",
      "batch 1485: loss 0.344326\n",
      "batch 1486: loss 0.354967\n",
      "batch 1487: loss 0.514013\n",
      "batch 1488: loss 0.362892\n",
      "batch 1489: loss 0.449729\n",
      "batch 1490: loss 0.360155\n",
      "batch 1491: loss 0.378760\n",
      "batch 1492: loss 0.256343\n",
      "batch 1493: loss 0.354067\n",
      "batch 1494: loss 0.388061\n",
      "batch 1495: loss 0.320161\n",
      "batch 1496: loss 0.355092\n",
      "batch 1497: loss 0.400835\n",
      "batch 1498: loss 0.442360\n",
      "batch 1499: loss 0.460386\n",
      "batch 1500: loss 0.407666\n",
      "batch 1501: loss 0.244018\n",
      "batch 1502: loss 0.341869\n",
      "batch 1503: loss 0.398863\n",
      "batch 1504: loss 0.319596\n",
      "batch 1505: loss 0.327123\n",
      "batch 1506: loss 0.356419\n",
      "batch 1507: loss 0.407421\n",
      "batch 1508: loss 0.438796\n",
      "batch 1509: loss 0.323289\n",
      "batch 1510: loss 0.451806\n",
      "batch 1511: loss 0.301251\n",
      "batch 1512: loss 0.345681\n",
      "batch 1513: loss 0.329532\n",
      "batch 1514: loss 0.464597\n",
      "batch 1515: loss 0.410466\n",
      "batch 1516: loss 0.473967\n",
      "batch 1517: loss 0.460229\n",
      "batch 1518: loss 0.309870\n",
      "batch 1519: loss 0.371391\n",
      "batch 1520: loss 0.316311\n",
      "batch 1521: loss 0.369061\n",
      "batch 1522: loss 0.353328\n",
      "batch 1523: loss 0.364255\n",
      "batch 1524: loss 0.488100\n",
      "batch 1525: loss 0.284626\n",
      "batch 1526: loss 0.331122\n",
      "batch 1527: loss 0.359416\n",
      "batch 1528: loss 0.343290\n",
      "batch 1529: loss 0.318128\n",
      "batch 1530: loss 0.361228\n",
      "batch 1531: loss 0.386186\n",
      "batch 1532: loss 0.312515\n",
      "batch 1533: loss 0.415952\n",
      "batch 1534: loss 0.256478\n",
      "batch 1535: loss 0.273631\n",
      "batch 1536: loss 0.313668\n",
      "batch 1537: loss 0.500591\n",
      "batch 1538: loss 0.346738\n",
      "batch 1539: loss 0.253100\n",
      "batch 1540: loss 0.337285\n",
      "batch 1541: loss 0.394812\n",
      "batch 1542: loss 0.422692\n",
      "batch 1543: loss 0.404332\n",
      "batch 1544: loss 0.469308\n",
      "batch 1545: loss 0.413089\n",
      "batch 1546: loss 0.311036\n",
      "batch 1547: loss 0.369062\n",
      "batch 1548: loss 0.368053\n",
      "batch 1549: loss 0.333633\n",
      "batch 1550: loss 0.360634\n",
      "batch 1551: loss 0.384403\n",
      "batch 1552: loss 0.315117\n",
      "batch 1553: loss 0.360983\n",
      "batch 1554: loss 0.273086\n",
      "batch 1555: loss 0.381209\n",
      "batch 1556: loss 0.260052\n",
      "batch 1557: loss 0.426417\n",
      "batch 1558: loss 0.337331\n",
      "batch 1559: loss 0.340667\n",
      "batch 1560: loss 0.482617\n",
      "batch 1561: loss 0.400761\n",
      "batch 1562: loss 0.323715\n",
      "batch 1563: loss 0.268822\n",
      "batch 1564: loss 0.267275\n",
      "batch 1565: loss 0.379291\n",
      "batch 1566: loss 0.572577\n",
      "batch 1567: loss 0.413154\n",
      "batch 1568: loss 0.341177\n",
      "batch 1569: loss 0.289764\n",
      "batch 1570: loss 0.280968\n",
      "batch 1571: loss 0.311611\n",
      "batch 1572: loss 0.320186\n",
      "batch 1573: loss 0.328721\n",
      "batch 1574: loss 0.448035\n",
      "batch 1575: loss 0.460151\n",
      "batch 1576: loss 0.302826\n",
      "batch 1577: loss 0.418829\n",
      "batch 1578: loss 0.278967\n",
      "batch 1579: loss 0.412877\n",
      "batch 1580: loss 0.374354\n",
      "batch 1581: loss 0.384267\n",
      "batch 1582: loss 0.289254\n",
      "batch 1583: loss 0.323589\n",
      "batch 1584: loss 0.349192\n",
      "batch 1585: loss 0.359904\n",
      "batch 1586: loss 0.324065\n",
      "batch 1587: loss 0.325017\n",
      "batch 1588: loss 0.367133\n",
      "batch 1589: loss 0.279727\n",
      "batch 1590: loss 0.483508\n",
      "batch 1591: loss 0.549146\n",
      "batch 1592: loss 0.383814\n",
      "batch 1593: loss 0.348066\n",
      "batch 1594: loss 0.425114\n",
      "batch 1595: loss 0.308383\n",
      "batch 1596: loss 0.274371\n",
      "batch 1597: loss 0.379606\n",
      "batch 1598: loss 0.405251\n",
      "batch 1599: loss 0.436002\n",
      "batch 1600: loss 0.386162\n",
      "batch 1601: loss 0.393773\n",
      "batch 1602: loss 0.458849\n",
      "batch 1603: loss 0.490650\n",
      "batch 1604: loss 0.345276\n",
      "batch 1605: loss 0.308578\n",
      "batch 1606: loss 0.394942\n",
      "batch 1607: loss 0.271228\n",
      "batch 1608: loss 0.424339\n",
      "batch 1609: loss 0.319183\n",
      "batch 1610: loss 0.389148\n",
      "batch 1611: loss 0.340197\n",
      "batch 1612: loss 0.391354\n",
      "batch 1613: loss 0.331295\n",
      "batch 1614: loss 0.502265\n",
      "batch 1615: loss 0.408461\n",
      "batch 1616: loss 0.470536\n",
      "batch 1617: loss 0.324404\n",
      "batch 1618: loss 0.285586\n",
      "batch 1619: loss 0.481665\n",
      "batch 1620: loss 0.340576\n",
      "batch 1621: loss 0.451967\n",
      "batch 1622: loss 0.348141\n",
      "batch 1623: loss 0.397026\n",
      "batch 1624: loss 0.474698\n",
      "batch 1625: loss 0.402545\n",
      "batch 1626: loss 0.454452\n",
      "batch 1627: loss 0.297653\n",
      "batch 1628: loss 0.321949\n",
      "batch 1629: loss 0.445687\n",
      "batch 1630: loss 0.382396\n",
      "batch 1631: loss 0.210434\n",
      "batch 1632: loss 0.387900\n",
      "batch 1633: loss 0.469413\n",
      "batch 1634: loss 0.460601\n",
      "batch 1635: loss 0.317826\n",
      "batch 1636: loss 0.304455\n",
      "batch 1637: loss 0.457842\n",
      "batch 1638: loss 0.275888\n",
      "batch 1639: loss 0.298328\n",
      "batch 1640: loss 0.309389\n",
      "batch 1641: loss 0.587957\n",
      "batch 1642: loss 0.447802\n",
      "batch 1643: loss 0.266228\n",
      "batch 1644: loss 0.293474\n",
      "batch 1645: loss 0.280297\n",
      "batch 1646: loss 0.543336\n",
      "batch 1647: loss 0.299365\n",
      "batch 1648: loss 0.353096\n",
      "batch 1649: loss 0.305957\n",
      "batch 1650: loss 0.355231\n",
      "batch 1651: loss 0.340597\n",
      "batch 1652: loss 0.331145\n",
      "batch 1653: loss 0.324426\n",
      "batch 1654: loss 0.387082\n",
      "batch 1655: loss 0.528272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1656: loss 0.463546\n",
      "batch 1657: loss 0.295563\n",
      "batch 1658: loss 0.252084\n",
      "batch 1659: loss 0.353326\n",
      "batch 1660: loss 0.350303\n",
      "batch 1661: loss 0.481378\n",
      "batch 1662: loss 0.391053\n",
      "batch 1663: loss 0.354312\n",
      "batch 1664: loss 0.556900\n",
      "batch 1665: loss 0.443560\n",
      "batch 1666: loss 0.485732\n",
      "batch 1667: loss 0.428042\n",
      "batch 1668: loss 0.287435\n",
      "batch 1669: loss 0.327201\n",
      "batch 1670: loss 0.333097\n",
      "batch 1671: loss 0.336975\n",
      "batch 1672: loss 0.385755\n",
      "batch 1673: loss 0.352227\n",
      "batch 1674: loss 0.350296\n",
      "batch 1675: loss 0.212919\n",
      "batch 1676: loss 0.413250\n",
      "batch 1677: loss 0.271692\n",
      "batch 1678: loss 0.334314\n",
      "batch 1679: loss 0.382896\n",
      "batch 1680: loss 0.315055\n",
      "batch 1681: loss 0.374542\n",
      "batch 1682: loss 0.277579\n",
      "batch 1683: loss 0.346546\n",
      "batch 1684: loss 0.491843\n",
      "batch 1685: loss 0.317153\n",
      "batch 1686: loss 0.355342\n",
      "batch 1687: loss 0.318513\n",
      "batch 1688: loss 0.359233\n",
      "batch 1689: loss 0.530930\n",
      "batch 1690: loss 0.376431\n",
      "batch 1691: loss 0.436257\n",
      "batch 1692: loss 0.505868\n",
      "batch 1693: loss 0.412747\n",
      "batch 1694: loss 0.313221\n",
      "batch 1695: loss 0.496414\n",
      "batch 1696: loss 0.359425\n",
      "batch 1697: loss 0.338713\n",
      "batch 1698: loss 0.370672\n",
      "batch 1699: loss 0.390248\n",
      "batch 1700: loss 0.462083\n",
      "batch 1701: loss 0.283464\n",
      "batch 1702: loss 0.373471\n",
      "batch 1703: loss 0.311446\n",
      "batch 1704: loss 0.402754\n",
      "batch 1705: loss 0.465863\n",
      "batch 1706: loss 0.371109\n",
      "batch 1707: loss 0.301001\n",
      "batch 1708: loss 0.366254\n",
      "batch 1709: loss 0.377809\n",
      "batch 1710: loss 0.337706\n",
      "batch 1711: loss 0.347445\n",
      "batch 1712: loss 0.300959\n",
      "batch 1713: loss 0.416177\n",
      "batch 1714: loss 0.265157\n",
      "batch 1715: loss 0.284969\n",
      "batch 1716: loss 0.426167\n",
      "batch 1717: loss 0.317464\n",
      "batch 1718: loss 0.375380\n",
      "batch 1719: loss 0.353421\n",
      "batch 1720: loss 0.331244\n",
      "batch 1721: loss 0.450394\n",
      "batch 1722: loss 0.427021\n",
      "batch 1723: loss 0.256540\n",
      "batch 1724: loss 0.497970\n",
      "batch 1725: loss 0.359622\n",
      "batch 1726: loss 0.430213\n",
      "batch 1727: loss 0.464528\n",
      "batch 1728: loss 0.344554\n",
      "batch 1729: loss 0.398184\n",
      "batch 1730: loss 0.472990\n",
      "batch 1731: loss 0.482786\n",
      "batch 1732: loss 0.347025\n",
      "batch 1733: loss 0.299335\n",
      "batch 1734: loss 0.344339\n",
      "batch 1735: loss 0.359365\n",
      "batch 1736: loss 0.282919\n",
      "batch 1737: loss 0.404190\n",
      "batch 1738: loss 0.435048\n",
      "batch 1739: loss 0.435391\n",
      "batch 1740: loss 0.361695\n",
      "batch 1741: loss 0.252474\n",
      "batch 1742: loss 0.288665\n",
      "batch 1743: loss 0.278405\n",
      "batch 1744: loss 0.440729\n",
      "batch 1745: loss 0.411429\n",
      "batch 1746: loss 0.260681\n",
      "batch 1747: loss 0.401097\n",
      "batch 1748: loss 0.423309\n",
      "batch 1749: loss 0.358314\n",
      "batch 1750: loss 0.251083\n",
      "batch 1751: loss 0.372294\n",
      "batch 1752: loss 0.367589\n",
      "batch 1753: loss 0.278852\n",
      "batch 1754: loss 0.500307\n",
      "batch 1755: loss 0.409135\n",
      "batch 1756: loss 0.349579\n",
      "batch 1757: loss 0.351846\n",
      "batch 1758: loss 0.468282\n",
      "batch 1759: loss 0.401107\n",
      "batch 1760: loss 0.364924\n",
      "batch 1761: loss 0.378111\n",
      "batch 1762: loss 0.340635\n",
      "batch 1763: loss 0.399044\n",
      "batch 1764: loss 0.284269\n",
      "batch 1765: loss 0.369993\n",
      "batch 1766: loss 0.337522\n",
      "batch 1767: loss 0.478709\n",
      "batch 1768: loss 0.342587\n",
      "batch 1769: loss 0.331647\n",
      "batch 1770: loss 0.415084\n",
      "batch 1771: loss 0.261011\n",
      "batch 1772: loss 0.300696\n",
      "batch 1773: loss 0.351992\n",
      "batch 1774: loss 0.323368\n",
      "batch 1775: loss 0.292645\n",
      "batch 1776: loss 0.328780\n",
      "batch 1777: loss 0.361864\n",
      "batch 1778: loss 0.352893\n",
      "batch 1779: loss 0.313504\n",
      "batch 1780: loss 0.394386\n",
      "batch 1781: loss 0.407152\n",
      "batch 1782: loss 0.340416\n",
      "batch 1783: loss 0.404600\n",
      "batch 1784: loss 0.353164\n",
      "batch 1785: loss 0.352381\n",
      "batch 1786: loss 0.311929\n",
      "batch 1787: loss 0.290635\n",
      "batch 1788: loss 0.462513\n",
      "batch 1789: loss 0.286357\n",
      "batch 1790: loss 0.260734\n",
      "batch 1791: loss 0.426471\n",
      "batch 1792: loss 0.410774\n",
      "batch 1793: loss 0.217188\n",
      "batch 1794: loss 0.345882\n",
      "batch 1795: loss 0.345162\n",
      "batch 1796: loss 0.399160\n",
      "batch 1797: loss 0.385366\n",
      "batch 1798: loss 0.314770\n",
      "batch 1799: loss 0.304557\n",
      "batch 1800: loss 0.309247\n",
      "batch 1801: loss 0.257803\n",
      "batch 1802: loss 0.387981\n",
      "batch 1803: loss 0.329272\n",
      "batch 1804: loss 0.362833\n",
      "batch 1805: loss 0.425617\n",
      "batch 1806: loss 0.383117\n",
      "batch 1807: loss 0.392553\n",
      "batch 1808: loss 0.238831\n",
      "batch 1809: loss 0.343146\n",
      "batch 1810: loss 0.248797\n",
      "batch 1811: loss 0.417118\n",
      "batch 1812: loss 0.314219\n",
      "batch 1813: loss 0.425080\n",
      "batch 1814: loss 0.375933\n",
      "batch 1815: loss 0.352596\n",
      "batch 1816: loss 0.365752\n",
      "batch 1817: loss 0.311830\n",
      "batch 1818: loss 0.310595\n",
      "batch 1819: loss 0.428500\n",
      "batch 1820: loss 0.330303\n",
      "batch 1821: loss 0.322696\n",
      "batch 1822: loss 0.344115\n",
      "batch 1823: loss 0.348242\n",
      "batch 1824: loss 0.385629\n",
      "batch 1825: loss 0.276775\n",
      "batch 1826: loss 0.411458\n",
      "batch 1827: loss 0.473425\n",
      "batch 1828: loss 0.347475\n",
      "batch 1829: loss 0.374517\n",
      "batch 1830: loss 0.404319\n",
      "batch 1831: loss 0.226108\n",
      "batch 1832: loss 0.368040\n",
      "batch 1833: loss 0.439783\n",
      "batch 1834: loss 0.380284\n",
      "batch 1835: loss 0.387988\n",
      "batch 1836: loss 0.223371\n",
      "batch 1837: loss 0.361895\n",
      "batch 1838: loss 0.288514\n",
      "batch 1839: loss 0.321084\n",
      "batch 1840: loss 0.363900\n",
      "batch 1841: loss 0.384433\n",
      "batch 1842: loss 0.316559\n",
      "batch 1843: loss 0.377824\n",
      "batch 1844: loss 0.291745\n",
      "batch 1845: loss 0.330846\n",
      "batch 1846: loss 0.399245\n",
      "batch 1847: loss 0.303064\n",
      "batch 1848: loss 0.378428\n",
      "batch 1849: loss 0.294679\n",
      "batch 1850: loss 0.315633\n",
      "batch 1851: loss 0.437816\n",
      "batch 1852: loss 0.543414\n",
      "batch 1853: loss 0.536657\n",
      "batch 1854: loss 0.317775\n",
      "batch 1855: loss 0.294563\n",
      "batch 1856: loss 0.349383\n",
      "batch 1857: loss 0.452107\n",
      "batch 1858: loss 0.278545\n",
      "batch 1859: loss 0.261550\n",
      "batch 1860: loss 0.308907\n",
      "batch 1861: loss 0.364019\n",
      "batch 1862: loss 0.301073\n",
      "batch 1863: loss 0.403990\n",
      "batch 1864: loss 0.422796\n",
      "batch 1865: loss 0.265513\n",
      "batch 1866: loss 0.384110\n",
      "batch 1867: loss 0.359360\n",
      "batch 1868: loss 0.260134\n",
      "batch 1869: loss 0.323573\n",
      "batch 1870: loss 0.415224\n",
      "batch 1871: loss 0.316246\n",
      "batch 1872: loss 0.352212\n",
      "batch 1873: loss 0.357110\n",
      "batch 1874: loss 0.331434\n",
      "batch 1875: loss 0.292727\n",
      "batch 1876: loss 0.288696\n",
      "batch 1877: loss 0.355441\n",
      "batch 1878: loss 0.395501\n",
      "batch 1879: loss 0.261564\n",
      "batch 1880: loss 0.326506\n",
      "batch 1881: loss 0.338749\n",
      "batch 1882: loss 0.300429\n",
      "batch 1883: loss 0.410918\n",
      "batch 1884: loss 0.323634\n",
      "batch 1885: loss 0.320521\n",
      "batch 1886: loss 0.381633\n",
      "batch 1887: loss 0.424262\n",
      "batch 1888: loss 0.349141\n",
      "batch 1889: loss 0.285360\n",
      "batch 1890: loss 0.442160\n",
      "batch 1891: loss 0.318068\n",
      "batch 1892: loss 0.420108\n",
      "batch 1893: loss 0.249860\n",
      "batch 1894: loss 0.355518\n",
      "batch 1895: loss 0.231348\n",
      "batch 1896: loss 0.342102\n",
      "batch 1897: loss 0.437478\n",
      "batch 1898: loss 0.330662\n",
      "batch 1899: loss 0.351229\n",
      "batch 1900: loss 0.290900\n",
      "batch 1901: loss 0.351929\n",
      "batch 1902: loss 0.436071\n",
      "batch 1903: loss 0.336360\n",
      "batch 1904: loss 0.430473\n",
      "batch 1905: loss 0.391301\n",
      "batch 1906: loss 0.406080\n",
      "batch 1907: loss 0.282042\n",
      "batch 1908: loss 0.335420\n",
      "batch 1909: loss 0.407701\n",
      "batch 1910: loss 0.268204\n",
      "batch 1911: loss 0.399766\n",
      "batch 1912: loss 0.444711\n",
      "batch 1913: loss 0.419830\n",
      "batch 1914: loss 0.275110\n",
      "batch 1915: loss 0.383681\n",
      "batch 1916: loss 0.316869\n",
      "batch 1917: loss 0.423850\n",
      "batch 1918: loss 0.277941\n",
      "batch 1919: loss 0.297174\n",
      "batch 1920: loss 0.290763\n",
      "batch 1921: loss 0.342040\n",
      "batch 1922: loss 0.337309\n",
      "batch 1923: loss 0.291684\n",
      "batch 1924: loss 0.393359\n",
      "batch 1925: loss 0.293774\n",
      "batch 1926: loss 0.333360\n",
      "batch 1927: loss 0.380268\n",
      "batch 1928: loss 0.433908\n",
      "batch 1929: loss 0.371098\n",
      "batch 1930: loss 0.326568\n",
      "batch 1931: loss 0.355391\n",
      "batch 1932: loss 0.478478\n",
      "batch 1933: loss 0.363547\n",
      "batch 1934: loss 0.364809\n",
      "batch 1935: loss 0.227256\n",
      "batch 1936: loss 0.291442\n",
      "batch 1937: loss 0.284674\n",
      "batch 1938: loss 0.273887\n",
      "batch 1939: loss 0.363678\n",
      "batch 1940: loss 0.401308\n",
      "batch 1941: loss 0.289139\n",
      "batch 1942: loss 0.273536\n",
      "batch 1943: loss 0.318865\n",
      "batch 1944: loss 0.420781\n",
      "batch 1945: loss 0.298073\n",
      "batch 1946: loss 0.324966\n",
      "batch 1947: loss 0.428292\n",
      "batch 1948: loss 0.261159\n",
      "batch 1949: loss 0.351151\n",
      "batch 1950: loss 0.383951\n",
      "batch 1951: loss 0.386631\n",
      "batch 1952: loss 0.342622\n",
      "batch 1953: loss 0.442771\n",
      "batch 1954: loss 0.413949\n",
      "batch 1955: loss 0.389734\n",
      "batch 1956: loss 0.311455\n",
      "batch 1957: loss 0.282409\n",
      "batch 1958: loss 0.347832\n",
      "batch 1959: loss 0.472558\n",
      "batch 1960: loss 0.419253\n",
      "batch 1961: loss 0.341609\n",
      "batch 1962: loss 0.269349\n",
      "batch 1963: loss 0.259895\n",
      "batch 1964: loss 0.356418\n",
      "batch 1965: loss 0.354843\n",
      "batch 1966: loss 0.308422\n",
      "batch 1967: loss 0.324804\n",
      "batch 1968: loss 0.407158\n",
      "batch 1969: loss 0.281456\n",
      "batch 1970: loss 0.403421\n",
      "batch 1971: loss 0.379775\n",
      "batch 1972: loss 0.415951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1973: loss 0.417585\n",
      "batch 1974: loss 0.268731\n",
      "batch 1975: loss 0.286221\n",
      "batch 1976: loss 0.261983\n",
      "batch 1977: loss 0.316985\n",
      "batch 1978: loss 0.351352\n",
      "batch 1979: loss 0.270975\n",
      "batch 1980: loss 0.394512\n",
      "batch 1981: loss 0.293535\n",
      "batch 1982: loss 0.300015\n",
      "batch 1983: loss 0.364382\n",
      "batch 1984: loss 0.212460\n",
      "batch 1985: loss 0.360016\n",
      "batch 1986: loss 0.394178\n",
      "batch 1987: loss 0.398398\n",
      "batch 1988: loss 0.329705\n",
      "batch 1989: loss 0.430101\n",
      "batch 1990: loss 0.421907\n",
      "batch 1991: loss 0.284642\n",
      "batch 1992: loss 0.312555\n",
      "batch 1993: loss 0.291629\n",
      "batch 1994: loss 0.286160\n",
      "batch 1995: loss 0.458452\n",
      "batch 1996: loss 0.242525\n",
      "batch 1997: loss 0.306107\n",
      "batch 1998: loss 0.255138\n",
      "batch 1999: loss 0.346455\n",
      "batch 2000: loss 0.387761\n",
      "batch 2001: loss 0.423019\n",
      "batch 2002: loss 0.434564\n",
      "batch 2003: loss 0.263678\n",
      "batch 2004: loss 0.337834\n",
      "batch 2005: loss 0.323962\n",
      "batch 2006: loss 0.380057\n",
      "batch 2007: loss 0.430901\n",
      "batch 2008: loss 0.276945\n",
      "batch 2009: loss 0.236994\n",
      "batch 2010: loss 0.390453\n",
      "batch 2011: loss 0.371805\n",
      "batch 2012: loss 0.287492\n",
      "batch 2013: loss 0.274639\n",
      "batch 2014: loss 0.271155\n",
      "batch 2015: loss 0.388335\n",
      "batch 2016: loss 0.385413\n",
      "batch 2017: loss 0.359133\n",
      "batch 2018: loss 0.498600\n",
      "batch 2019: loss 0.299810\n",
      "batch 2020: loss 0.420442\n",
      "batch 2021: loss 0.301916\n",
      "batch 2022: loss 0.309297\n",
      "batch 2023: loss 0.436412\n",
      "batch 2024: loss 0.244043\n",
      "batch 2025: loss 0.305955\n",
      "batch 2026: loss 0.415534\n",
      "batch 2027: loss 0.327998\n",
      "batch 2028: loss 0.363796\n",
      "batch 2029: loss 0.417711\n",
      "batch 2030: loss 0.363913\n",
      "batch 2031: loss 0.375956\n",
      "batch 2032: loss 0.271844\n",
      "batch 2033: loss 0.331618\n",
      "batch 2034: loss 0.471144\n",
      "batch 2035: loss 0.391103\n",
      "batch 2036: loss 0.397880\n",
      "batch 2037: loss 0.322773\n",
      "batch 2038: loss 0.415169\n",
      "batch 2039: loss 0.311734\n",
      "batch 2040: loss 0.435794\n",
      "batch 2041: loss 0.359683\n",
      "batch 2042: loss 0.393885\n",
      "batch 2043: loss 0.317555\n",
      "batch 2044: loss 0.340604\n",
      "batch 2045: loss 0.322256\n",
      "batch 2046: loss 0.396436\n",
      "batch 2047: loss 0.367530\n",
      "batch 2048: loss 0.375621\n",
      "batch 2049: loss 0.478310\n",
      "batch 2050: loss 0.362521\n",
      "batch 2051: loss 0.378408\n",
      "batch 2052: loss 0.355490\n",
      "batch 2053: loss 0.302050\n",
      "batch 2054: loss 0.494006\n",
      "batch 2055: loss 0.313992\n",
      "batch 2056: loss 0.366233\n",
      "batch 2057: loss 0.452238\n",
      "batch 2058: loss 0.309095\n",
      "batch 2059: loss 0.309551\n",
      "batch 2060: loss 0.313864\n",
      "batch 2061: loss 0.381107\n",
      "batch 2062: loss 0.324141\n",
      "batch 2063: loss 0.359668\n",
      "batch 2064: loss 0.415950\n",
      "batch 2065: loss 0.304169\n",
      "batch 2066: loss 0.342711\n",
      "batch 2067: loss 0.300067\n",
      "batch 2068: loss 0.244126\n",
      "batch 2069: loss 0.299091\n",
      "batch 2070: loss 0.314516\n",
      "batch 2071: loss 0.328711\n",
      "batch 2072: loss 0.385741\n",
      "batch 2073: loss 0.395852\n",
      "batch 2074: loss 0.261096\n",
      "batch 2075: loss 0.345962\n",
      "batch 2076: loss 0.318787\n",
      "batch 2077: loss 0.316988\n",
      "batch 2078: loss 0.385703\n",
      "batch 2079: loss 0.346670\n",
      "batch 2080: loss 0.330185\n",
      "batch 2081: loss 0.327361\n",
      "batch 2082: loss 0.239226\n",
      "batch 2083: loss 0.443417\n",
      "batch 2084: loss 0.316279\n",
      "batch 2085: loss 0.372066\n",
      "batch 2086: loss 0.382554\n",
      "batch 2087: loss 0.273907\n",
      "batch 2088: loss 0.362644\n",
      "batch 2089: loss 0.329302\n",
      "batch 2090: loss 0.425690\n",
      "batch 2091: loss 0.494650\n",
      "batch 2092: loss 0.299414\n",
      "batch 2093: loss 0.318160\n",
      "batch 2094: loss 0.347892\n",
      "batch 2095: loss 0.275925\n",
      "batch 2096: loss 0.348982\n",
      "batch 2097: loss 0.298816\n",
      "batch 2098: loss 0.325578\n",
      "batch 2099: loss 0.403178\n",
      "batch 2100: loss 0.388846\n",
      "batch 2101: loss 0.366038\n",
      "batch 2102: loss 0.276179\n",
      "batch 2103: loss 0.263521\n",
      "batch 2104: loss 0.314052\n",
      "batch 2105: loss 0.380743\n",
      "batch 2106: loss 0.392151\n",
      "batch 2107: loss 0.334033\n",
      "batch 2108: loss 0.319770\n",
      "batch 2109: loss 0.390231\n",
      "batch 2110: loss 0.328315\n",
      "batch 2111: loss 0.313534\n",
      "batch 2112: loss 0.318139\n",
      "batch 2113: loss 0.338323\n",
      "batch 2114: loss 0.356352\n",
      "batch 2115: loss 0.333968\n",
      "batch 2116: loss 0.328112\n",
      "batch 2117: loss 0.424660\n",
      "batch 2118: loss 0.282326\n",
      "batch 2119: loss 0.212590\n",
      "batch 2120: loss 0.324157\n",
      "batch 2121: loss 0.388674\n",
      "batch 2122: loss 0.313442\n",
      "batch 2123: loss 0.282573\n",
      "batch 2124: loss 0.436093\n",
      "batch 2125: loss 0.360596\n",
      "batch 2126: loss 0.397196\n",
      "batch 2127: loss 0.346394\n",
      "batch 2128: loss 0.302000\n",
      "batch 2129: loss 0.333809\n",
      "batch 2130: loss 0.390947\n",
      "batch 2131: loss 0.492129\n",
      "batch 2132: loss 0.318127\n",
      "batch 2133: loss 0.296300\n",
      "batch 2134: loss 0.275187\n",
      "batch 2135: loss 0.255781\n",
      "batch 2136: loss 0.330494\n",
      "batch 2137: loss 0.354661\n",
      "batch 2138: loss 0.482449\n",
      "batch 2139: loss 0.261830\n",
      "batch 2140: loss 0.304773\n",
      "batch 2141: loss 0.268749\n",
      "batch 2142: loss 0.274206\n",
      "batch 2143: loss 0.295313\n",
      "batch 2144: loss 0.328253\n",
      "batch 2145: loss 0.287028\n",
      "batch 2146: loss 0.394243\n",
      "batch 2147: loss 0.277451\n",
      "batch 2148: loss 0.320544\n",
      "batch 2149: loss 0.274408\n",
      "batch 2150: loss 0.221294\n",
      "batch 2151: loss 0.270727\n",
      "batch 2152: loss 0.438437\n",
      "batch 2153: loss 0.319321\n",
      "batch 2154: loss 0.310614\n",
      "batch 2155: loss 0.286272\n",
      "batch 2156: loss 0.355108\n",
      "batch 2157: loss 0.269815\n",
      "batch 2158: loss 0.427271\n",
      "batch 2159: loss 0.347902\n",
      "batch 2160: loss 0.352085\n",
      "batch 2161: loss 0.254531\n",
      "batch 2162: loss 0.285943\n",
      "batch 2163: loss 0.314408\n",
      "batch 2164: loss 0.326978\n",
      "batch 2165: loss 0.267573\n",
      "batch 2166: loss 0.325250\n",
      "batch 2167: loss 0.432767\n",
      "batch 2168: loss 0.298342\n",
      "batch 2169: loss 0.330445\n",
      "batch 2170: loss 0.389935\n",
      "batch 2171: loss 0.307396\n",
      "batch 2172: loss 0.445863\n",
      "batch 2173: loss 0.358873\n",
      "batch 2174: loss 0.286058\n",
      "batch 2175: loss 0.413240\n",
      "batch 2176: loss 0.278256\n",
      "batch 2177: loss 0.350279\n",
      "batch 2178: loss 0.288241\n",
      "batch 2179: loss 0.240745\n",
      "batch 2180: loss 0.302337\n",
      "batch 2181: loss 0.266296\n",
      "batch 2182: loss 0.275833\n",
      "batch 2183: loss 0.302954\n",
      "batch 2184: loss 0.343078\n",
      "batch 2185: loss 0.320306\n",
      "batch 2186: loss 0.444188\n",
      "batch 2187: loss 0.417032\n",
      "batch 2188: loss 0.394412\n",
      "batch 2189: loss 0.286711\n",
      "batch 2190: loss 0.335689\n",
      "batch 2191: loss 0.291657\n",
      "batch 2192: loss 0.322320\n",
      "batch 2193: loss 0.365639\n",
      "batch 2194: loss 0.342481\n",
      "batch 2195: loss 0.369144\n",
      "batch 2196: loss 0.438504\n",
      "batch 2197: loss 0.304988\n",
      "batch 2198: loss 0.371272\n",
      "batch 2199: loss 0.330421\n",
      "batch 2200: loss 0.316277\n",
      "batch 2201: loss 0.220971\n",
      "batch 2202: loss 0.400993\n",
      "batch 2203: loss 0.228149\n",
      "batch 2204: loss 0.402724\n",
      "batch 2205: loss 0.381968\n",
      "batch 2206: loss 0.208445\n",
      "batch 2207: loss 0.381728\n",
      "batch 2208: loss 0.313171\n",
      "batch 2209: loss 0.392020\n",
      "batch 2210: loss 0.344320\n",
      "batch 2211: loss 0.301422\n",
      "batch 2212: loss 0.302966\n",
      "batch 2213: loss 0.243807\n",
      "batch 2214: loss 0.329038\n",
      "batch 2215: loss 0.412607\n",
      "batch 2216: loss 0.333292\n",
      "batch 2217: loss 0.286357\n",
      "batch 2218: loss 0.431122\n",
      "batch 2219: loss 0.308724\n",
      "batch 2220: loss 0.239528\n",
      "batch 2221: loss 0.371358\n",
      "batch 2222: loss 0.341844\n",
      "batch 2223: loss 0.381365\n",
      "batch 2224: loss 0.356620\n",
      "batch 2225: loss 0.362233\n",
      "batch 2226: loss 0.343097\n",
      "batch 2227: loss 0.398817\n",
      "batch 2228: loss 0.302324\n",
      "batch 2229: loss 0.310125\n",
      "batch 2230: loss 0.274648\n",
      "batch 2231: loss 0.313237\n",
      "batch 2232: loss 0.325005\n",
      "batch 2233: loss 0.429490\n",
      "batch 2234: loss 0.310804\n",
      "batch 2235: loss 0.474537\n",
      "batch 2236: loss 0.371011\n",
      "batch 2237: loss 0.247424\n",
      "batch 2238: loss 0.330884\n",
      "batch 2239: loss 0.268740\n",
      "batch 2240: loss 0.423599\n",
      "batch 2241: loss 0.429185\n",
      "batch 2242: loss 0.263884\n",
      "batch 2243: loss 0.321022\n",
      "batch 2244: loss 0.358548\n",
      "batch 2245: loss 0.339324\n",
      "batch 2246: loss 0.338647\n",
      "batch 2247: loss 0.444958\n",
      "batch 2248: loss 0.331949\n",
      "batch 2249: loss 0.316303\n",
      "batch 2250: loss 0.387695\n",
      "batch 2251: loss 0.335030\n",
      "batch 2252: loss 0.333169\n",
      "batch 2253: loss 0.382246\n",
      "batch 2254: loss 0.372722\n",
      "batch 2255: loss 0.263201\n",
      "batch 2256: loss 0.337482\n",
      "batch 2257: loss 0.251467\n",
      "batch 2258: loss 0.306469\n",
      "batch 2259: loss 0.373588\n",
      "batch 2260: loss 0.511400\n",
      "batch 2261: loss 0.237228\n",
      "batch 2262: loss 0.300366\n",
      "batch 2263: loss 0.434771\n",
      "batch 2264: loss 0.394031\n",
      "batch 2265: loss 0.312944\n",
      "batch 2266: loss 0.321407\n",
      "batch 2267: loss 0.349734\n",
      "batch 2268: loss 0.308304\n",
      "batch 2269: loss 0.349288\n",
      "batch 2270: loss 0.291204\n",
      "batch 2271: loss 0.296215\n",
      "batch 2272: loss 0.378359\n",
      "batch 2273: loss 0.385672\n",
      "batch 2274: loss 0.368189\n",
      "batch 2275: loss 0.334290\n",
      "batch 2276: loss 0.406615\n",
      "batch 2277: loss 0.272228\n",
      "batch 2278: loss 0.295132\n",
      "batch 2279: loss 0.284670\n",
      "batch 2280: loss 0.304737\n",
      "batch 2281: loss 0.367586\n",
      "batch 2282: loss 0.305062\n",
      "batch 2283: loss 0.257202\n",
      "batch 2284: loss 0.297186\n",
      "batch 2285: loss 0.327304\n",
      "batch 2286: loss 0.426090\n",
      "batch 2287: loss 0.283207\n",
      "batch 2288: loss 0.342453\n",
      "batch 2289: loss 0.284829\n",
      "batch 2290: loss 0.291759\n",
      "batch 2291: loss 0.343520\n",
      "batch 2292: loss 0.417352\n",
      "batch 2293: loss 0.312933\n",
      "batch 2294: loss 0.315232\n",
      "batch 2295: loss 0.334124\n",
      "batch 2296: loss 0.319353\n",
      "batch 2297: loss 0.349310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2298: loss 0.513945\n",
      "batch 2299: loss 0.254181\n",
      "batch 2300: loss 0.346521\n",
      "batch 2301: loss 0.332937\n",
      "batch 2302: loss 0.352548\n",
      "batch 2303: loss 0.333800\n",
      "batch 2304: loss 0.331259\n",
      "batch 2305: loss 0.327760\n",
      "batch 2306: loss 0.289530\n",
      "batch 2307: loss 0.282710\n",
      "batch 2308: loss 0.295286\n",
      "batch 2309: loss 0.276866\n",
      "batch 2310: loss 0.256885\n",
      "batch 2311: loss 0.381560\n",
      "batch 2312: loss 0.326928\n",
      "batch 2313: loss 0.247597\n",
      "batch 2314: loss 0.289899\n",
      "batch 2315: loss 0.330708\n",
      "batch 2316: loss 0.339323\n",
      "batch 2317: loss 0.263174\n",
      "batch 2318: loss 0.350290\n",
      "batch 2319: loss 0.293749\n",
      "batch 2320: loss 0.351886\n",
      "batch 2321: loss 0.427937\n",
      "batch 2322: loss 0.341464\n",
      "batch 2323: loss 0.402960\n",
      "batch 2324: loss 0.321910\n",
      "batch 2325: loss 0.401766\n",
      "batch 2326: loss 0.293696\n",
      "batch 2327: loss 0.329898\n",
      "batch 2328: loss 0.271025\n",
      "batch 2329: loss 0.323404\n",
      "batch 2330: loss 0.326257\n",
      "batch 2331: loss 0.431312\n",
      "batch 2332: loss 0.396846\n",
      "batch 2333: loss 0.358850\n",
      "batch 2334: loss 0.228036\n",
      "batch 2335: loss 0.445995\n",
      "batch 2336: loss 0.380650\n",
      "batch 2337: loss 0.289707\n",
      "batch 2338: loss 0.357319\n",
      "batch 2339: loss 0.309355\n",
      "test accuracy: 0.970553\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.L1(0.01))\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(rate=0.2)\n",
    "\n",
    "    def call(self, inputs, training=None):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dropout_layer(x, training=training)\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        x = self.dropout_layer(x, training=training)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "# model.compile\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X, training=True)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    \n",
    "    \n",
    "\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上Dropout(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)   (60000,)\n",
      "(10000, 784)   (10000,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "165/165 - 0s - loss: 14.1817 - accuracy: 0.3163 - val_loss: 9.7888 - val_accuracy: 0.4633\n",
      "Epoch 2/100\n",
      "165/165 - 0s - loss: 8.7625 - accuracy: 0.5245 - val_loss: 7.5016 - val_accuracy: 0.6594\n",
      "Epoch 3/100\n",
      "165/165 - 0s - loss: 6.6568 - accuracy: 0.6931 - val_loss: 5.6912 - val_accuracy: 0.8068\n",
      "Epoch 4/100\n",
      "165/165 - 0s - loss: 5.0692 - accuracy: 0.8050 - val_loss: 4.3518 - val_accuracy: 0.8623\n",
      "Epoch 5/100\n",
      "165/165 - 0s - loss: 3.8899 - accuracy: 0.8499 - val_loss: 3.3255 - val_accuracy: 0.8871\n",
      "Epoch 6/100\n",
      "165/165 - 0s - loss: 3.0141 - accuracy: 0.8778 - val_loss: 2.6012 - val_accuracy: 0.8973\n",
      "Epoch 7/100\n",
      "165/165 - 0s - loss: 2.3885 - accuracy: 0.8901 - val_loss: 2.0736 - val_accuracy: 0.9047\n",
      "Epoch 8/100\n",
      "165/165 - 0s - loss: 1.9300 - accuracy: 0.8965 - val_loss: 1.6778 - val_accuracy: 0.9108\n",
      "Epoch 9/100\n",
      "165/165 - 0s - loss: 1.5942 - accuracy: 0.8996 - val_loss: 1.3969 - val_accuracy: 0.9160\n",
      "Epoch 10/100\n",
      "165/165 - 0s - loss: 1.3507 - accuracy: 0.9006 - val_loss: 1.1873 - val_accuracy: 0.9177\n",
      "Epoch 11/100\n",
      "165/165 - 0s - loss: 1.1610 - accuracy: 0.9067 - val_loss: 1.0232 - val_accuracy: 0.9194\n",
      "Epoch 12/100\n",
      "165/165 - 0s - loss: 1.0261 - accuracy: 0.9053 - val_loss: 0.9001 - val_accuracy: 0.9224\n",
      "Epoch 13/100\n",
      "165/165 - 1s - loss: 0.9265 - accuracy: 0.9076 - val_loss: 0.8203 - val_accuracy: 0.9229\n",
      "Epoch 14/100\n",
      "165/165 - 0s - loss: 0.8451 - accuracy: 0.9106 - val_loss: 0.7562 - val_accuracy: 0.9226\n",
      "Epoch 15/100\n",
      "165/165 - 0s - loss: 0.7921 - accuracy: 0.9109 - val_loss: 0.7041 - val_accuracy: 0.9251\n",
      "Epoch 16/100\n",
      "165/165 - 0s - loss: 0.7452 - accuracy: 0.9116 - val_loss: 0.6655 - val_accuracy: 0.9271\n",
      "Epoch 17/100\n",
      "165/165 - 0s - loss: 0.7087 - accuracy: 0.9122 - val_loss: 0.6463 - val_accuracy: 0.9230\n",
      "Epoch 18/100\n",
      "165/165 - 0s - loss: 0.6897 - accuracy: 0.9131 - val_loss: 0.6134 - val_accuracy: 0.9278\n",
      "Epoch 19/100\n",
      "165/165 - 0s - loss: 0.6669 - accuracy: 0.9141 - val_loss: 0.6054 - val_accuracy: 0.9262\n",
      "Epoch 20/100\n",
      "165/165 - 0s - loss: 0.6586 - accuracy: 0.9139 - val_loss: 0.5865 - val_accuracy: 0.9278\n",
      "Epoch 21/100\n",
      "165/165 - 0s - loss: 0.6358 - accuracy: 0.9161 - val_loss: 0.5725 - val_accuracy: 0.9277\n",
      "Epoch 22/100\n",
      "165/165 - 0s - loss: 0.6308 - accuracy: 0.9152 - val_loss: 0.5648 - val_accuracy: 0.9284\n",
      "Epoch 23/100\n",
      "165/165 - 0s - loss: 0.6157 - accuracy: 0.9175 - val_loss: 0.5606 - val_accuracy: 0.9270\n",
      "Epoch 24/100\n",
      "165/165 - 0s - loss: 0.6105 - accuracy: 0.9155 - val_loss: 0.5491 - val_accuracy: 0.9287\n",
      "Epoch 25/100\n",
      "165/165 - 0s - loss: 0.5979 - accuracy: 0.9168 - val_loss: 0.5461 - val_accuracy: 0.9298\n",
      "Epoch 26/100\n",
      "165/165 - 0s - loss: 0.5948 - accuracy: 0.9171 - val_loss: 0.5411 - val_accuracy: 0.9294\n",
      "Epoch 27/100\n",
      "165/165 - 0s - loss: 0.5897 - accuracy: 0.9173 - val_loss: 0.5352 - val_accuracy: 0.9313\n",
      "Epoch 28/100\n",
      "165/165 - 0s - loss: 0.5829 - accuracy: 0.9187 - val_loss: 0.5293 - val_accuracy: 0.9303\n",
      "Epoch 29/100\n",
      "165/165 - 0s - loss: 0.5759 - accuracy: 0.9200 - val_loss: 0.5245 - val_accuracy: 0.9292\n",
      "Epoch 30/100\n",
      "165/165 - 0s - loss: 0.5671 - accuracy: 0.9209 - val_loss: 0.5227 - val_accuracy: 0.9298\n",
      "Epoch 31/100\n",
      "165/165 - 0s - loss: 0.5645 - accuracy: 0.9213 - val_loss: 0.5199 - val_accuracy: 0.9303\n",
      "Epoch 32/100\n",
      "165/165 - 0s - loss: 0.5645 - accuracy: 0.9203 - val_loss: 0.5171 - val_accuracy: 0.9290\n",
      "Epoch 33/100\n",
      "165/165 - 0s - loss: 0.5604 - accuracy: 0.9198 - val_loss: 0.5185 - val_accuracy: 0.9292\n",
      "Epoch 34/100\n",
      "165/165 - 0s - loss: 0.5582 - accuracy: 0.9210 - val_loss: 0.5112 - val_accuracy: 0.9312\n",
      "Epoch 35/100\n",
      "165/165 - 0s - loss: 0.5570 - accuracy: 0.9205 - val_loss: 0.5140 - val_accuracy: 0.9309\n",
      "Epoch 36/100\n",
      "165/165 - 0s - loss: 0.5487 - accuracy: 0.9221 - val_loss: 0.5092 - val_accuracy: 0.9297\n",
      "Epoch 37/100\n",
      "165/165 - 0s - loss: 0.5438 - accuracy: 0.9223 - val_loss: 0.5055 - val_accuracy: 0.9309\n",
      "Epoch 38/100\n",
      "165/165 - 0s - loss: 0.5444 - accuracy: 0.9219 - val_loss: 0.5001 - val_accuracy: 0.9326\n",
      "Epoch 39/100\n",
      "165/165 - 0s - loss: 0.5390 - accuracy: 0.9240 - val_loss: 0.4971 - val_accuracy: 0.9332\n",
      "Epoch 40/100\n",
      "165/165 - 0s - loss: 0.5379 - accuracy: 0.9242 - val_loss: 0.4976 - val_accuracy: 0.9321\n",
      "Epoch 41/100\n",
      "165/165 - 0s - loss: 0.5374 - accuracy: 0.9237 - val_loss: 0.4995 - val_accuracy: 0.9306\n",
      "Epoch 42/100\n",
      "165/165 - 0s - loss: 0.5348 - accuracy: 0.9245 - val_loss: 0.4930 - val_accuracy: 0.9322\n",
      "Epoch 43/100\n",
      "165/165 - 0s - loss: 0.5343 - accuracy: 0.9242 - val_loss: 0.4905 - val_accuracy: 0.9332\n",
      "Epoch 44/100\n",
      "165/165 - 1s - loss: 0.5264 - accuracy: 0.9253 - val_loss: 0.4931 - val_accuracy: 0.9322\n",
      "Epoch 45/100\n",
      "165/165 - 0s - loss: 0.5281 - accuracy: 0.9244 - val_loss: 0.4850 - val_accuracy: 0.9335\n",
      "Epoch 46/100\n",
      "165/165 - 0s - loss: 0.5234 - accuracy: 0.9256 - val_loss: 0.4875 - val_accuracy: 0.9323\n",
      "Epoch 47/100\n",
      "165/165 - 1s - loss: 0.5276 - accuracy: 0.9245 - val_loss: 0.4881 - val_accuracy: 0.9334\n",
      "Epoch 48/100\n",
      "165/165 - 0s - loss: 0.5199 - accuracy: 0.9250 - val_loss: 0.4839 - val_accuracy: 0.9316\n",
      "Epoch 49/100\n",
      "165/165 - 0s - loss: 0.5149 - accuracy: 0.9263 - val_loss: 0.4797 - val_accuracy: 0.9343\n",
      "Epoch 50/100\n",
      "165/165 - 1s - loss: 0.5153 - accuracy: 0.9250 - val_loss: 0.4886 - val_accuracy: 0.9321\n",
      "Epoch 51/100\n",
      "165/165 - 0s - loss: 0.5123 - accuracy: 0.9273 - val_loss: 0.4800 - val_accuracy: 0.9329\n",
      "Epoch 52/100\n",
      "165/165 - 0s - loss: 0.5116 - accuracy: 0.9266 - val_loss: 0.4745 - val_accuracy: 0.9337\n",
      "Epoch 53/100\n",
      "165/165 - 0s - loss: 0.5094 - accuracy: 0.9259 - val_loss: 0.4779 - val_accuracy: 0.9329\n",
      "Epoch 54/100\n",
      "165/165 - 0s - loss: 0.5077 - accuracy: 0.9267 - val_loss: 0.4780 - val_accuracy: 0.9316\n",
      "Epoch 55/100\n",
      "165/165 - 0s - loss: 0.5067 - accuracy: 0.9287 - val_loss: 0.4776 - val_accuracy: 0.9331\n",
      "Epoch 56/100\n",
      "165/165 - 0s - loss: 0.5035 - accuracy: 0.9278 - val_loss: 0.4728 - val_accuracy: 0.9337\n",
      "Epoch 57/100\n",
      "165/165 - 1s - loss: 0.5076 - accuracy: 0.9266 - val_loss: 0.4782 - val_accuracy: 0.9333\n",
      "Epoch 58/100\n",
      "165/165 - 0s - loss: 0.5035 - accuracy: 0.9289 - val_loss: 0.4702 - val_accuracy: 0.9336\n",
      "Epoch 59/100\n",
      "165/165 - 0s - loss: 0.4998 - accuracy: 0.9287 - val_loss: 0.4792 - val_accuracy: 0.9318\n",
      "Epoch 60/100\n",
      "165/165 - 0s - loss: 0.4975 - accuracy: 0.9294 - val_loss: 0.4742 - val_accuracy: 0.9326\n",
      "Epoch 61/100\n",
      "165/165 - 0s - loss: 0.5011 - accuracy: 0.9300 - val_loss: 0.4796 - val_accuracy: 0.9316\n",
      "Epoch 62/100\n",
      "165/165 - 0s - loss: 0.4969 - accuracy: 0.9286 - val_loss: 0.4752 - val_accuracy: 0.9314\n",
      "Epoch 63/100\n",
      "165/165 - 0s - loss: 0.4939 - accuracy: 0.9288 - val_loss: 0.4718 - val_accuracy: 0.9326\n",
      "Epoch 64/100\n",
      "165/165 - 0s - loss: 0.4955 - accuracy: 0.9271 - val_loss: 0.4777 - val_accuracy: 0.9327\n",
      "Epoch 65/100\n",
      "165/165 - 0s - loss: 0.4984 - accuracy: 0.9280 - val_loss: 0.4702 - val_accuracy: 0.9345\n",
      "Epoch 66/100\n",
      "165/165 - 0s - loss: 0.4913 - accuracy: 0.9291 - val_loss: 0.4693 - val_accuracy: 0.9334\n",
      "Epoch 67/100\n",
      "165/165 - 0s - loss: 0.4889 - accuracy: 0.9300 - val_loss: 0.4777 - val_accuracy: 0.9301\n",
      "Epoch 68/100\n",
      "165/165 - 0s - loss: 0.4939 - accuracy: 0.9278 - val_loss: 0.4670 - val_accuracy: 0.9341\n",
      "Epoch 69/100\n",
      "165/165 - 0s - loss: 0.4850 - accuracy: 0.9293 - val_loss: 0.4667 - val_accuracy: 0.9334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "165/165 - 0s - loss: 0.4867 - accuracy: 0.9314 - val_loss: 0.4743 - val_accuracy: 0.9331\n",
      "Epoch 71/100\n",
      "165/165 - 0s - loss: 0.4890 - accuracy: 0.9289 - val_loss: 0.4623 - val_accuracy: 0.9336\n",
      "Epoch 72/100\n",
      "165/165 - 1s - loss: 0.4828 - accuracy: 0.9304 - val_loss: 0.4732 - val_accuracy: 0.9339\n",
      "Epoch 73/100\n",
      "165/165 - 0s - loss: 0.4846 - accuracy: 0.9300 - val_loss: 0.4667 - val_accuracy: 0.9334\n",
      "Epoch 74/100\n",
      "165/165 - 0s - loss: 0.4848 - accuracy: 0.9302 - val_loss: 0.4707 - val_accuracy: 0.9324\n",
      "Epoch 75/100\n",
      "165/165 - 0s - loss: 0.4783 - accuracy: 0.9306 - val_loss: 0.4600 - val_accuracy: 0.9340\n",
      "Epoch 76/100\n",
      "165/165 - 0s - loss: 0.4789 - accuracy: 0.9309 - val_loss: 0.4571 - val_accuracy: 0.9346\n",
      "Epoch 77/100\n",
      "165/165 - 0s - loss: 0.4783 - accuracy: 0.9298 - val_loss: 0.4627 - val_accuracy: 0.9338\n",
      "Epoch 78/100\n",
      "165/165 - 0s - loss: 0.4753 - accuracy: 0.9305 - val_loss: 0.4604 - val_accuracy: 0.9335\n",
      "Epoch 79/100\n",
      "165/165 - 0s - loss: 0.4816 - accuracy: 0.9292 - val_loss: 0.4573 - val_accuracy: 0.9334\n",
      "Epoch 80/100\n",
      "165/165 - 0s - loss: 0.4746 - accuracy: 0.9320 - val_loss: 0.4720 - val_accuracy: 0.9319\n",
      "Epoch 81/100\n",
      "165/165 - 0s - loss: 0.4755 - accuracy: 0.9318 - val_loss: 0.4676 - val_accuracy: 0.9329\n",
      "Epoch 82/100\n",
      "165/165 - 0s - loss: 0.4739 - accuracy: 0.9315 - val_loss: 0.4653 - val_accuracy: 0.9318\n",
      "Epoch 83/100\n",
      "165/165 - 0s - loss: 0.4750 - accuracy: 0.9308 - val_loss: 0.4587 - val_accuracy: 0.9337\n",
      "Epoch 84/100\n",
      "165/165 - 0s - loss: 0.4709 - accuracy: 0.9312 - val_loss: 0.4622 - val_accuracy: 0.9342\n",
      "Epoch 85/100\n",
      "165/165 - 0s - loss: 0.4746 - accuracy: 0.9322 - val_loss: 0.4605 - val_accuracy: 0.9322\n",
      "Epoch 86/100\n",
      "165/165 - 0s - loss: 0.4698 - accuracy: 0.9324 - val_loss: 0.4633 - val_accuracy: 0.9346\n",
      "Epoch 87/100\n",
      "165/165 - 0s - loss: 0.4698 - accuracy: 0.9318 - val_loss: 0.4550 - val_accuracy: 0.9338\n",
      "Epoch 88/100\n",
      "165/165 - 0s - loss: 0.4642 - accuracy: 0.9352 - val_loss: 0.4486 - val_accuracy: 0.9373\n",
      "Epoch 89/100\n",
      "165/165 - 0s - loss: 0.4582 - accuracy: 0.9360 - val_loss: 0.4470 - val_accuracy: 0.9382\n",
      "Epoch 90/100\n",
      "165/165 - 0s - loss: 0.4615 - accuracy: 0.9343 - val_loss: 0.4427 - val_accuracy: 0.9377\n",
      "Epoch 91/100\n",
      "165/165 - 0s - loss: 0.4569 - accuracy: 0.9354 - val_loss: 0.4481 - val_accuracy: 0.9366\n",
      "Epoch 92/100\n",
      "165/165 - 0s - loss: 0.4549 - accuracy: 0.9365 - val_loss: 0.4426 - val_accuracy: 0.9384\n",
      "Epoch 93/100\n",
      "165/165 - 0s - loss: 0.4567 - accuracy: 0.9356 - val_loss: 0.4424 - val_accuracy: 0.9369\n",
      "Epoch 94/100\n",
      "165/165 - 0s - loss: 0.4518 - accuracy: 0.9362 - val_loss: 0.4480 - val_accuracy: 0.9379\n",
      "Epoch 95/100\n",
      "165/165 - 0s - loss: 0.4573 - accuracy: 0.9364 - val_loss: 0.4424 - val_accuracy: 0.9392\n",
      "Epoch 96/100\n",
      "165/165 - 0s - loss: 0.4529 - accuracy: 0.9360 - val_loss: 0.4448 - val_accuracy: 0.9369\n",
      "Epoch 97/100\n",
      "165/165 - 0s - loss: 0.4499 - accuracy: 0.9362 - val_loss: 0.4388 - val_accuracy: 0.9394\n",
      "Epoch 98/100\n",
      "165/165 - 0s - loss: 0.4468 - accuracy: 0.9372 - val_loss: 0.4426 - val_accuracy: 0.9377\n",
      "Epoch 99/100\n",
      "165/165 - 0s - loss: 0.4513 - accuracy: 0.9356 - val_loss: 0.4489 - val_accuracy: 0.9382\n",
      "Epoch 100/100\n",
      "165/165 - 0s - loss: 0.4455 - accuracy: 0.9381 - val_loss: 0.4421 - val_accuracy: 0.9362\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])\n",
    "print(x_train.shape, ' ', y_train.shape)\n",
    "print(x_test.shape, ' ', y_test.shape)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(0.01))\n",
    "])\n",
    "\n",
    "\n",
    "#keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#keras.optimizers.Adam(learning_rate=0.01)\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# provide labels as one_hot representation => tf.keras.losses.CategoricalCrossentropy\n",
    "# provide labels as integers => tf.keras.losses.SparseCategoricalCrossentropy \n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=256, epochs=100, validation_split=0.3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 601us/step - loss: 0.4262 - accuracy: 0.9404\n",
      "[0.42615756392478943, 0.9404000043869019]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.300376\n",
      "batch 1: loss 2.244644\n",
      "batch 2: loss 2.135844\n",
      "batch 3: loss 2.042941\n",
      "batch 4: loss 2.070084\n",
      "batch 5: loss 2.051701\n",
      "batch 6: loss 2.004330\n",
      "batch 7: loss 1.965870\n",
      "batch 8: loss 1.891905\n",
      "batch 9: loss 1.791054\n",
      "batch 10: loss 1.722962\n",
      "batch 11: loss 1.514321\n",
      "batch 12: loss 1.610477\n",
      "batch 13: loss 1.546564\n",
      "batch 14: loss 1.587454\n",
      "batch 15: loss 1.504496\n",
      "batch 16: loss 1.384125\n",
      "batch 17: loss 1.398705\n",
      "batch 18: loss 1.299029\n",
      "batch 19: loss 1.315645\n",
      "batch 20: loss 1.495912\n",
      "batch 21: loss 1.241968\n",
      "batch 22: loss 1.174006\n",
      "batch 23: loss 1.158593\n",
      "batch 24: loss 1.042805\n",
      "batch 25: loss 1.064829\n",
      "batch 26: loss 1.016893\n",
      "batch 27: loss 0.874562\n",
      "batch 28: loss 0.989407\n",
      "batch 29: loss 1.019033\n",
      "batch 30: loss 0.875167\n",
      "batch 31: loss 0.837403\n",
      "batch 32: loss 1.035319\n",
      "batch 33: loss 1.026842\n",
      "batch 34: loss 0.649530\n",
      "batch 35: loss 0.673613\n",
      "batch 36: loss 0.680370\n",
      "batch 37: loss 0.803197\n",
      "batch 38: loss 0.946998\n",
      "batch 39: loss 0.979343\n",
      "batch 40: loss 0.734506\n",
      "batch 41: loss 0.881328\n",
      "batch 42: loss 0.633124\n",
      "batch 43: loss 0.836028\n",
      "batch 44: loss 0.798009\n",
      "batch 45: loss 0.623540\n",
      "batch 46: loss 0.658764\n",
      "batch 47: loss 0.816312\n",
      "batch 48: loss 0.633812\n",
      "batch 49: loss 0.845768\n",
      "batch 50: loss 0.599385\n",
      "batch 51: loss 0.620441\n",
      "batch 52: loss 0.532514\n",
      "batch 53: loss 0.658060\n",
      "batch 54: loss 0.451538\n",
      "batch 55: loss 0.357729\n",
      "batch 56: loss 0.853994\n",
      "batch 57: loss 0.727254\n",
      "batch 58: loss 0.650542\n",
      "batch 59: loss 0.579765\n",
      "batch 60: loss 0.510359\n",
      "batch 61: loss 0.513255\n",
      "batch 62: loss 0.594412\n",
      "batch 63: loss 0.496824\n",
      "batch 64: loss 0.581440\n",
      "batch 65: loss 0.349118\n",
      "batch 66: loss 0.357934\n",
      "batch 67: loss 0.428860\n",
      "batch 68: loss 0.666767\n",
      "batch 69: loss 0.549163\n",
      "batch 70: loss 0.532123\n",
      "batch 71: loss 0.533455\n",
      "batch 72: loss 0.500934\n",
      "batch 73: loss 0.513790\n",
      "batch 74: loss 0.334307\n",
      "batch 75: loss 0.528767\n",
      "batch 76: loss 0.555777\n",
      "batch 77: loss 0.571745\n",
      "batch 78: loss 0.405875\n",
      "batch 79: loss 0.682832\n",
      "batch 80: loss 0.307899\n",
      "batch 81: loss 0.440720\n",
      "batch 82: loss 0.520857\n",
      "batch 83: loss 0.413819\n",
      "batch 84: loss 0.377518\n",
      "batch 85: loss 0.566141\n",
      "batch 86: loss 0.511770\n",
      "batch 87: loss 0.303482\n",
      "batch 88: loss 0.247296\n",
      "batch 89: loss 0.323318\n",
      "batch 90: loss 0.359204\n",
      "batch 91: loss 0.231274\n",
      "batch 92: loss 0.268471\n",
      "batch 93: loss 0.289148\n",
      "batch 94: loss 0.625930\n",
      "batch 95: loss 0.370874\n",
      "batch 96: loss 0.584813\n",
      "batch 97: loss 0.435081\n",
      "batch 98: loss 0.427383\n",
      "batch 99: loss 0.485411\n",
      "batch 100: loss 0.280791\n",
      "batch 101: loss 0.576813\n",
      "batch 102: loss 0.389455\n",
      "batch 103: loss 0.438223\n",
      "batch 104: loss 0.456712\n",
      "batch 105: loss 0.387806\n",
      "batch 106: loss 0.355991\n",
      "batch 107: loss 0.522323\n",
      "batch 108: loss 0.331092\n",
      "batch 109: loss 0.558379\n",
      "batch 110: loss 0.446074\n",
      "batch 111: loss 0.345343\n",
      "batch 112: loss 0.442681\n",
      "batch 113: loss 0.399493\n",
      "batch 114: loss 0.548647\n",
      "batch 115: loss 0.507441\n",
      "batch 116: loss 0.517741\n",
      "batch 117: loss 0.349252\n",
      "batch 118: loss 0.342508\n",
      "batch 119: loss 0.397745\n",
      "batch 120: loss 0.314847\n",
      "batch 121: loss 0.329699\n",
      "batch 122: loss 0.250902\n",
      "batch 123: loss 0.330267\n",
      "batch 124: loss 0.293329\n",
      "batch 125: loss 0.448722\n",
      "batch 126: loss 0.355374\n",
      "batch 127: loss 0.309762\n",
      "batch 128: loss 0.442712\n",
      "batch 129: loss 0.312086\n",
      "batch 130: loss 0.553186\n",
      "batch 131: loss 0.533197\n",
      "batch 132: loss 0.310390\n",
      "batch 133: loss 0.227541\n",
      "batch 134: loss 0.686106\n",
      "batch 135: loss 0.493706\n",
      "batch 136: loss 0.263022\n",
      "batch 137: loss 0.267241\n",
      "batch 138: loss 0.377996\n",
      "batch 139: loss 0.355442\n",
      "batch 140: loss 0.330686\n",
      "batch 141: loss 0.397051\n",
      "batch 142: loss 0.227393\n",
      "batch 143: loss 0.464017\n",
      "batch 144: loss 0.210182\n",
      "batch 145: loss 0.444199\n",
      "batch 146: loss 0.450681\n",
      "batch 147: loss 0.321190\n",
      "batch 148: loss 0.200192\n",
      "batch 149: loss 0.447763\n",
      "batch 150: loss 0.242129\n",
      "batch 151: loss 0.311147\n",
      "batch 152: loss 0.251578\n",
      "batch 153: loss 0.230320\n",
      "batch 154: loss 0.216117\n",
      "batch 155: loss 0.351622\n",
      "batch 156: loss 0.454804\n",
      "batch 157: loss 0.229125\n",
      "batch 158: loss 0.319352\n",
      "batch 159: loss 0.272126\n",
      "batch 160: loss 0.446495\n",
      "batch 161: loss 0.438498\n",
      "batch 162: loss 0.331004\n",
      "batch 163: loss 0.341608\n",
      "batch 164: loss 0.356552\n",
      "batch 165: loss 0.343572\n",
      "batch 166: loss 0.235819\n",
      "batch 167: loss 0.315759\n",
      "batch 168: loss 0.275412\n",
      "batch 169: loss 0.410294\n",
      "batch 170: loss 0.344626\n",
      "batch 171: loss 0.489704\n",
      "batch 172: loss 0.315704\n",
      "batch 173: loss 0.340052\n",
      "batch 174: loss 0.368732\n",
      "batch 175: loss 0.477595\n",
      "batch 176: loss 0.535699\n",
      "batch 177: loss 0.254519\n",
      "batch 178: loss 0.418287\n",
      "batch 179: loss 0.143957\n",
      "batch 180: loss 0.334887\n",
      "batch 181: loss 0.356411\n",
      "batch 182: loss 0.583803\n",
      "batch 183: loss 0.220651\n",
      "batch 184: loss 0.440553\n",
      "batch 185: loss 0.234325\n",
      "batch 186: loss 0.308871\n",
      "batch 187: loss 0.281838\n",
      "batch 188: loss 0.290440\n",
      "batch 189: loss 0.231147\n",
      "batch 190: loss 0.496796\n",
      "batch 191: loss 0.259457\n",
      "batch 192: loss 0.413008\n",
      "batch 193: loss 0.353624\n",
      "batch 194: loss 0.358384\n",
      "batch 195: loss 0.283834\n",
      "batch 196: loss 0.368861\n",
      "batch 197: loss 0.257574\n",
      "batch 198: loss 0.322621\n",
      "batch 199: loss 0.481968\n",
      "batch 200: loss 0.274156\n",
      "batch 201: loss 0.465137\n",
      "batch 202: loss 0.295498\n",
      "batch 203: loss 0.195024\n",
      "batch 204: loss 0.277723\n",
      "batch 205: loss 0.216105\n",
      "batch 206: loss 0.568228\n",
      "batch 207: loss 0.194603\n",
      "batch 208: loss 0.280739\n",
      "batch 209: loss 0.441035\n",
      "batch 210: loss 0.276210\n",
      "batch 211: loss 0.430319\n",
      "batch 212: loss 0.210979\n",
      "batch 213: loss 0.551367\n",
      "batch 214: loss 0.247092\n",
      "batch 215: loss 0.375554\n",
      "batch 216: loss 0.266764\n",
      "batch 217: loss 0.432198\n",
      "batch 218: loss 0.211494\n",
      "batch 219: loss 0.181341\n",
      "batch 220: loss 0.104857\n",
      "batch 221: loss 0.184938\n",
      "batch 222: loss 0.515802\n",
      "batch 223: loss 0.268555\n",
      "batch 224: loss 0.661495\n",
      "batch 225: loss 0.445499\n",
      "batch 226: loss 0.332012\n",
      "batch 227: loss 0.312627\n",
      "batch 228: loss 0.334657\n",
      "batch 229: loss 0.211657\n",
      "batch 230: loss 0.219468\n",
      "batch 231: loss 0.320290\n",
      "batch 232: loss 0.200099\n",
      "batch 233: loss 0.431385\n",
      "batch 234: loss 0.208822\n",
      "batch 235: loss 0.217632\n",
      "batch 236: loss 0.284656\n",
      "batch 237: loss 0.182051\n",
      "batch 238: loss 0.399485\n",
      "batch 239: loss 0.412671\n",
      "batch 240: loss 0.191033\n",
      "batch 241: loss 0.246878\n",
      "batch 242: loss 0.290075\n",
      "batch 243: loss 0.302925\n",
      "batch 244: loss 0.372609\n",
      "batch 245: loss 0.145616\n",
      "batch 246: loss 0.787121\n",
      "batch 247: loss 0.255848\n",
      "batch 248: loss 0.440970\n",
      "batch 249: loss 0.213399\n",
      "batch 250: loss 0.483114\n",
      "batch 251: loss 0.239062\n",
      "batch 252: loss 0.252649\n",
      "batch 253: loss 0.277534\n",
      "batch 254: loss 0.528808\n",
      "batch 255: loss 0.201724\n",
      "batch 256: loss 0.207287\n",
      "batch 257: loss 0.265453\n",
      "batch 258: loss 0.395210\n",
      "batch 259: loss 0.326786\n",
      "batch 260: loss 0.095061\n",
      "batch 261: loss 0.227455\n",
      "batch 262: loss 0.547860\n",
      "batch 263: loss 0.261966\n",
      "batch 264: loss 0.133446\n",
      "batch 265: loss 0.165198\n",
      "batch 266: loss 0.309048\n",
      "batch 267: loss 0.420752\n",
      "batch 268: loss 0.361336\n",
      "batch 269: loss 0.376083\n",
      "batch 270: loss 0.229149\n",
      "batch 271: loss 0.291516\n",
      "batch 272: loss 0.395772\n",
      "batch 273: loss 0.202084\n",
      "batch 274: loss 0.281974\n",
      "batch 275: loss 0.280473\n",
      "batch 276: loss 0.105852\n",
      "batch 277: loss 0.481238\n",
      "batch 278: loss 0.298399\n",
      "batch 279: loss 0.337419\n",
      "batch 280: loss 0.365496\n",
      "batch 281: loss 0.478559\n",
      "batch 282: loss 0.155570\n",
      "batch 283: loss 0.261638\n",
      "batch 284: loss 0.226356\n",
      "batch 285: loss 0.393364\n",
      "batch 286: loss 0.338100\n",
      "batch 287: loss 0.204758\n",
      "batch 288: loss 0.174229\n",
      "batch 289: loss 0.664641\n",
      "batch 290: loss 0.152454\n",
      "batch 291: loss 0.274213\n",
      "batch 292: loss 0.206207\n",
      "batch 293: loss 0.276935\n",
      "batch 294: loss 0.336948\n",
      "batch 295: loss 0.373544\n",
      "batch 296: loss 0.335957\n",
      "batch 297: loss 0.325460\n",
      "batch 298: loss 0.301567\n",
      "batch 299: loss 0.241934\n",
      "batch 300: loss 0.202401\n",
      "batch 301: loss 0.305968\n",
      "batch 302: loss 0.278852\n",
      "batch 303: loss 0.173668\n",
      "batch 304: loss 0.140128\n",
      "batch 305: loss 0.279267\n",
      "batch 306: loss 0.326515\n",
      "batch 307: loss 0.428129\n",
      "batch 308: loss 0.533126\n",
      "batch 309: loss 0.149820\n",
      "batch 310: loss 0.290081\n",
      "batch 311: loss 0.195321\n",
      "batch 312: loss 0.227623\n",
      "batch 313: loss 0.499753\n",
      "batch 314: loss 0.287840\n",
      "batch 315: loss 0.387855\n",
      "batch 316: loss 0.273542\n",
      "batch 317: loss 0.296385\n",
      "batch 318: loss 0.276848\n",
      "batch 319: loss 0.185053\n",
      "batch 320: loss 0.500844\n",
      "batch 321: loss 0.449305\n",
      "batch 322: loss 0.193374\n",
      "batch 323: loss 0.156215\n",
      "batch 324: loss 0.339099\n",
      "batch 325: loss 0.252577\n",
      "batch 326: loss 0.443502\n",
      "batch 327: loss 0.214692\n",
      "batch 328: loss 0.101527\n",
      "batch 329: loss 0.444266\n",
      "batch 330: loss 0.372815\n",
      "batch 331: loss 0.498609\n",
      "batch 332: loss 0.184283\n",
      "batch 333: loss 0.413069\n",
      "batch 334: loss 0.340611\n",
      "batch 335: loss 0.209988\n",
      "batch 336: loss 0.289798\n",
      "batch 337: loss 0.320465\n",
      "batch 338: loss 0.209182\n",
      "batch 339: loss 0.247295\n",
      "batch 340: loss 0.154230\n",
      "batch 341: loss 0.395146\n",
      "batch 342: loss 0.267047\n",
      "batch 343: loss 0.227365\n",
      "batch 344: loss 0.286529\n",
      "batch 345: loss 0.547016\n",
      "batch 346: loss 0.205307\n",
      "batch 347: loss 0.291384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 348: loss 0.252687\n",
      "batch 349: loss 0.171533\n",
      "batch 350: loss 0.464378\n",
      "batch 351: loss 0.151648\n",
      "batch 352: loss 0.388390\n",
      "batch 353: loss 0.181905\n",
      "batch 354: loss 0.415234\n",
      "batch 355: loss 0.157202\n",
      "batch 356: loss 0.255685\n",
      "batch 357: loss 0.307585\n",
      "batch 358: loss 0.226237\n",
      "batch 359: loss 0.547283\n",
      "batch 360: loss 0.361755\n",
      "batch 361: loss 0.303769\n",
      "batch 362: loss 0.509508\n",
      "batch 363: loss 0.308833\n",
      "batch 364: loss 0.344018\n",
      "batch 365: loss 0.220481\n",
      "batch 366: loss 0.581784\n",
      "batch 367: loss 0.227999\n",
      "batch 368: loss 0.095050\n",
      "batch 369: loss 0.259848\n",
      "batch 370: loss 0.110438\n",
      "batch 371: loss 0.363719\n",
      "batch 372: loss 0.213979\n",
      "batch 373: loss 0.179563\n",
      "batch 374: loss 0.140545\n",
      "batch 375: loss 0.180561\n",
      "batch 376: loss 0.231421\n",
      "batch 377: loss 0.119296\n",
      "batch 378: loss 0.234714\n",
      "batch 379: loss 0.380187\n",
      "batch 380: loss 0.222900\n",
      "batch 381: loss 0.191367\n",
      "batch 382: loss 0.393101\n",
      "batch 383: loss 0.123177\n",
      "batch 384: loss 0.217729\n",
      "batch 385: loss 0.294316\n",
      "batch 386: loss 0.301708\n",
      "batch 387: loss 0.318840\n",
      "batch 388: loss 0.257627\n",
      "batch 389: loss 0.495202\n",
      "batch 390: loss 0.168601\n",
      "batch 391: loss 0.213054\n",
      "batch 392: loss 0.207379\n",
      "batch 393: loss 0.299209\n",
      "batch 394: loss 0.343220\n",
      "batch 395: loss 0.261734\n",
      "batch 396: loss 0.489628\n",
      "batch 397: loss 0.262958\n",
      "batch 398: loss 0.494589\n",
      "batch 399: loss 0.182659\n",
      "batch 400: loss 0.281138\n",
      "batch 401: loss 0.241332\n",
      "batch 402: loss 0.276116\n",
      "batch 403: loss 0.353301\n",
      "batch 404: loss 0.687106\n",
      "batch 405: loss 0.148632\n",
      "batch 406: loss 0.453912\n",
      "batch 407: loss 0.131475\n",
      "batch 408: loss 0.112438\n",
      "batch 409: loss 0.340046\n",
      "batch 410: loss 0.363725\n",
      "batch 411: loss 0.117435\n",
      "batch 412: loss 0.194296\n",
      "batch 413: loss 0.450075\n",
      "batch 414: loss 0.209348\n",
      "batch 415: loss 0.232890\n",
      "batch 416: loss 0.182664\n",
      "batch 417: loss 0.174336\n",
      "batch 418: loss 0.289006\n",
      "batch 419: loss 0.530288\n",
      "batch 420: loss 0.164647\n",
      "batch 421: loss 0.235275\n",
      "batch 422: loss 0.281388\n",
      "batch 423: loss 0.190360\n",
      "batch 424: loss 0.335454\n",
      "batch 425: loss 0.391556\n",
      "batch 426: loss 0.271388\n",
      "batch 427: loss 0.403743\n",
      "batch 428: loss 0.116718\n",
      "batch 429: loss 0.192149\n",
      "batch 430: loss 0.171758\n",
      "batch 431: loss 0.193538\n",
      "batch 432: loss 0.253099\n",
      "batch 433: loss 0.437124\n",
      "batch 434: loss 0.206490\n",
      "batch 435: loss 0.172530\n",
      "batch 436: loss 0.348532\n",
      "batch 437: loss 0.202924\n",
      "batch 438: loss 0.158047\n",
      "batch 439: loss 0.357190\n",
      "batch 440: loss 0.266374\n",
      "batch 441: loss 0.333129\n",
      "batch 442: loss 0.204470\n",
      "batch 443: loss 0.301857\n",
      "batch 444: loss 0.329272\n",
      "batch 445: loss 0.334048\n",
      "batch 446: loss 0.262241\n",
      "batch 447: loss 0.191592\n",
      "batch 448: loss 0.359752\n",
      "batch 449: loss 0.146988\n",
      "batch 450: loss 0.193670\n",
      "batch 451: loss 0.106725\n",
      "batch 452: loss 0.275810\n",
      "batch 453: loss 0.134853\n",
      "batch 454: loss 0.381886\n",
      "batch 455: loss 0.185544\n",
      "batch 456: loss 0.368384\n",
      "batch 457: loss 0.355418\n",
      "batch 458: loss 0.375425\n",
      "batch 459: loss 0.299059\n",
      "batch 460: loss 0.390671\n",
      "batch 461: loss 0.430616\n",
      "batch 462: loss 0.215048\n",
      "batch 463: loss 0.412541\n",
      "batch 464: loss 0.367556\n",
      "batch 465: loss 0.311395\n",
      "batch 466: loss 0.217471\n",
      "batch 467: loss 0.146600\n",
      "batch 468: loss 0.250884\n",
      "batch 469: loss 0.438440\n",
      "batch 470: loss 0.566493\n",
      "batch 471: loss 0.310169\n",
      "batch 472: loss 0.222948\n",
      "batch 473: loss 0.111162\n",
      "batch 474: loss 0.390077\n",
      "batch 475: loss 0.217201\n",
      "batch 476: loss 0.419012\n",
      "batch 477: loss 0.123774\n",
      "batch 478: loss 0.275516\n",
      "batch 479: loss 0.147594\n",
      "batch 480: loss 0.158840\n",
      "batch 481: loss 0.268080\n",
      "batch 482: loss 0.200689\n",
      "batch 483: loss 0.266500\n",
      "batch 484: loss 0.387414\n",
      "batch 485: loss 0.310897\n",
      "batch 486: loss 0.086428\n",
      "batch 487: loss 0.392515\n",
      "batch 488: loss 0.172942\n",
      "batch 489: loss 0.134825\n",
      "batch 490: loss 0.252705\n",
      "batch 491: loss 0.181767\n",
      "batch 492: loss 0.172344\n",
      "batch 493: loss 0.286413\n",
      "batch 494: loss 0.248499\n",
      "batch 495: loss 0.362275\n",
      "batch 496: loss 0.170649\n",
      "batch 497: loss 0.142173\n",
      "batch 498: loss 0.246149\n",
      "batch 499: loss 0.119469\n",
      "batch 500: loss 0.178043\n",
      "batch 501: loss 0.290593\n",
      "batch 502: loss 0.103601\n",
      "batch 503: loss 0.144491\n",
      "batch 504: loss 0.456038\n",
      "batch 505: loss 0.318336\n",
      "batch 506: loss 0.175734\n",
      "batch 507: loss 0.180013\n",
      "batch 508: loss 0.158114\n",
      "batch 509: loss 0.501320\n",
      "batch 510: loss 0.264619\n",
      "batch 511: loss 0.295582\n",
      "batch 512: loss 0.318857\n",
      "batch 513: loss 0.469293\n",
      "batch 514: loss 0.162152\n",
      "batch 515: loss 0.335068\n",
      "batch 516: loss 0.190141\n",
      "batch 517: loss 0.501530\n",
      "batch 518: loss 0.299600\n",
      "batch 519: loss 0.449126\n",
      "batch 520: loss 0.245176\n",
      "batch 521: loss 0.233155\n",
      "batch 522: loss 0.078303\n",
      "batch 523: loss 0.135371\n",
      "batch 524: loss 0.317976\n",
      "batch 525: loss 0.206967\n",
      "batch 526: loss 0.294195\n",
      "batch 527: loss 0.155627\n",
      "batch 528: loss 0.067506\n",
      "batch 529: loss 0.367943\n",
      "batch 530: loss 0.227197\n",
      "batch 531: loss 0.327301\n",
      "batch 532: loss 0.293514\n",
      "batch 533: loss 0.242949\n",
      "batch 534: loss 0.122864\n",
      "batch 535: loss 0.147420\n",
      "batch 536: loss 0.511990\n",
      "batch 537: loss 0.512341\n",
      "batch 538: loss 0.187079\n",
      "batch 539: loss 0.340608\n",
      "batch 540: loss 0.229218\n",
      "batch 541: loss 0.284447\n",
      "batch 542: loss 0.190286\n",
      "batch 543: loss 0.195267\n",
      "batch 544: loss 0.539803\n",
      "batch 545: loss 0.528123\n",
      "batch 546: loss 0.095702\n",
      "batch 547: loss 0.551220\n",
      "batch 548: loss 0.545000\n",
      "batch 549: loss 0.388175\n",
      "batch 550: loss 0.253042\n",
      "batch 551: loss 0.462136\n",
      "batch 552: loss 0.217434\n",
      "batch 553: loss 0.741221\n",
      "batch 554: loss 0.100826\n",
      "batch 555: loss 0.258872\n",
      "batch 556: loss 0.455496\n",
      "batch 557: loss 0.203384\n",
      "batch 558: loss 0.254501\n",
      "batch 559: loss 0.220807\n",
      "batch 560: loss 0.206975\n",
      "batch 561: loss 0.163056\n",
      "batch 562: loss 0.312333\n",
      "batch 563: loss 0.369660\n",
      "batch 564: loss 0.257338\n",
      "batch 565: loss 0.740938\n",
      "batch 566: loss 0.132921\n",
      "batch 567: loss 0.313476\n",
      "batch 568: loss 0.098825\n",
      "batch 569: loss 0.168911\n",
      "batch 570: loss 0.263878\n",
      "batch 571: loss 0.308270\n",
      "batch 572: loss 0.271770\n",
      "batch 573: loss 0.272106\n",
      "batch 574: loss 0.334743\n",
      "batch 575: loss 0.214935\n",
      "batch 576: loss 0.143811\n",
      "batch 577: loss 0.285131\n",
      "batch 578: loss 0.229746\n",
      "batch 579: loss 0.218480\n",
      "batch 580: loss 0.196816\n",
      "batch 581: loss 0.194367\n",
      "batch 582: loss 0.329648\n",
      "batch 583: loss 0.374658\n",
      "batch 584: loss 0.389994\n",
      "batch 585: loss 0.215425\n",
      "batch 586: loss 0.233858\n",
      "batch 587: loss 0.199319\n",
      "batch 588: loss 0.250777\n",
      "batch 589: loss 0.261952\n",
      "batch 590: loss 0.271209\n",
      "batch 591: loss 0.371419\n",
      "batch 592: loss 0.210425\n",
      "batch 593: loss 0.157568\n",
      "batch 594: loss 0.183699\n",
      "batch 595: loss 0.258913\n",
      "batch 596: loss 0.218828\n",
      "batch 597: loss 0.360495\n",
      "batch 598: loss 0.268276\n",
      "batch 599: loss 0.280176\n",
      "batch 600: loss 0.098202\n",
      "batch 601: loss 0.248394\n",
      "batch 602: loss 0.172107\n",
      "batch 603: loss 0.384313\n",
      "batch 604: loss 0.413905\n",
      "batch 605: loss 0.279744\n",
      "batch 606: loss 0.230402\n",
      "batch 607: loss 0.218683\n",
      "batch 608: loss 0.261915\n",
      "batch 609: loss 0.290429\n",
      "batch 610: loss 0.252186\n",
      "batch 611: loss 0.184761\n",
      "batch 612: loss 0.154967\n",
      "batch 613: loss 0.252717\n",
      "batch 614: loss 0.220748\n",
      "batch 615: loss 0.397576\n",
      "batch 616: loss 0.265084\n",
      "batch 617: loss 0.150059\n",
      "batch 618: loss 0.241305\n",
      "batch 619: loss 0.209720\n",
      "batch 620: loss 0.316695\n",
      "batch 621: loss 0.279503\n",
      "batch 622: loss 0.293685\n",
      "batch 623: loss 0.314327\n",
      "batch 624: loss 0.166770\n",
      "batch 625: loss 0.285619\n",
      "batch 626: loss 0.096364\n",
      "batch 627: loss 0.426538\n",
      "batch 628: loss 0.396321\n",
      "batch 629: loss 0.169320\n",
      "batch 630: loss 0.363638\n",
      "batch 631: loss 0.293005\n",
      "batch 632: loss 0.333727\n",
      "batch 633: loss 0.102910\n",
      "batch 634: loss 0.165562\n",
      "batch 635: loss 0.079254\n",
      "batch 636: loss 0.197848\n",
      "batch 637: loss 0.270755\n",
      "batch 638: loss 0.277713\n",
      "batch 639: loss 0.235775\n",
      "batch 640: loss 0.274044\n",
      "batch 641: loss 0.342510\n",
      "batch 642: loss 0.268056\n",
      "batch 643: loss 0.424979\n",
      "batch 644: loss 0.170383\n",
      "batch 645: loss 0.230134\n",
      "batch 646: loss 0.236978\n",
      "batch 647: loss 0.238915\n",
      "batch 648: loss 0.184973\n",
      "batch 649: loss 0.247035\n",
      "batch 650: loss 0.397122\n",
      "batch 651: loss 0.307894\n",
      "batch 652: loss 0.136688\n",
      "batch 653: loss 0.287444\n",
      "batch 654: loss 0.141604\n",
      "batch 655: loss 0.170866\n",
      "batch 656: loss 0.320588\n",
      "batch 657: loss 0.237801\n",
      "batch 658: loss 0.634109\n",
      "batch 659: loss 0.306561\n",
      "batch 660: loss 0.399606\n",
      "batch 661: loss 0.101172\n",
      "batch 662: loss 0.131707\n",
      "batch 663: loss 0.261848\n",
      "batch 664: loss 0.209101\n",
      "batch 665: loss 0.290141\n",
      "batch 666: loss 0.358341\n",
      "batch 667: loss 0.256025\n",
      "batch 668: loss 0.184499\n",
      "batch 669: loss 0.164117\n",
      "batch 670: loss 0.165340\n",
      "batch 671: loss 0.208563\n",
      "batch 672: loss 0.197489\n",
      "batch 673: loss 0.399873\n",
      "batch 674: loss 0.360083\n",
      "batch 675: loss 0.399485\n",
      "batch 676: loss 0.367336\n",
      "batch 677: loss 0.247830\n",
      "batch 678: loss 0.059425\n",
      "batch 679: loss 0.346583\n",
      "batch 680: loss 0.058729\n",
      "batch 681: loss 0.348707\n",
      "batch 682: loss 0.299608\n",
      "batch 683: loss 0.374106\n",
      "batch 684: loss 0.168598\n",
      "batch 685: loss 0.450227\n",
      "batch 686: loss 0.110164\n",
      "batch 687: loss 0.170927\n",
      "batch 688: loss 0.084349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 689: loss 0.276521\n",
      "batch 690: loss 0.197018\n",
      "batch 691: loss 0.146852\n",
      "batch 692: loss 0.278351\n",
      "batch 693: loss 0.347144\n",
      "batch 694: loss 0.245358\n",
      "batch 695: loss 0.149734\n",
      "batch 696: loss 0.406438\n",
      "batch 697: loss 0.246454\n",
      "batch 698: loss 0.263120\n",
      "batch 699: loss 0.367937\n",
      "batch 700: loss 0.197677\n",
      "batch 701: loss 0.133493\n",
      "batch 702: loss 0.224651\n",
      "batch 703: loss 0.148068\n",
      "batch 704: loss 0.171978\n",
      "batch 705: loss 0.176914\n",
      "batch 706: loss 0.242766\n",
      "batch 707: loss 0.234031\n",
      "batch 708: loss 0.146912\n",
      "batch 709: loss 0.249944\n",
      "batch 710: loss 0.313430\n",
      "batch 711: loss 0.153269\n",
      "batch 712: loss 0.379415\n",
      "batch 713: loss 0.303345\n",
      "batch 714: loss 0.284786\n",
      "batch 715: loss 0.168590\n",
      "batch 716: loss 0.257250\n",
      "batch 717: loss 0.284959\n",
      "batch 718: loss 0.282893\n",
      "batch 719: loss 0.308859\n",
      "batch 720: loss 0.269452\n",
      "batch 721: loss 0.116037\n",
      "batch 722: loss 0.164277\n",
      "batch 723: loss 0.251132\n",
      "batch 724: loss 0.206345\n",
      "batch 725: loss 0.158812\n",
      "batch 726: loss 0.225475\n",
      "batch 727: loss 0.205370\n",
      "batch 728: loss 0.108253\n",
      "batch 729: loss 0.332475\n",
      "batch 730: loss 0.202984\n",
      "batch 731: loss 0.398229\n",
      "batch 732: loss 0.272558\n",
      "batch 733: loss 0.166159\n",
      "batch 734: loss 0.249589\n",
      "batch 735: loss 0.159070\n",
      "batch 736: loss 0.142921\n",
      "batch 737: loss 0.413367\n",
      "batch 738: loss 0.444057\n",
      "batch 739: loss 0.138182\n",
      "batch 740: loss 0.187191\n",
      "batch 741: loss 0.293467\n",
      "batch 742: loss 0.217010\n",
      "batch 743: loss 0.265963\n",
      "batch 744: loss 0.228214\n",
      "batch 745: loss 0.285867\n",
      "batch 746: loss 0.159868\n",
      "batch 747: loss 0.177509\n",
      "batch 748: loss 0.100936\n",
      "batch 749: loss 0.265431\n",
      "batch 750: loss 0.230849\n",
      "batch 751: loss 0.232065\n",
      "batch 752: loss 0.251665\n",
      "batch 753: loss 0.129789\n",
      "batch 754: loss 0.235402\n",
      "batch 755: loss 0.357242\n",
      "batch 756: loss 0.342059\n",
      "batch 757: loss 0.151108\n",
      "batch 758: loss 0.112304\n",
      "batch 759: loss 0.456489\n",
      "batch 760: loss 0.199101\n",
      "batch 761: loss 0.283452\n",
      "batch 762: loss 0.162524\n",
      "batch 763: loss 0.154952\n",
      "batch 764: loss 0.227628\n",
      "batch 765: loss 0.183012\n",
      "batch 766: loss 0.105600\n",
      "batch 767: loss 0.232410\n",
      "batch 768: loss 0.307458\n",
      "batch 769: loss 0.308538\n",
      "batch 770: loss 0.131878\n",
      "batch 771: loss 0.257541\n",
      "batch 772: loss 0.244571\n",
      "batch 773: loss 0.220207\n",
      "batch 774: loss 0.141473\n",
      "batch 775: loss 0.288952\n",
      "batch 776: loss 0.600649\n",
      "batch 777: loss 0.279429\n",
      "batch 778: loss 0.395540\n",
      "batch 779: loss 0.173265\n",
      "batch 780: loss 0.489813\n",
      "batch 781: loss 0.109606\n",
      "batch 782: loss 0.353864\n",
      "batch 783: loss 0.516853\n",
      "batch 784: loss 0.224594\n",
      "batch 785: loss 0.189038\n",
      "batch 786: loss 0.222281\n",
      "batch 787: loss 0.239645\n",
      "batch 788: loss 0.072850\n",
      "batch 789: loss 0.276745\n",
      "batch 790: loss 0.424079\n",
      "batch 791: loss 0.196488\n",
      "batch 792: loss 0.260870\n",
      "batch 793: loss 0.051474\n",
      "batch 794: loss 0.214813\n",
      "batch 795: loss 0.169840\n",
      "batch 796: loss 0.244915\n",
      "batch 797: loss 0.400906\n",
      "batch 798: loss 0.262976\n",
      "batch 799: loss 0.186214\n",
      "batch 800: loss 0.208038\n",
      "batch 801: loss 0.189066\n",
      "batch 802: loss 0.197092\n",
      "batch 803: loss 0.390048\n",
      "batch 804: loss 0.179475\n",
      "batch 805: loss 0.171898\n",
      "batch 806: loss 0.355158\n",
      "batch 807: loss 0.172390\n",
      "batch 808: loss 0.173641\n",
      "batch 809: loss 0.222242\n",
      "batch 810: loss 0.151108\n",
      "batch 811: loss 0.120114\n",
      "batch 812: loss 0.079667\n",
      "batch 813: loss 0.505940\n",
      "batch 814: loss 0.105434\n",
      "batch 815: loss 0.291187\n",
      "batch 816: loss 0.189079\n",
      "batch 817: loss 0.123294\n",
      "batch 818: loss 0.224572\n",
      "batch 819: loss 0.149499\n",
      "batch 820: loss 0.341067\n",
      "batch 821: loss 0.184941\n",
      "batch 822: loss 0.155114\n",
      "batch 823: loss 0.295780\n",
      "batch 824: loss 0.113283\n",
      "batch 825: loss 0.070802\n",
      "batch 826: loss 0.240651\n",
      "batch 827: loss 0.142079\n",
      "batch 828: loss 0.250727\n",
      "batch 829: loss 0.223717\n",
      "batch 830: loss 0.217870\n",
      "batch 831: loss 0.194989\n",
      "batch 832: loss 0.273690\n",
      "batch 833: loss 0.192242\n",
      "batch 834: loss 0.172057\n",
      "batch 835: loss 0.768494\n",
      "batch 836: loss 0.158646\n",
      "batch 837: loss 0.158564\n",
      "batch 838: loss 0.170878\n",
      "batch 839: loss 0.278796\n",
      "batch 840: loss 0.148890\n",
      "batch 841: loss 0.092551\n",
      "batch 842: loss 0.253707\n",
      "batch 843: loss 0.229055\n",
      "batch 844: loss 0.271748\n",
      "batch 845: loss 0.295613\n",
      "batch 846: loss 0.145687\n",
      "batch 847: loss 0.300645\n",
      "batch 848: loss 0.210212\n",
      "batch 849: loss 0.335056\n",
      "batch 850: loss 0.100494\n",
      "batch 851: loss 0.371898\n",
      "batch 852: loss 0.322040\n",
      "batch 853: loss 0.164644\n",
      "batch 854: loss 0.453797\n",
      "batch 855: loss 0.076487\n",
      "batch 856: loss 0.092810\n",
      "batch 857: loss 0.086531\n",
      "batch 858: loss 0.102997\n",
      "batch 859: loss 0.086332\n",
      "batch 860: loss 0.192498\n",
      "batch 861: loss 0.130774\n",
      "batch 862: loss 0.228266\n",
      "batch 863: loss 0.200916\n",
      "batch 864: loss 0.211627\n",
      "batch 865: loss 0.156518\n",
      "batch 866: loss 0.206487\n",
      "batch 867: loss 0.354224\n",
      "batch 868: loss 0.355933\n",
      "batch 869: loss 0.246086\n",
      "batch 870: loss 0.102805\n",
      "batch 871: loss 0.312806\n",
      "batch 872: loss 0.269111\n",
      "batch 873: loss 0.173809\n",
      "batch 874: loss 0.164609\n",
      "batch 875: loss 0.334884\n",
      "batch 876: loss 0.081638\n",
      "batch 877: loss 0.239630\n",
      "batch 878: loss 0.382669\n",
      "batch 879: loss 0.143467\n",
      "batch 880: loss 0.291840\n",
      "batch 881: loss 0.177002\n",
      "batch 882: loss 0.045555\n",
      "batch 883: loss 0.224074\n",
      "batch 884: loss 0.351278\n",
      "batch 885: loss 0.119858\n",
      "batch 886: loss 0.402094\n",
      "batch 887: loss 0.277234\n",
      "batch 888: loss 0.292477\n",
      "batch 889: loss 0.208325\n",
      "batch 890: loss 0.444812\n",
      "batch 891: loss 0.360999\n",
      "batch 892: loss 0.265203\n",
      "batch 893: loss 0.186682\n",
      "batch 894: loss 0.439521\n",
      "batch 895: loss 0.336455\n",
      "batch 896: loss 0.196538\n",
      "batch 897: loss 0.196109\n",
      "batch 898: loss 0.305727\n",
      "batch 899: loss 0.394184\n",
      "batch 900: loss 0.286174\n",
      "batch 901: loss 0.073409\n",
      "batch 902: loss 0.231404\n",
      "batch 903: loss 0.193731\n",
      "batch 904: loss 0.067033\n",
      "batch 905: loss 0.118338\n",
      "batch 906: loss 0.388088\n",
      "batch 907: loss 0.283198\n",
      "batch 908: loss 0.209847\n",
      "batch 909: loss 0.160412\n",
      "batch 910: loss 0.223320\n",
      "batch 911: loss 0.075839\n",
      "batch 912: loss 0.119133\n",
      "batch 913: loss 0.136980\n",
      "batch 914: loss 0.151433\n",
      "batch 915: loss 0.224672\n",
      "batch 916: loss 0.149184\n",
      "batch 917: loss 0.134801\n",
      "batch 918: loss 0.295730\n",
      "batch 919: loss 0.287036\n",
      "batch 920: loss 0.200908\n",
      "batch 921: loss 0.163517\n",
      "batch 922: loss 0.221331\n",
      "batch 923: loss 0.169185\n",
      "batch 924: loss 0.149678\n",
      "batch 925: loss 0.079540\n",
      "batch 926: loss 0.078628\n",
      "batch 927: loss 0.162498\n",
      "batch 928: loss 0.243492\n",
      "batch 929: loss 0.157221\n",
      "batch 930: loss 0.213412\n",
      "batch 931: loss 0.294293\n",
      "batch 932: loss 0.143134\n",
      "batch 933: loss 0.136711\n",
      "batch 934: loss 0.123567\n",
      "batch 935: loss 0.161178\n",
      "batch 936: loss 0.236771\n",
      "batch 937: loss 0.176854\n",
      "batch 938: loss 0.269143\n",
      "batch 939: loss 0.077436\n",
      "batch 940: loss 0.106548\n",
      "batch 941: loss 0.195111\n",
      "batch 942: loss 0.237518\n",
      "batch 943: loss 0.467460\n",
      "batch 944: loss 0.174417\n",
      "batch 945: loss 0.416327\n",
      "batch 946: loss 0.263304\n",
      "batch 947: loss 0.501312\n",
      "batch 948: loss 0.301235\n",
      "batch 949: loss 0.109031\n",
      "batch 950: loss 0.285246\n",
      "batch 951: loss 0.169238\n",
      "batch 952: loss 0.192490\n",
      "batch 953: loss 0.148883\n",
      "batch 954: loss 0.093688\n",
      "batch 955: loss 0.285032\n",
      "batch 956: loss 0.071371\n",
      "batch 957: loss 0.081870\n",
      "batch 958: loss 0.334152\n",
      "batch 959: loss 0.295863\n",
      "batch 960: loss 0.074769\n",
      "batch 961: loss 0.143149\n",
      "batch 962: loss 0.116235\n",
      "batch 963: loss 0.474649\n",
      "batch 964: loss 0.417961\n",
      "batch 965: loss 0.144430\n",
      "batch 966: loss 0.191141\n",
      "batch 967: loss 0.137553\n",
      "batch 968: loss 0.260210\n",
      "batch 969: loss 0.444065\n",
      "batch 970: loss 0.157354\n",
      "batch 971: loss 0.171859\n",
      "batch 972: loss 0.089686\n",
      "batch 973: loss 0.185671\n",
      "batch 974: loss 0.184463\n",
      "batch 975: loss 0.098622\n",
      "batch 976: loss 0.151526\n",
      "batch 977: loss 0.263677\n",
      "batch 978: loss 0.153324\n",
      "batch 979: loss 0.340473\n",
      "batch 980: loss 0.268252\n",
      "batch 981: loss 0.068009\n",
      "batch 982: loss 0.180985\n",
      "batch 983: loss 0.153524\n",
      "batch 984: loss 0.203474\n",
      "batch 985: loss 0.259299\n",
      "batch 986: loss 0.111548\n",
      "batch 987: loss 0.218107\n",
      "batch 988: loss 0.131198\n",
      "batch 989: loss 0.367819\n",
      "batch 990: loss 0.260205\n",
      "batch 991: loss 0.105391\n",
      "batch 992: loss 0.244162\n",
      "batch 993: loss 0.133328\n",
      "batch 994: loss 0.190407\n",
      "batch 995: loss 0.177519\n",
      "batch 996: loss 0.181761\n",
      "batch 997: loss 0.471425\n",
      "batch 998: loss 0.253550\n",
      "batch 999: loss 0.302396\n",
      "batch 1000: loss 0.252982\n",
      "batch 1001: loss 0.112117\n",
      "batch 1002: loss 0.309968\n",
      "batch 1003: loss 0.274866\n",
      "batch 1004: loss 0.372569\n",
      "batch 1005: loss 0.093960\n",
      "batch 1006: loss 0.270445\n",
      "batch 1007: loss 0.105686\n",
      "batch 1008: loss 0.169003\n",
      "batch 1009: loss 0.174823\n",
      "batch 1010: loss 0.231092\n",
      "batch 1011: loss 0.103317\n",
      "batch 1012: loss 0.306222\n",
      "batch 1013: loss 0.164179\n",
      "batch 1014: loss 0.185308\n",
      "batch 1015: loss 0.509880\n",
      "batch 1016: loss 0.189211\n",
      "batch 1017: loss 0.133245\n",
      "batch 1018: loss 0.129887\n",
      "batch 1019: loss 0.397506\n",
      "batch 1020: loss 0.144865\n",
      "batch 1021: loss 0.250948\n",
      "batch 1022: loss 0.196017\n",
      "batch 1023: loss 0.456615\n",
      "batch 1024: loss 0.155027\n",
      "batch 1025: loss 0.091884\n",
      "batch 1026: loss 0.147028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1027: loss 0.347910\n",
      "batch 1028: loss 0.124907\n",
      "batch 1029: loss 0.153223\n",
      "batch 1030: loss 0.124346\n",
      "batch 1031: loss 0.346138\n",
      "batch 1032: loss 0.444010\n",
      "batch 1033: loss 0.239679\n",
      "batch 1034: loss 0.233300\n",
      "batch 1035: loss 0.287567\n",
      "batch 1036: loss 0.132080\n",
      "batch 1037: loss 0.131373\n",
      "batch 1038: loss 0.142361\n",
      "batch 1039: loss 0.094616\n",
      "batch 1040: loss 0.465594\n",
      "batch 1041: loss 0.076073\n",
      "batch 1042: loss 0.165712\n",
      "batch 1043: loss 0.315378\n",
      "batch 1044: loss 0.121304\n",
      "batch 1045: loss 0.171625\n",
      "batch 1046: loss 0.219969\n",
      "batch 1047: loss 0.181089\n",
      "batch 1048: loss 0.076397\n",
      "batch 1049: loss 0.206546\n",
      "batch 1050: loss 0.375027\n",
      "batch 1051: loss 0.066648\n",
      "batch 1052: loss 0.110971\n",
      "batch 1053: loss 0.148272\n",
      "batch 1054: loss 0.343098\n",
      "batch 1055: loss 0.058196\n",
      "batch 1056: loss 0.146542\n",
      "batch 1057: loss 0.116397\n",
      "batch 1058: loss 0.122697\n",
      "batch 1059: loss 0.105441\n",
      "batch 1060: loss 0.143216\n",
      "batch 1061: loss 0.050943\n",
      "batch 1062: loss 0.351195\n",
      "batch 1063: loss 0.106332\n",
      "batch 1064: loss 0.102014\n",
      "batch 1065: loss 0.250603\n",
      "batch 1066: loss 0.187510\n",
      "batch 1067: loss 0.297756\n",
      "batch 1068: loss 0.054617\n",
      "batch 1069: loss 0.147509\n",
      "batch 1070: loss 0.176911\n",
      "batch 1071: loss 0.054885\n",
      "batch 1072: loss 0.145101\n",
      "batch 1073: loss 0.249805\n",
      "batch 1074: loss 0.155793\n",
      "batch 1075: loss 0.255284\n",
      "batch 1076: loss 0.454715\n",
      "batch 1077: loss 0.201141\n",
      "batch 1078: loss 0.126493\n",
      "batch 1079: loss 0.199469\n",
      "batch 1080: loss 0.304395\n",
      "batch 1081: loss 0.337510\n",
      "batch 1082: loss 0.289631\n",
      "batch 1083: loss 0.240047\n",
      "batch 1084: loss 0.158301\n",
      "batch 1085: loss 0.125175\n",
      "batch 1086: loss 0.164557\n",
      "batch 1087: loss 0.143629\n",
      "batch 1088: loss 0.315846\n",
      "batch 1089: loss 0.195745\n",
      "batch 1090: loss 0.262852\n",
      "batch 1091: loss 0.246459\n",
      "batch 1092: loss 0.246063\n",
      "batch 1093: loss 0.297113\n",
      "batch 1094: loss 0.181925\n",
      "batch 1095: loss 0.202002\n",
      "batch 1096: loss 0.103951\n",
      "batch 1097: loss 0.269386\n",
      "batch 1098: loss 0.303875\n",
      "batch 1099: loss 0.205652\n",
      "batch 1100: loss 0.143553\n",
      "batch 1101: loss 0.180668\n",
      "batch 1102: loss 0.068994\n",
      "batch 1103: loss 0.209940\n",
      "batch 1104: loss 0.217607\n",
      "batch 1105: loss 0.242707\n",
      "batch 1106: loss 0.104227\n",
      "batch 1107: loss 0.074483\n",
      "batch 1108: loss 0.214958\n",
      "batch 1109: loss 0.205640\n",
      "batch 1110: loss 0.168242\n",
      "batch 1111: loss 0.113087\n",
      "batch 1112: loss 0.089099\n",
      "batch 1113: loss 0.229533\n",
      "batch 1114: loss 0.101655\n",
      "batch 1115: loss 0.467839\n",
      "batch 1116: loss 0.212640\n",
      "batch 1117: loss 0.240247\n",
      "batch 1118: loss 0.143820\n",
      "batch 1119: loss 0.138113\n",
      "batch 1120: loss 0.418721\n",
      "batch 1121: loss 0.247325\n",
      "batch 1122: loss 0.274685\n",
      "batch 1123: loss 0.135000\n",
      "batch 1124: loss 0.146372\n",
      "batch 1125: loss 0.097549\n",
      "batch 1126: loss 0.125917\n",
      "batch 1127: loss 0.061163\n",
      "batch 1128: loss 0.134704\n",
      "batch 1129: loss 0.143983\n",
      "batch 1130: loss 0.154915\n",
      "batch 1131: loss 0.098795\n",
      "batch 1132: loss 0.295715\n",
      "batch 1133: loss 0.179273\n",
      "batch 1134: loss 0.076628\n",
      "batch 1135: loss 0.132278\n",
      "batch 1136: loss 0.245502\n",
      "batch 1137: loss 0.227308\n",
      "batch 1138: loss 0.106454\n",
      "batch 1139: loss 0.265598\n",
      "batch 1140: loss 0.177614\n",
      "batch 1141: loss 0.255312\n",
      "batch 1142: loss 0.235518\n",
      "batch 1143: loss 0.170583\n",
      "batch 1144: loss 0.256201\n",
      "batch 1145: loss 0.098173\n",
      "batch 1146: loss 0.211501\n",
      "batch 1147: loss 0.149772\n",
      "batch 1148: loss 0.204817\n",
      "batch 1149: loss 0.262201\n",
      "batch 1150: loss 0.170949\n",
      "batch 1151: loss 0.085371\n",
      "batch 1152: loss 0.346903\n",
      "batch 1153: loss 0.154204\n",
      "batch 1154: loss 0.111379\n",
      "batch 1155: loss 0.168822\n",
      "batch 1156: loss 0.065350\n",
      "batch 1157: loss 0.067569\n",
      "batch 1158: loss 0.218880\n",
      "batch 1159: loss 0.176624\n",
      "batch 1160: loss 0.204164\n",
      "batch 1161: loss 0.115203\n",
      "batch 1162: loss 0.266675\n",
      "batch 1163: loss 0.354747\n",
      "batch 1164: loss 0.200922\n",
      "batch 1165: loss 0.281580\n",
      "batch 1166: loss 0.353728\n",
      "batch 1167: loss 0.083669\n",
      "batch 1168: loss 0.191699\n",
      "batch 1169: loss 0.130422\n",
      "batch 1170: loss 0.318328\n",
      "batch 1171: loss 0.135190\n",
      "batch 1172: loss 0.213135\n",
      "batch 1173: loss 0.276942\n",
      "batch 1174: loss 0.179310\n",
      "batch 1175: loss 0.110322\n",
      "batch 1176: loss 0.151625\n",
      "batch 1177: loss 0.172840\n",
      "batch 1178: loss 0.150892\n",
      "batch 1179: loss 0.393430\n",
      "batch 1180: loss 0.082792\n",
      "batch 1181: loss 0.318272\n",
      "batch 1182: loss 0.091479\n",
      "batch 1183: loss 0.154168\n",
      "batch 1184: loss 0.108903\n",
      "batch 1185: loss 0.126417\n",
      "batch 1186: loss 0.120071\n",
      "batch 1187: loss 0.393686\n",
      "batch 1188: loss 0.069945\n",
      "batch 1189: loss 0.340988\n",
      "batch 1190: loss 0.127382\n",
      "batch 1191: loss 0.153753\n",
      "batch 1192: loss 0.188088\n",
      "batch 1193: loss 0.168183\n",
      "batch 1194: loss 0.327433\n",
      "batch 1195: loss 0.075919\n",
      "batch 1196: loss 0.379079\n",
      "batch 1197: loss 0.182800\n",
      "batch 1198: loss 0.274423\n",
      "batch 1199: loss 0.113663\n",
      "batch 1200: loss 0.063374\n",
      "batch 1201: loss 0.303684\n",
      "batch 1202: loss 0.137770\n",
      "batch 1203: loss 0.125273\n",
      "batch 1204: loss 0.093423\n",
      "batch 1205: loss 0.173060\n",
      "batch 1206: loss 0.142813\n",
      "batch 1207: loss 0.275464\n",
      "batch 1208: loss 0.169789\n",
      "batch 1209: loss 0.174839\n",
      "batch 1210: loss 0.144500\n",
      "batch 1211: loss 0.226397\n",
      "batch 1212: loss 0.147356\n",
      "batch 1213: loss 0.242396\n",
      "batch 1214: loss 0.255263\n",
      "batch 1215: loss 0.174854\n",
      "batch 1216: loss 0.149012\n",
      "batch 1217: loss 0.180966\n",
      "batch 1218: loss 0.073078\n",
      "batch 1219: loss 0.117785\n",
      "batch 1220: loss 0.091845\n",
      "batch 1221: loss 0.203782\n",
      "batch 1222: loss 0.126300\n",
      "batch 1223: loss 0.093220\n",
      "batch 1224: loss 0.142885\n",
      "batch 1225: loss 0.164409\n",
      "batch 1226: loss 0.165318\n",
      "batch 1227: loss 0.169819\n",
      "batch 1228: loss 0.303735\n",
      "batch 1229: loss 0.503753\n",
      "batch 1230: loss 0.114343\n",
      "batch 1231: loss 0.067395\n",
      "batch 1232: loss 0.240008\n",
      "batch 1233: loss 0.136525\n",
      "batch 1234: loss 0.196724\n",
      "batch 1235: loss 0.250817\n",
      "batch 1236: loss 0.339520\n",
      "batch 1237: loss 0.247793\n",
      "batch 1238: loss 0.236575\n",
      "batch 1239: loss 0.162557\n",
      "batch 1240: loss 0.330877\n",
      "batch 1241: loss 0.114324\n",
      "batch 1242: loss 0.162122\n",
      "batch 1243: loss 0.195398\n",
      "batch 1244: loss 0.227531\n",
      "batch 1245: loss 0.198349\n",
      "batch 1246: loss 0.099994\n",
      "batch 1247: loss 0.507498\n",
      "batch 1248: loss 0.062296\n",
      "batch 1249: loss 0.284040\n",
      "batch 1250: loss 0.251997\n",
      "batch 1251: loss 0.248567\n",
      "batch 1252: loss 0.112588\n",
      "batch 1253: loss 0.148085\n",
      "batch 1254: loss 0.240252\n",
      "batch 1255: loss 0.060379\n",
      "batch 1256: loss 0.217291\n",
      "batch 1257: loss 0.259890\n",
      "batch 1258: loss 0.166573\n",
      "batch 1259: loss 0.104376\n",
      "batch 1260: loss 0.164931\n",
      "batch 1261: loss 0.072829\n",
      "batch 1262: loss 0.161355\n",
      "batch 1263: loss 0.276216\n",
      "batch 1264: loss 0.201272\n",
      "batch 1265: loss 0.135141\n",
      "batch 1266: loss 0.179743\n",
      "batch 1267: loss 0.395247\n",
      "batch 1268: loss 0.217260\n",
      "batch 1269: loss 0.022334\n",
      "batch 1270: loss 0.036175\n",
      "batch 1271: loss 0.143355\n",
      "batch 1272: loss 0.227492\n",
      "batch 1273: loss 0.280860\n",
      "batch 1274: loss 0.299067\n",
      "batch 1275: loss 0.143419\n",
      "batch 1276: loss 0.155250\n",
      "batch 1277: loss 0.188947\n",
      "batch 1278: loss 0.287464\n",
      "batch 1279: loss 0.103726\n",
      "batch 1280: loss 0.108382\n",
      "batch 1281: loss 0.168454\n",
      "batch 1282: loss 0.156252\n",
      "batch 1283: loss 0.466070\n",
      "batch 1284: loss 0.099060\n",
      "batch 1285: loss 0.195115\n",
      "batch 1286: loss 0.145305\n",
      "batch 1287: loss 0.075121\n",
      "batch 1288: loss 0.159221\n",
      "batch 1289: loss 0.051946\n",
      "batch 1290: loss 0.063478\n",
      "batch 1291: loss 0.164264\n",
      "batch 1292: loss 0.144477\n",
      "batch 1293: loss 0.504921\n",
      "batch 1294: loss 0.112646\n",
      "batch 1295: loss 0.175466\n",
      "batch 1296: loss 0.275629\n",
      "batch 1297: loss 0.158934\n",
      "batch 1298: loss 0.197357\n",
      "batch 1299: loss 0.058057\n",
      "batch 1300: loss 0.098818\n",
      "batch 1301: loss 0.121587\n",
      "batch 1302: loss 0.208070\n",
      "batch 1303: loss 0.117109\n",
      "batch 1304: loss 0.033939\n",
      "batch 1305: loss 0.212217\n",
      "batch 1306: loss 0.163860\n",
      "batch 1307: loss 0.174604\n",
      "batch 1308: loss 0.293395\n",
      "batch 1309: loss 0.202105\n",
      "batch 1310: loss 0.160538\n",
      "batch 1311: loss 0.221503\n",
      "batch 1312: loss 0.282522\n",
      "batch 1313: loss 0.140356\n",
      "batch 1314: loss 0.256001\n",
      "batch 1315: loss 0.206568\n",
      "batch 1316: loss 0.127139\n",
      "batch 1317: loss 0.072320\n",
      "batch 1318: loss 0.081406\n",
      "batch 1319: loss 0.099764\n",
      "batch 1320: loss 0.052601\n",
      "batch 1321: loss 0.149498\n",
      "batch 1322: loss 0.264407\n",
      "batch 1323: loss 0.113004\n",
      "batch 1324: loss 0.219903\n",
      "batch 1325: loss 0.073144\n",
      "batch 1326: loss 0.210649\n",
      "batch 1327: loss 0.105903\n",
      "batch 1328: loss 0.147302\n",
      "batch 1329: loss 0.116975\n",
      "batch 1330: loss 0.156800\n",
      "batch 1331: loss 0.173790\n",
      "batch 1332: loss 0.241983\n",
      "batch 1333: loss 0.128216\n",
      "batch 1334: loss 0.141526\n",
      "batch 1335: loss 0.068504\n",
      "batch 1336: loss 0.182606\n",
      "batch 1337: loss 0.110495\n",
      "batch 1338: loss 0.043827\n",
      "batch 1339: loss 0.109987\n",
      "batch 1340: loss 0.100352\n",
      "batch 1341: loss 0.047639\n",
      "batch 1342: loss 0.152015\n",
      "batch 1343: loss 0.388436\n",
      "batch 1344: loss 0.136580\n",
      "batch 1345: loss 0.147870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1346: loss 0.165231\n",
      "batch 1347: loss 0.098109\n",
      "batch 1348: loss 0.050048\n",
      "batch 1349: loss 0.153288\n",
      "batch 1350: loss 0.179972\n",
      "batch 1351: loss 0.175190\n",
      "batch 1352: loss 0.484593\n",
      "batch 1353: loss 0.281796\n",
      "batch 1354: loss 0.160132\n",
      "batch 1355: loss 0.127394\n",
      "batch 1356: loss 0.258443\n",
      "batch 1357: loss 0.193033\n",
      "batch 1358: loss 0.214489\n",
      "batch 1359: loss 0.273306\n",
      "batch 1360: loss 0.145166\n",
      "batch 1361: loss 0.153093\n",
      "batch 1362: loss 0.110154\n",
      "batch 1363: loss 0.050463\n",
      "batch 1364: loss 0.297580\n",
      "batch 1365: loss 0.116928\n",
      "batch 1366: loss 0.304578\n",
      "batch 1367: loss 0.114398\n",
      "batch 1368: loss 0.211822\n",
      "batch 1369: loss 0.113991\n",
      "batch 1370: loss 0.085770\n",
      "batch 1371: loss 0.083854\n",
      "batch 1372: loss 0.212035\n",
      "batch 1373: loss 0.094398\n",
      "batch 1374: loss 0.495733\n",
      "batch 1375: loss 0.105103\n",
      "batch 1376: loss 0.050907\n",
      "batch 1377: loss 0.345814\n",
      "batch 1378: loss 0.235748\n",
      "batch 1379: loss 0.101768\n",
      "batch 1380: loss 0.350953\n",
      "batch 1381: loss 0.210437\n",
      "batch 1382: loss 0.152381\n",
      "batch 1383: loss 0.181024\n",
      "batch 1384: loss 0.206888\n",
      "batch 1385: loss 0.301557\n",
      "batch 1386: loss 0.366216\n",
      "batch 1387: loss 0.105084\n",
      "batch 1388: loss 0.185382\n",
      "batch 1389: loss 0.259095\n",
      "batch 1390: loss 0.049623\n",
      "batch 1391: loss 0.255309\n",
      "batch 1392: loss 0.106055\n",
      "batch 1393: loss 0.137707\n",
      "batch 1394: loss 0.074313\n",
      "batch 1395: loss 0.139165\n",
      "batch 1396: loss 0.260834\n",
      "batch 1397: loss 0.172192\n",
      "batch 1398: loss 0.230459\n",
      "batch 1399: loss 0.249419\n",
      "batch 1400: loss 0.160627\n",
      "batch 1401: loss 0.085284\n",
      "batch 1402: loss 0.306987\n",
      "batch 1403: loss 0.408299\n",
      "batch 1404: loss 0.051867\n",
      "batch 1405: loss 0.108334\n",
      "batch 1406: loss 0.140301\n",
      "batch 1407: loss 0.172687\n",
      "batch 1408: loss 0.172456\n",
      "batch 1409: loss 0.084119\n",
      "batch 1410: loss 0.176113\n",
      "batch 1411: loss 0.249439\n",
      "batch 1412: loss 0.139715\n",
      "batch 1413: loss 0.380180\n",
      "batch 1414: loss 0.287874\n",
      "batch 1415: loss 0.106674\n",
      "batch 1416: loss 0.129694\n",
      "batch 1417: loss 0.071501\n",
      "batch 1418: loss 0.139701\n",
      "batch 1419: loss 0.255556\n",
      "batch 1420: loss 0.068830\n",
      "batch 1421: loss 0.212833\n",
      "batch 1422: loss 0.232340\n",
      "batch 1423: loss 0.070428\n",
      "batch 1424: loss 0.169932\n",
      "batch 1425: loss 0.187337\n",
      "batch 1426: loss 0.129974\n",
      "batch 1427: loss 0.161250\n",
      "batch 1428: loss 0.109676\n",
      "batch 1429: loss 0.252334\n",
      "batch 1430: loss 0.084311\n",
      "batch 1431: loss 0.167562\n",
      "batch 1432: loss 0.310289\n",
      "batch 1433: loss 0.161545\n",
      "batch 1434: loss 0.191782\n",
      "batch 1435: loss 0.088800\n",
      "batch 1436: loss 0.169943\n",
      "batch 1437: loss 0.260474\n",
      "batch 1438: loss 0.073673\n",
      "batch 1439: loss 0.197173\n",
      "batch 1440: loss 0.139002\n",
      "batch 1441: loss 0.106871\n",
      "batch 1442: loss 0.173294\n",
      "batch 1443: loss 0.193792\n",
      "batch 1444: loss 0.097038\n",
      "batch 1445: loss 0.247586\n",
      "batch 1446: loss 0.282845\n",
      "batch 1447: loss 0.202574\n",
      "batch 1448: loss 0.188446\n",
      "batch 1449: loss 0.112915\n",
      "batch 1450: loss 0.269734\n",
      "batch 1451: loss 0.164588\n",
      "batch 1452: loss 0.185432\n",
      "batch 1453: loss 0.281062\n",
      "batch 1454: loss 0.149054\n",
      "batch 1455: loss 0.241715\n",
      "batch 1456: loss 0.225569\n",
      "batch 1457: loss 0.114031\n",
      "batch 1458: loss 0.213935\n",
      "batch 1459: loss 0.251336\n",
      "batch 1460: loss 0.425046\n",
      "batch 1461: loss 0.160972\n",
      "batch 1462: loss 0.208382\n",
      "batch 1463: loss 0.093779\n",
      "batch 1464: loss 0.098610\n",
      "batch 1465: loss 0.079559\n",
      "batch 1466: loss 0.172782\n",
      "batch 1467: loss 0.432109\n",
      "batch 1468: loss 0.094425\n",
      "batch 1469: loss 0.239662\n",
      "batch 1470: loss 0.052826\n",
      "batch 1471: loss 0.182576\n",
      "batch 1472: loss 0.172104\n",
      "batch 1473: loss 0.156074\n",
      "batch 1474: loss 0.118034\n",
      "batch 1475: loss 0.322656\n",
      "batch 1476: loss 0.147700\n",
      "batch 1477: loss 0.078642\n",
      "batch 1478: loss 0.044792\n",
      "batch 1479: loss 0.180549\n",
      "batch 1480: loss 0.101135\n",
      "batch 1481: loss 0.188901\n",
      "batch 1482: loss 0.161947\n",
      "batch 1483: loss 0.086625\n",
      "batch 1484: loss 0.044722\n",
      "batch 1485: loss 0.428027\n",
      "batch 1486: loss 0.418992\n",
      "batch 1487: loss 0.155738\n",
      "batch 1488: loss 0.299255\n",
      "batch 1489: loss 0.288252\n",
      "batch 1490: loss 0.082452\n",
      "batch 1491: loss 0.117664\n",
      "batch 1492: loss 0.143527\n",
      "batch 1493: loss 0.240959\n",
      "batch 1494: loss 0.139194\n",
      "batch 1495: loss 0.130319\n",
      "batch 1496: loss 0.140975\n",
      "batch 1497: loss 0.117407\n",
      "batch 1498: loss 0.187050\n",
      "batch 1499: loss 0.234923\n",
      "batch 1500: loss 0.089460\n",
      "batch 1501: loss 0.090129\n",
      "batch 1502: loss 0.044720\n",
      "batch 1503: loss 0.309682\n",
      "batch 1504: loss 0.232435\n",
      "batch 1505: loss 0.179095\n",
      "batch 1506: loss 0.061709\n",
      "batch 1507: loss 0.050834\n",
      "batch 1508: loss 0.167689\n",
      "batch 1509: loss 0.091833\n",
      "batch 1510: loss 0.242176\n",
      "batch 1511: loss 0.237417\n",
      "batch 1512: loss 0.268708\n",
      "batch 1513: loss 0.127679\n",
      "batch 1514: loss 0.165640\n",
      "batch 1515: loss 0.121758\n",
      "batch 1516: loss 0.132644\n",
      "batch 1517: loss 0.176246\n",
      "batch 1518: loss 0.049254\n",
      "batch 1519: loss 0.130209\n",
      "batch 1520: loss 0.198295\n",
      "batch 1521: loss 0.175799\n",
      "batch 1522: loss 0.036362\n",
      "batch 1523: loss 0.167128\n",
      "batch 1524: loss 0.108241\n",
      "batch 1525: loss 0.308051\n",
      "batch 1526: loss 0.206869\n",
      "batch 1527: loss 0.215664\n",
      "batch 1528: loss 0.173752\n",
      "batch 1529: loss 0.204242\n",
      "batch 1530: loss 0.184316\n",
      "batch 1531: loss 0.239178\n",
      "batch 1532: loss 0.077688\n",
      "batch 1533: loss 0.092012\n",
      "batch 1534: loss 0.239015\n",
      "batch 1535: loss 0.095013\n",
      "batch 1536: loss 0.161188\n",
      "batch 1537: loss 0.146220\n",
      "batch 1538: loss 0.180938\n",
      "batch 1539: loss 0.086960\n",
      "batch 1540: loss 0.197261\n",
      "batch 1541: loss 0.152229\n",
      "batch 1542: loss 0.203552\n",
      "batch 1543: loss 0.123739\n",
      "batch 1544: loss 0.188859\n",
      "batch 1545: loss 0.167225\n",
      "batch 1546: loss 0.215253\n",
      "batch 1547: loss 0.280417\n",
      "batch 1548: loss 0.128963\n",
      "batch 1549: loss 0.124405\n",
      "batch 1550: loss 0.258717\n",
      "batch 1551: loss 0.245845\n",
      "batch 1552: loss 0.126908\n",
      "batch 1553: loss 0.111187\n",
      "batch 1554: loss 0.282497\n",
      "batch 1555: loss 0.156804\n",
      "batch 1556: loss 0.063397\n",
      "batch 1557: loss 0.170910\n",
      "batch 1558: loss 0.373807\n",
      "batch 1559: loss 0.341112\n",
      "batch 1560: loss 0.089861\n",
      "batch 1561: loss 0.136154\n",
      "batch 1562: loss 0.128930\n",
      "batch 1563: loss 0.101338\n",
      "batch 1564: loss 0.132678\n",
      "batch 1565: loss 0.071364\n",
      "batch 1566: loss 0.095889\n",
      "batch 1567: loss 0.212106\n",
      "batch 1568: loss 0.124851\n",
      "batch 1569: loss 0.072342\n",
      "batch 1570: loss 0.124533\n",
      "batch 1571: loss 0.138655\n",
      "batch 1572: loss 0.232931\n",
      "batch 1573: loss 0.213787\n",
      "batch 1574: loss 0.134424\n",
      "batch 1575: loss 0.338522\n",
      "batch 1576: loss 0.183648\n",
      "batch 1577: loss 0.087874\n",
      "batch 1578: loss 0.185633\n",
      "batch 1579: loss 0.036440\n",
      "batch 1580: loss 0.151089\n",
      "batch 1581: loss 0.146308\n",
      "batch 1582: loss 0.160340\n",
      "batch 1583: loss 0.158004\n",
      "batch 1584: loss 0.163937\n",
      "batch 1585: loss 0.236383\n",
      "batch 1586: loss 0.302699\n",
      "batch 1587: loss 0.139300\n",
      "batch 1588: loss 0.098836\n",
      "batch 1589: loss 0.381504\n",
      "batch 1590: loss 0.262981\n",
      "batch 1591: loss 0.431627\n",
      "batch 1592: loss 0.122284\n",
      "batch 1593: loss 0.060361\n",
      "batch 1594: loss 0.096586\n",
      "batch 1595: loss 0.310593\n",
      "batch 1596: loss 0.120638\n",
      "batch 1597: loss 0.246947\n",
      "batch 1598: loss 0.312159\n",
      "batch 1599: loss 0.130472\n",
      "batch 1600: loss 0.224967\n",
      "batch 1601: loss 0.121632\n",
      "batch 1602: loss 0.032558\n",
      "batch 1603: loss 0.106149\n",
      "batch 1604: loss 0.437608\n",
      "batch 1605: loss 0.247629\n",
      "batch 1606: loss 0.048911\n",
      "batch 1607: loss 0.206620\n",
      "batch 1608: loss 0.458535\n",
      "batch 1609: loss 0.278269\n",
      "batch 1610: loss 0.053279\n",
      "batch 1611: loss 0.297371\n",
      "batch 1612: loss 0.168112\n",
      "batch 1613: loss 0.123659\n",
      "batch 1614: loss 0.160421\n",
      "batch 1615: loss 0.136630\n",
      "batch 1616: loss 0.130759\n",
      "batch 1617: loss 0.160449\n",
      "batch 1618: loss 0.238186\n",
      "batch 1619: loss 0.118135\n",
      "batch 1620: loss 0.187783\n",
      "batch 1621: loss 0.152972\n",
      "batch 1622: loss 0.256977\n",
      "batch 1623: loss 0.081890\n",
      "batch 1624: loss 0.179480\n",
      "batch 1625: loss 0.108388\n",
      "batch 1626: loss 0.060979\n",
      "batch 1627: loss 0.123457\n",
      "batch 1628: loss 0.118233\n",
      "batch 1629: loss 0.128856\n",
      "batch 1630: loss 0.212809\n",
      "batch 1631: loss 0.159975\n",
      "batch 1632: loss 0.126430\n",
      "batch 1633: loss 0.113194\n",
      "batch 1634: loss 0.180742\n",
      "batch 1635: loss 0.054038\n",
      "batch 1636: loss 0.131298\n",
      "batch 1637: loss 0.181454\n",
      "batch 1638: loss 0.185050\n",
      "batch 1639: loss 0.114876\n",
      "batch 1640: loss 0.135890\n",
      "batch 1641: loss 0.142830\n",
      "batch 1642: loss 0.236967\n",
      "batch 1643: loss 0.098623\n",
      "batch 1644: loss 0.205537\n",
      "batch 1645: loss 0.131268\n",
      "batch 1646: loss 0.161922\n",
      "batch 1647: loss 0.386049\n",
      "batch 1648: loss 0.054114\n",
      "batch 1649: loss 0.126325\n",
      "batch 1650: loss 0.158645\n",
      "batch 1651: loss 0.207946\n",
      "batch 1652: loss 0.276639\n",
      "batch 1653: loss 0.194087\n",
      "batch 1654: loss 0.019153\n",
      "batch 1655: loss 0.131959\n",
      "batch 1656: loss 0.190457\n",
      "batch 1657: loss 0.132344\n",
      "batch 1658: loss 0.217971\n",
      "batch 1659: loss 0.112822\n",
      "batch 1660: loss 0.067799\n",
      "batch 1661: loss 0.089475\n",
      "batch 1662: loss 0.179536\n",
      "batch 1663: loss 0.124221\n",
      "batch 1664: loss 0.227501\n",
      "batch 1665: loss 0.034159\n",
      "batch 1666: loss 0.101624\n",
      "batch 1667: loss 0.041591\n",
      "batch 1668: loss 0.095739\n",
      "batch 1669: loss 0.060153\n",
      "batch 1670: loss 0.119771\n",
      "batch 1671: loss 0.064490\n",
      "batch 1672: loss 0.305449\n",
      "batch 1673: loss 0.205191\n",
      "batch 1674: loss 0.074134\n",
      "batch 1675: loss 0.354437\n",
      "batch 1676: loss 0.035375\n",
      "batch 1677: loss 0.433222\n",
      "batch 1678: loss 0.084635\n",
      "batch 1679: loss 0.019210\n",
      "batch 1680: loss 0.306755\n",
      "batch 1681: loss 0.169830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1682: loss 0.106059\n",
      "batch 1683: loss 0.155422\n",
      "batch 1684: loss 0.067914\n",
      "batch 1685: loss 0.032503\n",
      "batch 1686: loss 0.158514\n",
      "batch 1687: loss 0.086944\n",
      "batch 1688: loss 0.306649\n",
      "batch 1689: loss 0.075447\n",
      "batch 1690: loss 0.201285\n",
      "batch 1691: loss 0.074178\n",
      "batch 1692: loss 0.276848\n",
      "batch 1693: loss 0.165014\n",
      "batch 1694: loss 0.153902\n",
      "batch 1695: loss 0.096460\n",
      "batch 1696: loss 0.076109\n",
      "batch 1697: loss 0.156154\n",
      "batch 1698: loss 0.041903\n",
      "batch 1699: loss 0.133861\n",
      "batch 1700: loss 0.129685\n",
      "batch 1701: loss 0.061693\n",
      "batch 1702: loss 0.296134\n",
      "batch 1703: loss 0.241687\n",
      "batch 1704: loss 0.063006\n",
      "batch 1705: loss 0.100702\n",
      "batch 1706: loss 0.125319\n",
      "batch 1707: loss 0.118888\n",
      "batch 1708: loss 0.227933\n",
      "batch 1709: loss 0.207318\n",
      "batch 1710: loss 0.133666\n",
      "batch 1711: loss 0.102067\n",
      "batch 1712: loss 0.209704\n",
      "batch 1713: loss 0.211222\n",
      "batch 1714: loss 0.388598\n",
      "batch 1715: loss 0.169505\n",
      "batch 1716: loss 0.217117\n",
      "batch 1717: loss 0.116716\n",
      "batch 1718: loss 0.053240\n",
      "batch 1719: loss 0.161757\n",
      "batch 1720: loss 0.137353\n",
      "batch 1721: loss 0.136022\n",
      "batch 1722: loss 0.040188\n",
      "batch 1723: loss 0.195915\n",
      "batch 1724: loss 0.170263\n",
      "batch 1725: loss 0.118501\n",
      "batch 1726: loss 0.165511\n",
      "batch 1727: loss 0.200512\n",
      "batch 1728: loss 0.144080\n",
      "batch 1729: loss 0.178182\n",
      "batch 1730: loss 0.029227\n",
      "batch 1731: loss 0.204972\n",
      "batch 1732: loss 0.105135\n",
      "batch 1733: loss 0.107329\n",
      "batch 1734: loss 0.092562\n",
      "batch 1735: loss 0.118062\n",
      "batch 1736: loss 0.039234\n",
      "batch 1737: loss 0.241780\n",
      "batch 1738: loss 0.154009\n",
      "batch 1739: loss 0.064197\n",
      "batch 1740: loss 0.110598\n",
      "batch 1741: loss 0.079587\n",
      "batch 1742: loss 0.168898\n",
      "batch 1743: loss 0.323373\n",
      "batch 1744: loss 0.175002\n",
      "batch 1745: loss 0.263865\n",
      "batch 1746: loss 0.062828\n",
      "batch 1747: loss 0.105274\n",
      "batch 1748: loss 0.194131\n",
      "batch 1749: loss 0.104450\n",
      "batch 1750: loss 0.129403\n",
      "batch 1751: loss 0.094584\n",
      "batch 1752: loss 0.146178\n",
      "batch 1753: loss 0.075945\n",
      "batch 1754: loss 0.172268\n",
      "batch 1755: loss 0.159556\n",
      "batch 1756: loss 0.116873\n",
      "batch 1757: loss 0.318693\n",
      "batch 1758: loss 0.135276\n",
      "batch 1759: loss 0.178478\n",
      "batch 1760: loss 0.150179\n",
      "batch 1761: loss 0.184800\n",
      "batch 1762: loss 0.034751\n",
      "batch 1763: loss 0.171770\n",
      "batch 1764: loss 0.131132\n",
      "batch 1765: loss 0.082514\n",
      "batch 1766: loss 0.092145\n",
      "batch 1767: loss 0.222117\n",
      "batch 1768: loss 0.092270\n",
      "batch 1769: loss 0.294421\n",
      "batch 1770: loss 0.143469\n",
      "batch 1771: loss 0.134697\n",
      "batch 1772: loss 0.105776\n",
      "batch 1773: loss 0.039707\n",
      "batch 1774: loss 0.241527\n",
      "batch 1775: loss 0.110503\n",
      "batch 1776: loss 0.334139\n",
      "batch 1777: loss 0.077760\n",
      "batch 1778: loss 0.137651\n",
      "batch 1779: loss 0.152659\n",
      "batch 1780: loss 0.169951\n",
      "batch 1781: loss 0.185780\n",
      "batch 1782: loss 0.086208\n",
      "batch 1783: loss 0.076607\n",
      "batch 1784: loss 0.182118\n",
      "batch 1785: loss 0.093660\n",
      "batch 1786: loss 0.027672\n",
      "batch 1787: loss 0.138550\n",
      "batch 1788: loss 0.070178\n",
      "batch 1789: loss 0.142659\n",
      "batch 1790: loss 0.147431\n",
      "batch 1791: loss 0.059175\n",
      "batch 1792: loss 0.438846\n",
      "batch 1793: loss 0.067491\n",
      "batch 1794: loss 0.054708\n",
      "batch 1795: loss 0.170932\n",
      "batch 1796: loss 0.046378\n",
      "batch 1797: loss 0.089001\n",
      "batch 1798: loss 0.168617\n",
      "batch 1799: loss 0.046071\n",
      "batch 1800: loss 0.178514\n",
      "batch 1801: loss 0.086470\n",
      "batch 1802: loss 0.161269\n",
      "batch 1803: loss 0.136116\n",
      "batch 1804: loss 0.144344\n",
      "batch 1805: loss 0.095237\n",
      "batch 1806: loss 0.206980\n",
      "batch 1807: loss 0.113042\n",
      "batch 1808: loss 0.037230\n",
      "batch 1809: loss 0.081623\n",
      "batch 1810: loss 0.071287\n",
      "batch 1811: loss 0.099177\n",
      "batch 1812: loss 0.156909\n",
      "batch 1813: loss 0.175929\n",
      "batch 1814: loss 0.282301\n",
      "batch 1815: loss 0.066006\n",
      "batch 1816: loss 0.103852\n",
      "batch 1817: loss 0.105012\n",
      "batch 1818: loss 0.070074\n",
      "batch 1819: loss 0.034294\n",
      "batch 1820: loss 0.124688\n",
      "batch 1821: loss 0.084167\n",
      "batch 1822: loss 0.060868\n",
      "batch 1823: loss 0.282972\n",
      "batch 1824: loss 0.081630\n",
      "batch 1825: loss 0.110105\n",
      "batch 1826: loss 0.095386\n",
      "batch 1827: loss 0.220395\n",
      "batch 1828: loss 0.266558\n",
      "batch 1829: loss 0.170741\n",
      "batch 1830: loss 0.282022\n",
      "batch 1831: loss 0.056620\n",
      "batch 1832: loss 0.052745\n",
      "batch 1833: loss 0.150099\n",
      "batch 1834: loss 0.289733\n",
      "batch 1835: loss 0.114157\n",
      "batch 1836: loss 0.057605\n",
      "batch 1837: loss 0.163755\n",
      "batch 1838: loss 0.060634\n",
      "batch 1839: loss 0.133760\n",
      "batch 1840: loss 0.100255\n",
      "batch 1841: loss 0.183408\n",
      "batch 1842: loss 0.304773\n",
      "batch 1843: loss 0.041163\n",
      "batch 1844: loss 0.101562\n",
      "batch 1845: loss 0.084983\n",
      "batch 1846: loss 0.085235\n",
      "batch 1847: loss 0.169430\n",
      "batch 1848: loss 0.167204\n",
      "batch 1849: loss 0.108800\n",
      "batch 1850: loss 0.367074\n",
      "batch 1851: loss 0.023396\n",
      "batch 1852: loss 0.232236\n",
      "batch 1853: loss 0.110857\n",
      "batch 1854: loss 0.106601\n",
      "batch 1855: loss 0.152182\n",
      "batch 1856: loss 0.284091\n",
      "batch 1857: loss 0.094160\n",
      "batch 1858: loss 0.093816\n",
      "batch 1859: loss 0.032789\n",
      "batch 1860: loss 0.117457\n",
      "batch 1861: loss 0.135952\n",
      "batch 1862: loss 0.175445\n",
      "batch 1863: loss 0.203814\n",
      "batch 1864: loss 0.166361\n",
      "batch 1865: loss 0.062682\n",
      "batch 1866: loss 0.087961\n",
      "batch 1867: loss 0.059703\n",
      "batch 1868: loss 0.241714\n",
      "batch 1869: loss 0.254680\n",
      "batch 1870: loss 0.144297\n",
      "batch 1871: loss 0.185715\n",
      "batch 1872: loss 0.069448\n",
      "batch 1873: loss 0.220537\n",
      "batch 1874: loss 0.107990\n",
      "batch 1875: loss 0.103094\n",
      "batch 1876: loss 0.092159\n",
      "batch 1877: loss 0.291544\n",
      "batch 1878: loss 0.109289\n",
      "batch 1879: loss 0.223263\n",
      "batch 1880: loss 0.193374\n",
      "batch 1881: loss 0.289075\n",
      "batch 1882: loss 0.271519\n",
      "batch 1883: loss 0.041585\n",
      "batch 1884: loss 0.082776\n",
      "batch 1885: loss 0.168207\n",
      "batch 1886: loss 0.203080\n",
      "batch 1887: loss 0.063518\n",
      "batch 1888: loss 0.205123\n",
      "batch 1889: loss 0.061355\n",
      "batch 1890: loss 0.318020\n",
      "batch 1891: loss 0.154343\n",
      "batch 1892: loss 0.286531\n",
      "batch 1893: loss 0.234577\n",
      "batch 1894: loss 0.255239\n",
      "batch 1895: loss 0.277132\n",
      "batch 1896: loss 0.341593\n",
      "batch 1897: loss 0.214455\n",
      "batch 1898: loss 0.046440\n",
      "batch 1899: loss 0.178055\n",
      "batch 1900: loss 0.265998\n",
      "batch 1901: loss 0.179038\n",
      "batch 1902: loss 0.170665\n",
      "batch 1903: loss 0.141241\n",
      "batch 1904: loss 0.269265\n",
      "batch 1905: loss 0.216773\n",
      "batch 1906: loss 0.030724\n",
      "batch 1907: loss 0.125004\n",
      "batch 1908: loss 0.126389\n",
      "batch 1909: loss 0.117579\n",
      "batch 1910: loss 0.263094\n",
      "batch 1911: loss 0.235150\n",
      "batch 1912: loss 0.403565\n",
      "batch 1913: loss 0.063352\n",
      "batch 1914: loss 0.058333\n",
      "batch 1915: loss 0.108904\n",
      "batch 1916: loss 0.107693\n",
      "batch 1917: loss 0.189453\n",
      "batch 1918: loss 0.097386\n",
      "batch 1919: loss 0.069515\n",
      "batch 1920: loss 0.257608\n",
      "batch 1921: loss 0.203386\n",
      "batch 1922: loss 0.239037\n",
      "batch 1923: loss 0.102886\n",
      "batch 1924: loss 0.085118\n",
      "batch 1925: loss 0.251283\n",
      "batch 1926: loss 0.238117\n",
      "batch 1927: loss 0.195349\n",
      "batch 1928: loss 0.125055\n",
      "batch 1929: loss 0.077073\n",
      "batch 1930: loss 0.097444\n",
      "batch 1931: loss 0.186816\n",
      "batch 1932: loss 0.186572\n",
      "batch 1933: loss 0.075794\n",
      "batch 1934: loss 0.109418\n",
      "batch 1935: loss 0.181763\n",
      "batch 1936: loss 0.092923\n",
      "batch 1937: loss 0.292667\n",
      "batch 1938: loss 0.043584\n",
      "batch 1939: loss 0.242275\n",
      "batch 1940: loss 0.155857\n",
      "batch 1941: loss 0.146529\n",
      "batch 1942: loss 0.154061\n",
      "batch 1943: loss 0.180292\n",
      "batch 1944: loss 0.047473\n",
      "batch 1945: loss 0.164746\n",
      "batch 1946: loss 0.208124\n",
      "batch 1947: loss 0.088159\n",
      "batch 1948: loss 0.028496\n",
      "batch 1949: loss 0.156133\n",
      "batch 1950: loss 0.066153\n",
      "batch 1951: loss 0.021716\n",
      "batch 1952: loss 0.250274\n",
      "batch 1953: loss 0.486778\n",
      "batch 1954: loss 0.066473\n",
      "batch 1955: loss 0.087023\n",
      "batch 1956: loss 0.115190\n",
      "batch 1957: loss 0.144139\n",
      "batch 1958: loss 0.188758\n",
      "batch 1959: loss 0.105301\n",
      "batch 1960: loss 0.177065\n",
      "batch 1961: loss 0.129741\n",
      "batch 1962: loss 0.102203\n",
      "batch 1963: loss 0.200608\n",
      "batch 1964: loss 0.151715\n",
      "batch 1965: loss 0.059571\n",
      "batch 1966: loss 0.043812\n",
      "batch 1967: loss 0.118674\n",
      "batch 1968: loss 0.205324\n",
      "batch 1969: loss 0.161954\n",
      "batch 1970: loss 0.097690\n",
      "batch 1971: loss 0.082087\n",
      "batch 1972: loss 0.072910\n",
      "batch 1973: loss 0.047854\n",
      "batch 1974: loss 0.062699\n",
      "batch 1975: loss 0.130528\n",
      "batch 1976: loss 0.177635\n",
      "batch 1977: loss 0.085215\n",
      "batch 1978: loss 0.210050\n",
      "batch 1979: loss 0.349615\n",
      "batch 1980: loss 0.077759\n",
      "batch 1981: loss 0.072162\n",
      "batch 1982: loss 0.194283\n",
      "batch 1983: loss 0.125277\n",
      "batch 1984: loss 0.224501\n",
      "batch 1985: loss 0.077513\n",
      "batch 1986: loss 0.132762\n",
      "batch 1987: loss 0.222213\n",
      "batch 1988: loss 0.072570\n",
      "batch 1989: loss 0.094457\n",
      "batch 1990: loss 0.225292\n",
      "batch 1991: loss 0.218588\n",
      "batch 1992: loss 0.119509\n",
      "batch 1993: loss 0.165401\n",
      "batch 1994: loss 0.063305\n",
      "batch 1995: loss 0.169790\n",
      "batch 1996: loss 0.421706\n",
      "batch 1997: loss 0.122506\n",
      "batch 1998: loss 0.052391\n",
      "batch 1999: loss 0.100605\n",
      "batch 2000: loss 0.130748\n",
      "batch 2001: loss 0.073363\n",
      "batch 2002: loss 0.119494\n",
      "batch 2003: loss 0.107880\n",
      "batch 2004: loss 0.207757\n",
      "batch 2005: loss 0.037099\n",
      "batch 2006: loss 0.226715\n",
      "batch 2007: loss 0.142050\n",
      "batch 2008: loss 0.051969\n",
      "batch 2009: loss 0.109197\n",
      "batch 2010: loss 0.340483\n",
      "batch 2011: loss 0.066631\n",
      "batch 2012: loss 0.076098\n",
      "batch 2013: loss 0.246123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2014: loss 0.096140\n",
      "batch 2015: loss 0.185638\n",
      "batch 2016: loss 0.028842\n",
      "batch 2017: loss 0.095127\n",
      "batch 2018: loss 0.086261\n",
      "batch 2019: loss 0.164219\n",
      "batch 2020: loss 0.121721\n",
      "batch 2021: loss 0.084937\n",
      "batch 2022: loss 0.055990\n",
      "batch 2023: loss 0.106583\n",
      "batch 2024: loss 0.239944\n",
      "batch 2025: loss 0.153963\n",
      "batch 2026: loss 0.186301\n",
      "batch 2027: loss 0.090254\n",
      "batch 2028: loss 0.068991\n",
      "batch 2029: loss 0.065619\n",
      "batch 2030: loss 0.549623\n",
      "batch 2031: loss 0.236523\n",
      "batch 2032: loss 0.114059\n",
      "batch 2033: loss 0.109789\n",
      "batch 2034: loss 0.128600\n",
      "batch 2035: loss 0.469855\n",
      "batch 2036: loss 0.066020\n",
      "batch 2037: loss 0.104970\n",
      "batch 2038: loss 0.242552\n",
      "batch 2039: loss 0.206974\n",
      "batch 2040: loss 0.294311\n",
      "batch 2041: loss 0.110053\n",
      "batch 2042: loss 0.211607\n",
      "batch 2043: loss 0.316714\n",
      "batch 2044: loss 0.099908\n",
      "batch 2045: loss 0.127750\n",
      "batch 2046: loss 0.109405\n",
      "batch 2047: loss 0.101389\n",
      "batch 2048: loss 0.204154\n",
      "batch 2049: loss 0.085742\n",
      "batch 2050: loss 0.138670\n",
      "batch 2051: loss 0.041038\n",
      "batch 2052: loss 0.192473\n",
      "batch 2053: loss 0.222330\n",
      "batch 2054: loss 0.064379\n",
      "batch 2055: loss 0.090034\n",
      "batch 2056: loss 0.274695\n",
      "batch 2057: loss 0.041492\n",
      "batch 2058: loss 0.214194\n",
      "batch 2059: loss 0.356675\n",
      "batch 2060: loss 0.203750\n",
      "batch 2061: loss 0.234371\n",
      "batch 2062: loss 0.061918\n",
      "batch 2063: loss 0.163329\n",
      "batch 2064: loss 0.109234\n",
      "batch 2065: loss 0.243038\n",
      "batch 2066: loss 0.245051\n",
      "batch 2067: loss 0.161975\n",
      "batch 2068: loss 0.068652\n",
      "batch 2069: loss 0.141078\n",
      "batch 2070: loss 0.187227\n",
      "batch 2071: loss 0.052209\n",
      "batch 2072: loss 0.142455\n",
      "batch 2073: loss 0.073579\n",
      "batch 2074: loss 0.280355\n",
      "batch 2075: loss 0.249177\n",
      "batch 2076: loss 0.198122\n",
      "batch 2077: loss 0.069201\n",
      "batch 2078: loss 0.168328\n",
      "batch 2079: loss 0.108607\n",
      "batch 2080: loss 0.202005\n",
      "batch 2081: loss 0.245591\n",
      "batch 2082: loss 0.188140\n",
      "batch 2083: loss 0.038044\n",
      "batch 2084: loss 0.095130\n",
      "batch 2085: loss 0.366515\n",
      "batch 2086: loss 0.207957\n",
      "batch 2087: loss 0.120622\n",
      "batch 2088: loss 0.164113\n",
      "batch 2089: loss 0.119775\n",
      "batch 2090: loss 0.098731\n",
      "batch 2091: loss 0.107524\n",
      "batch 2092: loss 0.179935\n",
      "batch 2093: loss 0.055736\n",
      "batch 2094: loss 0.069671\n",
      "batch 2095: loss 0.317531\n",
      "batch 2096: loss 0.240272\n",
      "batch 2097: loss 0.362962\n",
      "batch 2098: loss 0.272469\n",
      "batch 2099: loss 0.107070\n",
      "batch 2100: loss 0.064273\n",
      "batch 2101: loss 0.126581\n",
      "batch 2102: loss 0.203843\n",
      "batch 2103: loss 0.104385\n",
      "batch 2104: loss 0.145178\n",
      "batch 2105: loss 0.374739\n",
      "batch 2106: loss 0.136620\n",
      "batch 2107: loss 0.141302\n",
      "batch 2108: loss 0.226412\n",
      "batch 2109: loss 0.051714\n",
      "batch 2110: loss 0.054448\n",
      "batch 2111: loss 0.325656\n",
      "batch 2112: loss 0.285884\n",
      "batch 2113: loss 0.141762\n",
      "batch 2114: loss 0.151739\n",
      "batch 2115: loss 0.176543\n",
      "batch 2116: loss 0.071018\n",
      "batch 2117: loss 0.108928\n",
      "batch 2118: loss 0.203873\n",
      "batch 2119: loss 0.156519\n",
      "batch 2120: loss 0.133616\n",
      "batch 2121: loss 0.155712\n",
      "batch 2122: loss 0.266656\n",
      "batch 2123: loss 0.153654\n",
      "batch 2124: loss 0.059511\n",
      "batch 2125: loss 0.223132\n",
      "batch 2126: loss 0.215558\n",
      "batch 2127: loss 0.033072\n",
      "batch 2128: loss 0.194371\n",
      "batch 2129: loss 0.029005\n",
      "batch 2130: loss 0.077396\n",
      "batch 2131: loss 0.146878\n",
      "batch 2132: loss 0.080699\n",
      "batch 2133: loss 0.071080\n",
      "batch 2134: loss 0.055941\n",
      "batch 2135: loss 0.122244\n",
      "batch 2136: loss 0.226772\n",
      "batch 2137: loss 0.102802\n",
      "batch 2138: loss 0.169345\n",
      "batch 2139: loss 0.169141\n",
      "batch 2140: loss 0.155211\n",
      "batch 2141: loss 0.220131\n",
      "batch 2142: loss 0.107705\n",
      "batch 2143: loss 0.059909\n",
      "batch 2144: loss 0.327121\n",
      "batch 2145: loss 0.079085\n",
      "batch 2146: loss 0.117684\n",
      "batch 2147: loss 0.128230\n",
      "batch 2148: loss 0.029588\n",
      "batch 2149: loss 0.155293\n",
      "batch 2150: loss 0.171470\n",
      "batch 2151: loss 0.137633\n",
      "batch 2152: loss 0.038995\n",
      "batch 2153: loss 0.245976\n",
      "batch 2154: loss 0.245712\n",
      "batch 2155: loss 0.121914\n",
      "batch 2156: loss 0.155091\n",
      "batch 2157: loss 0.078252\n",
      "batch 2158: loss 0.230122\n",
      "batch 2159: loss 0.190889\n",
      "batch 2160: loss 0.193793\n",
      "batch 2161: loss 0.083334\n",
      "batch 2162: loss 0.152014\n",
      "batch 2163: loss 0.028662\n",
      "batch 2164: loss 0.213128\n",
      "batch 2165: loss 0.143577\n",
      "batch 2166: loss 0.161381\n",
      "batch 2167: loss 0.107970\n",
      "batch 2168: loss 0.059453\n",
      "batch 2169: loss 0.038346\n",
      "batch 2170: loss 0.137508\n",
      "batch 2171: loss 0.081891\n",
      "batch 2172: loss 0.068791\n",
      "batch 2173: loss 0.206760\n",
      "batch 2174: loss 0.142671\n",
      "batch 2175: loss 0.214241\n",
      "batch 2176: loss 0.116600\n",
      "batch 2177: loss 0.043154\n",
      "batch 2178: loss 0.112715\n",
      "batch 2179: loss 0.244340\n",
      "batch 2180: loss 0.208120\n",
      "batch 2181: loss 0.145052\n",
      "batch 2182: loss 0.207922\n",
      "batch 2183: loss 0.294038\n",
      "batch 2184: loss 0.139319\n",
      "batch 2185: loss 0.042475\n",
      "batch 2186: loss 0.102975\n",
      "batch 2187: loss 0.048506\n",
      "batch 2188: loss 0.136966\n",
      "batch 2189: loss 0.055987\n",
      "batch 2190: loss 0.039176\n",
      "batch 2191: loss 0.163585\n",
      "batch 2192: loss 0.077512\n",
      "batch 2193: loss 0.030561\n",
      "batch 2194: loss 0.165523\n",
      "batch 2195: loss 0.228078\n",
      "batch 2196: loss 0.165255\n",
      "batch 2197: loss 0.170357\n",
      "batch 2198: loss 0.119844\n",
      "batch 2199: loss 0.384740\n",
      "batch 2200: loss 0.221780\n",
      "batch 2201: loss 0.191650\n",
      "batch 2202: loss 0.090503\n",
      "batch 2203: loss 0.154215\n",
      "batch 2204: loss 0.123053\n",
      "batch 2205: loss 0.203769\n",
      "batch 2206: loss 0.121191\n",
      "batch 2207: loss 0.029546\n",
      "batch 2208: loss 0.218846\n",
      "batch 2209: loss 0.199163\n",
      "batch 2210: loss 0.064632\n",
      "batch 2211: loss 0.148486\n",
      "batch 2212: loss 0.064145\n",
      "batch 2213: loss 0.107590\n",
      "batch 2214: loss 0.133418\n",
      "batch 2215: loss 0.075424\n",
      "batch 2216: loss 0.302240\n",
      "batch 2217: loss 0.038815\n",
      "batch 2218: loss 0.063231\n",
      "batch 2219: loss 0.121666\n",
      "batch 2220: loss 0.138840\n",
      "batch 2221: loss 0.239610\n",
      "batch 2222: loss 0.056520\n",
      "batch 2223: loss 0.078798\n",
      "batch 2224: loss 0.111290\n",
      "batch 2225: loss 0.055498\n",
      "batch 2226: loss 0.093620\n",
      "batch 2227: loss 0.117036\n",
      "batch 2228: loss 0.299450\n",
      "batch 2229: loss 0.088279\n",
      "batch 2230: loss 0.040506\n",
      "batch 2231: loss 0.109727\n",
      "batch 2232: loss 0.075489\n",
      "batch 2233: loss 0.124349\n",
      "batch 2234: loss 0.102861\n",
      "batch 2235: loss 0.024167\n",
      "batch 2236: loss 0.052698\n",
      "batch 2237: loss 0.307328\n",
      "batch 2238: loss 0.151045\n",
      "batch 2239: loss 0.121344\n",
      "batch 2240: loss 0.165712\n",
      "batch 2241: loss 0.190587\n",
      "batch 2242: loss 0.305492\n",
      "batch 2243: loss 0.069387\n",
      "batch 2244: loss 0.094266\n",
      "batch 2245: loss 0.365433\n",
      "batch 2246: loss 0.181788\n",
      "batch 2247: loss 0.211353\n",
      "batch 2248: loss 0.061302\n",
      "batch 2249: loss 0.303414\n",
      "batch 2250: loss 0.091136\n",
      "batch 2251: loss 0.159091\n",
      "batch 2252: loss 0.302565\n",
      "batch 2253: loss 0.079074\n",
      "batch 2254: loss 0.123147\n",
      "batch 2255: loss 0.266003\n",
      "batch 2256: loss 0.167237\n",
      "batch 2257: loss 0.022762\n",
      "batch 2258: loss 0.054935\n",
      "batch 2259: loss 0.150160\n",
      "batch 2260: loss 0.119219\n",
      "batch 2261: loss 0.059570\n",
      "batch 2262: loss 0.098357\n",
      "batch 2263: loss 0.106867\n",
      "batch 2264: loss 0.128255\n",
      "batch 2265: loss 0.035765\n",
      "batch 2266: loss 0.102165\n",
      "batch 2267: loss 0.174330\n",
      "batch 2268: loss 0.073478\n",
      "batch 2269: loss 0.128963\n",
      "batch 2270: loss 0.127644\n",
      "batch 2271: loss 0.306936\n",
      "batch 2272: loss 0.237588\n",
      "batch 2273: loss 0.162343\n",
      "batch 2274: loss 0.091184\n",
      "batch 2275: loss 0.347560\n",
      "batch 2276: loss 0.083277\n",
      "batch 2277: loss 0.063536\n",
      "batch 2278: loss 0.061411\n",
      "batch 2279: loss 0.100646\n",
      "batch 2280: loss 0.087266\n",
      "batch 2281: loss 0.118522\n",
      "batch 2282: loss 0.087088\n",
      "batch 2283: loss 0.121095\n",
      "batch 2284: loss 0.090581\n",
      "batch 2285: loss 0.108137\n",
      "batch 2286: loss 0.139702\n",
      "batch 2287: loss 0.038257\n",
      "batch 2288: loss 0.190966\n",
      "batch 2289: loss 0.104466\n",
      "batch 2290: loss 0.274623\n",
      "batch 2291: loss 0.118644\n",
      "batch 2292: loss 0.202576\n",
      "batch 2293: loss 0.153062\n",
      "batch 2294: loss 0.056287\n",
      "batch 2295: loss 0.299164\n",
      "batch 2296: loss 0.043138\n",
      "batch 2297: loss 0.120357\n",
      "batch 2298: loss 0.135615\n",
      "batch 2299: loss 0.156438\n",
      "batch 2300: loss 0.066379\n",
      "batch 2301: loss 0.038737\n",
      "batch 2302: loss 0.129273\n",
      "batch 2303: loss 0.081594\n",
      "batch 2304: loss 0.044172\n",
      "batch 2305: loss 0.096838\n",
      "batch 2306: loss 0.035627\n",
      "batch 2307: loss 0.144351\n",
      "batch 2308: loss 0.205383\n",
      "batch 2309: loss 0.045488\n",
      "batch 2310: loss 0.161862\n",
      "batch 2311: loss 0.161956\n",
      "batch 2312: loss 0.084397\n",
      "batch 2313: loss 0.219345\n",
      "batch 2314: loss 0.265921\n",
      "batch 2315: loss 0.040856\n",
      "batch 2316: loss 0.328472\n",
      "batch 2317: loss 0.076260\n",
      "batch 2318: loss 0.106205\n",
      "batch 2319: loss 0.043617\n",
      "batch 2320: loss 0.163539\n",
      "batch 2321: loss 0.096094\n",
      "batch 2322: loss 0.055586\n",
      "batch 2323: loss 0.197111\n",
      "batch 2324: loss 0.180959\n",
      "batch 2325: loss 0.043277\n",
      "batch 2326: loss 0.068074\n",
      "batch 2327: loss 0.145304\n",
      "batch 2328: loss 0.014819\n",
      "batch 2329: loss 0.210584\n",
      "batch 2330: loss 0.069632\n",
      "batch 2331: loss 0.054142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2332: loss 0.163957\n",
      "batch 2333: loss 0.158243\n",
      "batch 2334: loss 0.150856\n",
      "batch 2335: loss 0.209184\n",
      "batch 2336: loss 0.100295\n",
      "batch 2337: loss 0.228212\n",
      "batch 2338: loss 0.121776\n",
      "batch 2339: loss 0.082591\n",
      "batch 2340: loss 0.220396\n",
      "batch 2341: loss 0.057209\n",
      "batch 2342: loss 0.268248\n",
      "batch 2343: loss 0.140721\n",
      "batch 2344: loss 0.365746\n",
      "batch 2345: loss 0.043458\n",
      "batch 2346: loss 0.121836\n",
      "batch 2347: loss 0.150188\n",
      "batch 2348: loss 0.081043\n",
      "batch 2349: loss 0.195613\n",
      "batch 2350: loss 0.148859\n",
      "batch 2351: loss 0.114283\n",
      "batch 2352: loss 0.045785\n",
      "batch 2353: loss 0.146081\n",
      "batch 2354: loss 0.326705\n",
      "batch 2355: loss 0.077907\n",
      "batch 2356: loss 0.020569\n",
      "batch 2357: loss 0.046695\n",
      "batch 2358: loss 0.121854\n",
      "batch 2359: loss 0.159874\n",
      "batch 2360: loss 0.087219\n",
      "batch 2361: loss 0.108566\n",
      "batch 2362: loss 0.143380\n",
      "batch 2363: loss 0.295614\n",
      "batch 2364: loss 0.172474\n",
      "batch 2365: loss 0.198445\n",
      "batch 2366: loss 0.101348\n",
      "batch 2367: loss 0.182972\n",
      "batch 2368: loss 0.110609\n",
      "batch 2369: loss 0.096703\n",
      "batch 2370: loss 0.065305\n",
      "batch 2371: loss 0.195406\n",
      "batch 2372: loss 0.138430\n",
      "batch 2373: loss 0.033875\n",
      "batch 2374: loss 0.144832\n",
      "batch 2375: loss 0.130664\n",
      "batch 2376: loss 0.297757\n",
      "batch 2377: loss 0.454742\n",
      "batch 2378: loss 0.091282\n",
      "batch 2379: loss 0.154572\n",
      "batch 2380: loss 0.052232\n",
      "batch 2381: loss 0.172325\n",
      "batch 2382: loss 0.375576\n",
      "batch 2383: loss 0.083276\n",
      "batch 2384: loss 0.047010\n",
      "batch 2385: loss 0.080360\n",
      "batch 2386: loss 0.068295\n",
      "batch 2387: loss 0.068500\n",
      "batch 2388: loss 0.151744\n",
      "batch 2389: loss 0.089849\n",
      "batch 2390: loss 0.034057\n",
      "batch 2391: loss 0.237036\n",
      "batch 2392: loss 0.181906\n",
      "batch 2393: loss 0.113628\n",
      "batch 2394: loss 0.054800\n",
      "batch 2395: loss 0.099051\n",
      "batch 2396: loss 0.049597\n",
      "batch 2397: loss 0.203276\n",
      "batch 2398: loss 0.153622\n",
      "batch 2399: loss 0.033337\n",
      "batch 2400: loss 0.263715\n",
      "batch 2401: loss 0.053036\n",
      "batch 2402: loss 0.023383\n",
      "batch 2403: loss 0.044615\n",
      "batch 2404: loss 0.075249\n",
      "batch 2405: loss 0.053120\n",
      "batch 2406: loss 0.114789\n",
      "batch 2407: loss 0.077123\n",
      "batch 2408: loss 0.215533\n",
      "batch 2409: loss 0.217385\n",
      "batch 2410: loss 0.240312\n",
      "batch 2411: loss 0.052038\n",
      "batch 2412: loss 0.085371\n",
      "batch 2413: loss 0.090651\n",
      "batch 2414: loss 0.410461\n",
      "batch 2415: loss 0.137332\n",
      "batch 2416: loss 0.342640\n",
      "batch 2417: loss 0.142069\n",
      "batch 2418: loss 0.079768\n",
      "batch 2419: loss 0.174745\n",
      "batch 2420: loss 0.264685\n",
      "batch 2421: loss 0.217334\n",
      "batch 2422: loss 0.203703\n",
      "batch 2423: loss 0.086662\n",
      "batch 2424: loss 0.057813\n",
      "batch 2425: loss 0.067924\n",
      "batch 2426: loss 0.277233\n",
      "batch 2427: loss 0.157413\n",
      "batch 2428: loss 0.052186\n",
      "batch 2429: loss 0.128077\n",
      "batch 2430: loss 0.174014\n",
      "batch 2431: loss 0.176721\n",
      "batch 2432: loss 0.306380\n",
      "batch 2433: loss 0.121441\n",
      "batch 2434: loss 0.109285\n",
      "batch 2435: loss 0.252151\n",
      "batch 2436: loss 0.098429\n",
      "batch 2437: loss 0.171444\n",
      "batch 2438: loss 0.267427\n",
      "batch 2439: loss 0.132979\n",
      "batch 2440: loss 0.199281\n",
      "batch 2441: loss 0.130192\n",
      "batch 2442: loss 0.079288\n",
      "batch 2443: loss 0.186599\n",
      "batch 2444: loss 0.053198\n",
      "batch 2445: loss 0.151027\n",
      "batch 2446: loss 0.123275\n",
      "batch 2447: loss 0.134667\n",
      "batch 2448: loss 0.335027\n",
      "batch 2449: loss 0.117353\n",
      "batch 2450: loss 0.059147\n",
      "batch 2451: loss 0.184991\n",
      "batch 2452: loss 0.164965\n",
      "batch 2453: loss 0.086758\n",
      "batch 2454: loss 0.078544\n",
      "batch 2455: loss 0.057943\n",
      "batch 2456: loss 0.047004\n",
      "batch 2457: loss 0.043074\n",
      "batch 2458: loss 0.050150\n",
      "batch 2459: loss 0.095840\n",
      "batch 2460: loss 0.155526\n",
      "batch 2461: loss 0.080933\n",
      "batch 2462: loss 0.074044\n",
      "batch 2463: loss 0.304462\n",
      "batch 2464: loss 0.159694\n",
      "batch 2465: loss 0.185817\n",
      "batch 2466: loss 0.073973\n",
      "batch 2467: loss 0.195039\n",
      "batch 2468: loss 0.121288\n",
      "batch 2469: loss 0.098053\n",
      "batch 2470: loss 0.049657\n",
      "batch 2471: loss 0.103246\n",
      "batch 2472: loss 0.041286\n",
      "batch 2473: loss 0.291736\n",
      "batch 2474: loss 0.206117\n",
      "batch 2475: loss 0.137707\n",
      "batch 2476: loss 0.071796\n",
      "batch 2477: loss 0.055445\n",
      "batch 2478: loss 0.172650\n",
      "batch 2479: loss 0.140832\n",
      "batch 2480: loss 0.135195\n",
      "batch 2481: loss 0.062571\n",
      "batch 2482: loss 0.113375\n",
      "batch 2483: loss 0.108446\n",
      "batch 2484: loss 0.181621\n",
      "batch 2485: loss 0.157113\n",
      "batch 2486: loss 0.235678\n",
      "batch 2487: loss 0.064270\n",
      "batch 2488: loss 0.111368\n",
      "batch 2489: loss 0.117699\n",
      "batch 2490: loss 0.069619\n",
      "batch 2491: loss 0.021675\n",
      "batch 2492: loss 0.067762\n",
      "batch 2493: loss 0.104744\n",
      "batch 2494: loss 0.137844\n",
      "batch 2495: loss 0.140463\n",
      "batch 2496: loss 0.046875\n",
      "batch 2497: loss 0.206608\n",
      "batch 2498: loss 0.118068\n",
      "batch 2499: loss 0.254922\n",
      "batch 2500: loss 0.027055\n",
      "batch 2501: loss 0.141309\n",
      "batch 2502: loss 0.084500\n",
      "batch 2503: loss 0.203989\n",
      "batch 2504: loss 0.183323\n",
      "batch 2505: loss 0.121360\n",
      "batch 2506: loss 0.162001\n",
      "batch 2507: loss 0.082122\n",
      "batch 2508: loss 0.353625\n",
      "batch 2509: loss 0.020034\n",
      "batch 2510: loss 0.101812\n",
      "batch 2511: loss 0.221618\n",
      "batch 2512: loss 0.130816\n",
      "batch 2513: loss 0.054813\n",
      "batch 2514: loss 0.086201\n",
      "batch 2515: loss 0.269502\n",
      "batch 2516: loss 0.162062\n",
      "batch 2517: loss 0.068277\n",
      "batch 2518: loss 0.057102\n",
      "batch 2519: loss 0.044788\n",
      "batch 2520: loss 0.082273\n",
      "batch 2521: loss 0.077056\n",
      "batch 2522: loss 0.092070\n",
      "batch 2523: loss 0.241747\n",
      "batch 2524: loss 0.169436\n",
      "batch 2525: loss 0.083360\n",
      "batch 2526: loss 0.115012\n",
      "batch 2527: loss 0.129508\n",
      "batch 2528: loss 0.251418\n",
      "batch 2529: loss 0.050387\n",
      "batch 2530: loss 0.070194\n",
      "batch 2531: loss 0.236604\n",
      "batch 2532: loss 0.049130\n",
      "batch 2533: loss 0.069401\n",
      "batch 2534: loss 0.238339\n",
      "batch 2535: loss 0.021733\n",
      "batch 2536: loss 0.110173\n",
      "batch 2537: loss 0.155459\n",
      "batch 2538: loss 0.147575\n",
      "batch 2539: loss 0.039865\n",
      "batch 2540: loss 0.145473\n",
      "batch 2541: loss 0.123227\n",
      "batch 2542: loss 0.065476\n",
      "batch 2543: loss 0.077562\n",
      "batch 2544: loss 0.180440\n",
      "batch 2545: loss 0.053779\n",
      "batch 2546: loss 0.086630\n",
      "batch 2547: loss 0.108752\n",
      "batch 2548: loss 0.122524\n",
      "batch 2549: loss 0.171461\n",
      "batch 2550: loss 0.070038\n",
      "batch 2551: loss 0.058386\n",
      "batch 2552: loss 0.188034\n",
      "batch 2553: loss 0.234005\n",
      "batch 2554: loss 0.057578\n",
      "batch 2555: loss 0.096357\n",
      "batch 2556: loss 0.366114\n",
      "batch 2557: loss 0.093917\n",
      "batch 2558: loss 0.100691\n",
      "batch 2559: loss 0.111881\n",
      "batch 2560: loss 0.040929\n",
      "batch 2561: loss 0.063958\n",
      "batch 2562: loss 0.077277\n",
      "batch 2563: loss 0.220156\n",
      "batch 2564: loss 0.091750\n",
      "batch 2565: loss 0.126406\n",
      "batch 2566: loss 0.059988\n",
      "batch 2567: loss 0.126438\n",
      "batch 2568: loss 0.046587\n",
      "batch 2569: loss 0.278028\n",
      "batch 2570: loss 0.026279\n",
      "batch 2571: loss 0.066869\n",
      "batch 2572: loss 0.020863\n",
      "batch 2573: loss 0.326468\n",
      "batch 2574: loss 0.204702\n",
      "batch 2575: loss 0.176934\n",
      "batch 2576: loss 0.128209\n",
      "batch 2577: loss 0.112994\n",
      "batch 2578: loss 0.183492\n",
      "batch 2579: loss 0.151406\n",
      "batch 2580: loss 0.185576\n",
      "batch 2581: loss 0.067674\n",
      "batch 2582: loss 0.034533\n",
      "batch 2583: loss 0.062901\n",
      "batch 2584: loss 0.143003\n",
      "batch 2585: loss 0.085009\n",
      "batch 2586: loss 0.264130\n",
      "batch 2587: loss 0.067744\n",
      "batch 2588: loss 0.074446\n",
      "batch 2589: loss 0.044678\n",
      "batch 2590: loss 0.049816\n",
      "batch 2591: loss 0.140457\n",
      "batch 2592: loss 0.173370\n",
      "batch 2593: loss 0.106639\n",
      "batch 2594: loss 0.129216\n",
      "batch 2595: loss 0.070735\n",
      "batch 2596: loss 0.041338\n",
      "batch 2597: loss 0.039921\n",
      "batch 2598: loss 0.132199\n",
      "batch 2599: loss 0.054798\n",
      "batch 2600: loss 0.041685\n",
      "batch 2601: loss 0.171096\n",
      "batch 2602: loss 0.177439\n",
      "batch 2603: loss 0.019984\n",
      "batch 2604: loss 0.246819\n",
      "batch 2605: loss 0.255348\n",
      "batch 2606: loss 0.059580\n",
      "batch 2607: loss 0.175209\n",
      "batch 2608: loss 0.265106\n",
      "batch 2609: loss 0.047900\n",
      "batch 2610: loss 0.076946\n",
      "batch 2611: loss 0.094258\n",
      "batch 2612: loss 0.087793\n",
      "batch 2613: loss 0.160709\n",
      "batch 2614: loss 0.092844\n",
      "batch 2615: loss 0.021442\n",
      "batch 2616: loss 0.098737\n",
      "batch 2617: loss 0.066787\n",
      "batch 2618: loss 0.177557\n",
      "batch 2619: loss 0.103565\n",
      "batch 2620: loss 0.091764\n",
      "batch 2621: loss 0.078079\n",
      "batch 2622: loss 0.095127\n",
      "batch 2623: loss 0.037731\n",
      "batch 2624: loss 0.057780\n",
      "batch 2625: loss 0.110595\n",
      "batch 2626: loss 0.046811\n",
      "batch 2627: loss 0.138054\n",
      "batch 2628: loss 0.040351\n",
      "batch 2629: loss 0.065417\n",
      "batch 2630: loss 0.104511\n",
      "batch 2631: loss 0.035660\n",
      "batch 2632: loss 0.126517\n",
      "batch 2633: loss 0.072870\n",
      "batch 2634: loss 0.152963\n",
      "batch 2635: loss 0.150291\n",
      "batch 2636: loss 0.145946\n",
      "batch 2637: loss 0.159381\n",
      "batch 2638: loss 0.305129\n",
      "batch 2639: loss 0.201382\n",
      "batch 2640: loss 0.014152\n",
      "batch 2641: loss 0.098169\n",
      "batch 2642: loss 0.225875\n",
      "batch 2643: loss 0.218982\n",
      "batch 2644: loss 0.183995\n",
      "batch 2645: loss 0.071866\n",
      "batch 2646: loss 0.124206\n",
      "batch 2647: loss 0.066765\n",
      "batch 2648: loss 0.096581\n",
      "batch 2649: loss 0.079857\n",
      "batch 2650: loss 0.053635\n",
      "batch 2651: loss 0.079821\n",
      "batch 2652: loss 0.117359\n",
      "batch 2653: loss 0.124973\n",
      "batch 2654: loss 0.107034\n",
      "batch 2655: loss 0.049903\n",
      "batch 2656: loss 0.120301\n",
      "batch 2657: loss 0.080316\n",
      "batch 2658: loss 0.331993\n",
      "batch 2659: loss 0.094559\n",
      "batch 2660: loss 0.075544\n",
      "batch 2661: loss 0.097779\n",
      "batch 2662: loss 0.108413\n",
      "batch 2663: loss 0.145118\n",
      "batch 2664: loss 0.125317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2665: loss 0.149532\n",
      "batch 2666: loss 0.316872\n",
      "batch 2667: loss 0.066117\n",
      "batch 2668: loss 0.222993\n",
      "batch 2669: loss 0.029289\n",
      "batch 2670: loss 0.037607\n",
      "batch 2671: loss 0.088181\n",
      "batch 2672: loss 0.120809\n",
      "batch 2673: loss 0.163700\n",
      "batch 2674: loss 0.073215\n",
      "batch 2675: loss 0.210029\n",
      "batch 2676: loss 0.221294\n",
      "batch 2677: loss 0.158358\n",
      "batch 2678: loss 0.035218\n",
      "batch 2679: loss 0.231932\n",
      "batch 2680: loss 0.117060\n",
      "batch 2681: loss 0.028824\n",
      "batch 2682: loss 0.045635\n",
      "batch 2683: loss 0.161583\n",
      "batch 2684: loss 0.092493\n",
      "batch 2685: loss 0.273877\n",
      "batch 2686: loss 0.053876\n",
      "batch 2687: loss 0.057645\n",
      "batch 2688: loss 0.044212\n",
      "batch 2689: loss 0.118741\n",
      "batch 2690: loss 0.059352\n",
      "batch 2691: loss 0.049184\n",
      "batch 2692: loss 0.024428\n",
      "batch 2693: loss 0.121416\n",
      "batch 2694: loss 0.210165\n",
      "batch 2695: loss 0.105119\n",
      "batch 2696: loss 0.095316\n",
      "batch 2697: loss 0.111112\n",
      "batch 2698: loss 0.088085\n",
      "batch 2699: loss 0.048601\n",
      "batch 2700: loss 0.104102\n",
      "batch 2701: loss 0.098021\n",
      "batch 2702: loss 0.052528\n",
      "batch 2703: loss 0.180958\n",
      "batch 2704: loss 0.175048\n",
      "batch 2705: loss 0.319691\n",
      "batch 2706: loss 0.195573\n",
      "batch 2707: loss 0.033279\n",
      "batch 2708: loss 0.199776\n",
      "batch 2709: loss 0.196273\n",
      "batch 2710: loss 0.075382\n",
      "batch 2711: loss 0.110904\n",
      "batch 2712: loss 0.182474\n",
      "batch 2713: loss 0.040378\n",
      "batch 2714: loss 0.020294\n",
      "batch 2715: loss 0.044037\n",
      "batch 2716: loss 0.106910\n",
      "batch 2717: loss 0.049853\n",
      "batch 2718: loss 0.129899\n",
      "batch 2719: loss 0.169301\n",
      "batch 2720: loss 0.050869\n",
      "batch 2721: loss 0.049018\n",
      "batch 2722: loss 0.021751\n",
      "batch 2723: loss 0.066576\n",
      "batch 2724: loss 0.029076\n",
      "batch 2725: loss 0.208054\n",
      "batch 2726: loss 0.058122\n",
      "batch 2727: loss 0.019381\n",
      "batch 2728: loss 0.027330\n",
      "batch 2729: loss 0.322415\n",
      "batch 2730: loss 0.061678\n",
      "batch 2731: loss 0.175745\n",
      "batch 2732: loss 0.081721\n",
      "batch 2733: loss 0.181605\n",
      "batch 2734: loss 0.044665\n",
      "batch 2735: loss 0.145696\n",
      "batch 2736: loss 0.220279\n",
      "batch 2737: loss 0.033705\n",
      "batch 2738: loss 0.046927\n",
      "batch 2739: loss 0.187570\n",
      "batch 2740: loss 0.325208\n",
      "batch 2741: loss 0.052732\n",
      "batch 2742: loss 0.088585\n",
      "batch 2743: loss 0.134305\n",
      "batch 2744: loss 0.137310\n",
      "batch 2745: loss 0.046942\n",
      "batch 2746: loss 0.284498\n",
      "batch 2747: loss 0.106338\n",
      "batch 2748: loss 0.043113\n",
      "batch 2749: loss 0.130708\n",
      "batch 2750: loss 0.046757\n",
      "batch 2751: loss 0.066200\n",
      "batch 2752: loss 0.091986\n",
      "batch 2753: loss 0.079533\n",
      "batch 2754: loss 0.126228\n",
      "batch 2755: loss 0.030893\n",
      "batch 2756: loss 0.046813\n",
      "batch 2757: loss 0.118344\n",
      "batch 2758: loss 0.061591\n",
      "batch 2759: loss 0.169500\n",
      "batch 2760: loss 0.014551\n",
      "batch 2761: loss 0.246138\n",
      "batch 2762: loss 0.026168\n",
      "batch 2763: loss 0.109323\n",
      "batch 2764: loss 0.135111\n",
      "batch 2765: loss 0.114710\n",
      "batch 2766: loss 0.121065\n",
      "batch 2767: loss 0.030838\n",
      "batch 2768: loss 0.114771\n",
      "batch 2769: loss 0.051236\n",
      "batch 2770: loss 0.087053\n",
      "batch 2771: loss 0.073555\n",
      "batch 2772: loss 0.464979\n",
      "batch 2773: loss 0.119837\n",
      "batch 2774: loss 0.132607\n",
      "batch 2775: loss 0.017989\n",
      "batch 2776: loss 0.057855\n",
      "batch 2777: loss 0.132674\n",
      "batch 2778: loss 0.011778\n",
      "batch 2779: loss 0.079064\n",
      "batch 2780: loss 0.070059\n",
      "batch 2781: loss 0.220784\n",
      "batch 2782: loss 0.052362\n",
      "batch 2783: loss 0.054937\n",
      "batch 2784: loss 0.125660\n",
      "batch 2785: loss 0.282643\n",
      "batch 2786: loss 0.116972\n",
      "batch 2787: loss 0.268318\n",
      "batch 2788: loss 0.064213\n",
      "batch 2789: loss 0.244819\n",
      "batch 2790: loss 0.090514\n",
      "batch 2791: loss 0.056558\n",
      "batch 2792: loss 0.233897\n",
      "batch 2793: loss 0.105784\n",
      "batch 2794: loss 0.162888\n",
      "batch 2795: loss 0.153663\n",
      "batch 2796: loss 0.066444\n",
      "batch 2797: loss 0.089056\n",
      "batch 2798: loss 0.062429\n",
      "batch 2799: loss 0.036063\n",
      "batch 2800: loss 0.158070\n",
      "batch 2801: loss 0.056677\n",
      "batch 2802: loss 0.372155\n",
      "batch 2803: loss 0.144172\n",
      "batch 2804: loss 0.049428\n",
      "batch 2805: loss 0.179230\n",
      "batch 2806: loss 0.118758\n",
      "batch 2807: loss 0.217331\n",
      "batch 2808: loss 0.081980\n",
      "batch 2809: loss 0.101916\n",
      "batch 2810: loss 0.061836\n",
      "batch 2811: loss 0.063641\n",
      "batch 2812: loss 0.168966\n",
      "batch 2813: loss 0.045467\n",
      "batch 2814: loss 0.084140\n",
      "batch 2815: loss 0.086343\n",
      "batch 2816: loss 0.097618\n",
      "batch 2817: loss 0.157363\n",
      "batch 2818: loss 0.133094\n",
      "batch 2819: loss 0.094815\n",
      "batch 2820: loss 0.141648\n",
      "batch 2821: loss 0.220858\n",
      "batch 2822: loss 0.118245\n",
      "batch 2823: loss 0.158421\n",
      "batch 2824: loss 0.135027\n",
      "batch 2825: loss 0.253836\n",
      "batch 2826: loss 0.149323\n",
      "batch 2827: loss 0.029061\n",
      "batch 2828: loss 0.055912\n",
      "batch 2829: loss 0.103945\n",
      "batch 2830: loss 0.014184\n",
      "batch 2831: loss 0.070315\n",
      "batch 2832: loss 0.194088\n",
      "batch 2833: loss 0.038823\n",
      "batch 2834: loss 0.203794\n",
      "batch 2835: loss 0.088264\n",
      "batch 2836: loss 0.157274\n",
      "batch 2837: loss 0.137558\n",
      "batch 2838: loss 0.034289\n",
      "batch 2839: loss 0.382191\n",
      "batch 2840: loss 0.177456\n",
      "batch 2841: loss 0.166700\n",
      "batch 2842: loss 0.019001\n",
      "batch 2843: loss 0.204886\n",
      "batch 2844: loss 0.122335\n",
      "batch 2845: loss 0.097868\n",
      "batch 2846: loss 0.073635\n",
      "batch 2847: loss 0.012437\n",
      "batch 2848: loss 0.205879\n",
      "batch 2849: loss 0.163173\n",
      "batch 2850: loss 0.045335\n",
      "batch 2851: loss 0.124447\n",
      "batch 2852: loss 0.085369\n",
      "batch 2853: loss 0.018049\n",
      "batch 2854: loss 0.118989\n",
      "batch 2855: loss 0.092922\n",
      "batch 2856: loss 0.096292\n",
      "batch 2857: loss 0.075728\n",
      "batch 2858: loss 0.055356\n",
      "batch 2859: loss 0.342320\n",
      "batch 2860: loss 0.061213\n",
      "batch 2861: loss 0.138961\n",
      "batch 2862: loss 0.137401\n",
      "batch 2863: loss 0.180277\n",
      "batch 2864: loss 0.127052\n",
      "batch 2865: loss 0.057775\n",
      "batch 2866: loss 0.019261\n",
      "batch 2867: loss 0.113587\n",
      "batch 2868: loss 0.059996\n",
      "batch 2869: loss 0.077545\n",
      "batch 2870: loss 0.064299\n",
      "batch 2871: loss 0.054573\n",
      "batch 2872: loss 0.052988\n",
      "batch 2873: loss 0.087566\n",
      "batch 2874: loss 0.053467\n",
      "batch 2875: loss 0.032309\n",
      "batch 2876: loss 0.171271\n",
      "batch 2877: loss 0.090642\n",
      "batch 2878: loss 0.042405\n",
      "batch 2879: loss 0.050408\n",
      "batch 2880: loss 0.241624\n",
      "batch 2881: loss 0.042055\n",
      "batch 2882: loss 0.059908\n",
      "batch 2883: loss 0.197612\n",
      "batch 2884: loss 0.084505\n",
      "batch 2885: loss 0.026820\n",
      "batch 2886: loss 0.112193\n",
      "batch 2887: loss 0.132507\n",
      "batch 2888: loss 0.162665\n",
      "batch 2889: loss 0.047197\n",
      "batch 2890: loss 0.161572\n",
      "batch 2891: loss 0.209088\n",
      "batch 2892: loss 0.280934\n",
      "batch 2893: loss 0.187733\n",
      "batch 2894: loss 0.086613\n",
      "batch 2895: loss 0.104473\n",
      "batch 2896: loss 0.053323\n",
      "batch 2897: loss 0.160585\n",
      "batch 2898: loss 0.110958\n",
      "batch 2899: loss 0.097326\n",
      "batch 2900: loss 0.094344\n",
      "batch 2901: loss 0.066227\n",
      "batch 2902: loss 0.164427\n",
      "batch 2903: loss 0.042056\n",
      "batch 2904: loss 0.078532\n",
      "batch 2905: loss 0.045028\n",
      "batch 2906: loss 0.064129\n",
      "batch 2907: loss 0.086362\n",
      "batch 2908: loss 0.031091\n",
      "batch 2909: loss 0.050463\n",
      "batch 2910: loss 0.057570\n",
      "batch 2911: loss 0.253733\n",
      "batch 2912: loss 0.065563\n",
      "batch 2913: loss 0.063939\n",
      "batch 2914: loss 0.158431\n",
      "batch 2915: loss 0.170518\n",
      "batch 2916: loss 0.078515\n",
      "batch 2917: loss 0.046957\n",
      "batch 2918: loss 0.063168\n",
      "batch 2919: loss 0.042950\n",
      "batch 2920: loss 0.080112\n",
      "batch 2921: loss 0.022815\n",
      "batch 2922: loss 0.069733\n",
      "batch 2923: loss 0.331446\n",
      "batch 2924: loss 0.053624\n",
      "batch 2925: loss 0.168315\n",
      "batch 2926: loss 0.101881\n",
      "batch 2927: loss 0.109427\n",
      "batch 2928: loss 0.040383\n",
      "batch 2929: loss 0.095528\n",
      "batch 2930: loss 0.071926\n",
      "batch 2931: loss 0.053624\n",
      "batch 2932: loss 0.096308\n",
      "batch 2933: loss 0.112484\n",
      "batch 2934: loss 0.071481\n",
      "batch 2935: loss 0.052714\n",
      "batch 2936: loss 0.118080\n",
      "batch 2937: loss 0.085271\n",
      "batch 2938: loss 0.066071\n",
      "batch 2939: loss 0.036776\n",
      "batch 2940: loss 0.149238\n",
      "batch 2941: loss 0.201975\n",
      "batch 2942: loss 0.141186\n",
      "batch 2943: loss 0.192145\n",
      "batch 2944: loss 0.090014\n",
      "batch 2945: loss 0.037353\n",
      "batch 2946: loss 0.128045\n",
      "batch 2947: loss 0.202320\n",
      "batch 2948: loss 0.029539\n",
      "batch 2949: loss 0.048402\n",
      "batch 2950: loss 0.063058\n",
      "batch 2951: loss 0.221399\n",
      "batch 2952: loss 0.125511\n",
      "batch 2953: loss 0.153289\n",
      "batch 2954: loss 0.057588\n",
      "batch 2955: loss 0.022356\n",
      "batch 2956: loss 0.246436\n",
      "batch 2957: loss 0.179370\n",
      "batch 2958: loss 0.087188\n",
      "batch 2959: loss 0.096084\n",
      "batch 2960: loss 0.019631\n",
      "batch 2961: loss 0.085523\n",
      "batch 2962: loss 0.214176\n",
      "batch 2963: loss 0.370900\n",
      "batch 2964: loss 0.197560\n",
      "batch 2965: loss 0.116677\n",
      "batch 2966: loss 0.044529\n",
      "batch 2967: loss 0.057299\n",
      "batch 2968: loss 0.043776\n",
      "batch 2969: loss 0.016886\n",
      "batch 2970: loss 0.027134\n",
      "batch 2971: loss 0.125645\n",
      "batch 2972: loss 0.061757\n",
      "batch 2973: loss 0.131274\n",
      "batch 2974: loss 0.129615\n",
      "batch 2975: loss 0.143597\n",
      "batch 2976: loss 0.228992\n",
      "batch 2977: loss 0.049861\n",
      "batch 2978: loss 0.076832\n",
      "batch 2979: loss 0.143196\n",
      "batch 2980: loss 0.046500\n",
      "batch 2981: loss 0.049241\n",
      "batch 2982: loss 0.025211\n",
      "batch 2983: loss 0.136767\n",
      "batch 2984: loss 0.104554\n",
      "batch 2985: loss 0.085536\n",
      "batch 2986: loss 0.168879\n",
      "batch 2987: loss 0.015222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2988: loss 0.059145\n",
      "batch 2989: loss 0.388073\n",
      "batch 2990: loss 0.166376\n",
      "batch 2991: loss 0.104573\n",
      "batch 2992: loss 0.082548\n",
      "batch 2993: loss 0.209527\n",
      "batch 2994: loss 0.117618\n",
      "batch 2995: loss 0.260501\n",
      "batch 2996: loss 0.209530\n",
      "batch 2997: loss 0.167641\n",
      "batch 2998: loss 0.280068\n",
      "batch 2999: loss 0.025032\n",
      "batch 3000: loss 0.090712\n",
      "batch 3001: loss 0.203622\n",
      "batch 3002: loss 0.204772\n",
      "batch 3003: loss 0.186089\n",
      "batch 3004: loss 0.119651\n",
      "batch 3005: loss 0.146869\n",
      "batch 3006: loss 0.256083\n",
      "batch 3007: loss 0.096084\n",
      "batch 3008: loss 0.142876\n",
      "batch 3009: loss 0.042550\n",
      "batch 3010: loss 0.144292\n",
      "batch 3011: loss 0.125737\n",
      "batch 3012: loss 0.064198\n",
      "batch 3013: loss 0.201699\n",
      "batch 3014: loss 0.073864\n",
      "batch 3015: loss 0.131641\n",
      "batch 3016: loss 0.048231\n",
      "batch 3017: loss 0.055752\n",
      "batch 3018: loss 0.145460\n",
      "batch 3019: loss 0.042168\n",
      "batch 3020: loss 0.148286\n",
      "batch 3021: loss 0.116558\n",
      "batch 3022: loss 0.087036\n",
      "batch 3023: loss 0.111479\n",
      "batch 3024: loss 0.071641\n",
      "batch 3025: loss 0.094811\n",
      "batch 3026: loss 0.029687\n",
      "batch 3027: loss 0.147258\n",
      "batch 3028: loss 0.077280\n",
      "batch 3029: loss 0.088198\n",
      "batch 3030: loss 0.089712\n",
      "batch 3031: loss 0.060014\n",
      "batch 3032: loss 0.091096\n",
      "batch 3033: loss 0.075378\n",
      "batch 3034: loss 0.085028\n",
      "batch 3035: loss 0.107006\n",
      "batch 3036: loss 0.057545\n",
      "batch 3037: loss 0.131007\n",
      "batch 3038: loss 0.243524\n",
      "batch 3039: loss 0.120861\n",
      "batch 3040: loss 0.073310\n",
      "batch 3041: loss 0.172109\n",
      "batch 3042: loss 0.070413\n",
      "batch 3043: loss 0.069659\n",
      "batch 3044: loss 0.039636\n",
      "batch 3045: loss 0.031300\n",
      "batch 3046: loss 0.034135\n",
      "batch 3047: loss 0.094537\n",
      "batch 3048: loss 0.084720\n",
      "batch 3049: loss 0.039839\n",
      "batch 3050: loss 0.182164\n",
      "batch 3051: loss 0.054246\n",
      "batch 3052: loss 0.050814\n",
      "batch 3053: loss 0.119672\n",
      "batch 3054: loss 0.183197\n",
      "batch 3055: loss 0.173605\n",
      "batch 3056: loss 0.321920\n",
      "batch 3057: loss 0.123760\n",
      "batch 3058: loss 0.225229\n",
      "batch 3059: loss 0.091421\n",
      "batch 3060: loss 0.278747\n",
      "batch 3061: loss 0.113282\n",
      "batch 3062: loss 0.154849\n",
      "batch 3063: loss 0.136834\n",
      "batch 3064: loss 0.043883\n",
      "batch 3065: loss 0.039438\n",
      "batch 3066: loss 0.014533\n",
      "batch 3067: loss 0.197681\n",
      "batch 3068: loss 0.315519\n",
      "batch 3069: loss 0.148014\n",
      "batch 3070: loss 0.142831\n",
      "batch 3071: loss 0.025141\n",
      "batch 3072: loss 0.121425\n",
      "batch 3073: loss 0.033238\n",
      "batch 3074: loss 0.100901\n",
      "batch 3075: loss 0.054272\n",
      "batch 3076: loss 0.174053\n",
      "batch 3077: loss 0.102680\n",
      "batch 3078: loss 0.255074\n",
      "batch 3079: loss 0.134191\n",
      "batch 3080: loss 0.107786\n",
      "batch 3081: loss 0.115404\n",
      "batch 3082: loss 0.144561\n",
      "batch 3083: loss 0.273335\n",
      "batch 3084: loss 0.014921\n",
      "batch 3085: loss 0.199169\n",
      "batch 3086: loss 0.063803\n",
      "batch 3087: loss 0.025481\n",
      "batch 3088: loss 0.129712\n",
      "batch 3089: loss 0.055712\n",
      "batch 3090: loss 0.054345\n",
      "batch 3091: loss 0.079245\n",
      "batch 3092: loss 0.020025\n",
      "batch 3093: loss 0.069885\n",
      "batch 3094: loss 0.051992\n",
      "batch 3095: loss 0.058279\n",
      "batch 3096: loss 0.140235\n",
      "batch 3097: loss 0.016858\n",
      "batch 3098: loss 0.142468\n",
      "batch 3099: loss 0.069668\n",
      "batch 3100: loss 0.085482\n",
      "batch 3101: loss 0.203808\n",
      "batch 3102: loss 0.144540\n",
      "batch 3103: loss 0.060584\n",
      "batch 3104: loss 0.099378\n",
      "batch 3105: loss 0.057200\n",
      "batch 3106: loss 0.112246\n",
      "batch 3107: loss 0.049184\n",
      "batch 3108: loss 0.053997\n",
      "batch 3109: loss 0.136432\n",
      "batch 3110: loss 0.046729\n",
      "batch 3111: loss 0.023107\n",
      "batch 3112: loss 0.055739\n",
      "batch 3113: loss 0.281500\n",
      "batch 3114: loss 0.049642\n",
      "batch 3115: loss 0.117881\n",
      "batch 3116: loss 0.089955\n",
      "batch 3117: loss 0.212466\n",
      "batch 3118: loss 0.080561\n",
      "batch 3119: loss 0.087652\n",
      "batch 3120: loss 0.071464\n",
      "batch 3121: loss 0.061099\n",
      "batch 3122: loss 0.057385\n",
      "batch 3123: loss 0.129561\n",
      "batch 3124: loss 0.029981\n",
      "batch 3125: loss 0.198633\n",
      "batch 3126: loss 0.242674\n",
      "batch 3127: loss 0.070190\n",
      "batch 3128: loss 0.051047\n",
      "batch 3129: loss 0.069222\n",
      "batch 3130: loss 0.141240\n",
      "batch 3131: loss 0.189486\n",
      "batch 3132: loss 0.042855\n",
      "batch 3133: loss 0.169576\n",
      "batch 3134: loss 0.100406\n",
      "batch 3135: loss 0.016166\n",
      "batch 3136: loss 0.068105\n",
      "batch 3137: loss 0.099052\n",
      "batch 3138: loss 0.130602\n",
      "batch 3139: loss 0.065459\n",
      "batch 3140: loss 0.047173\n",
      "batch 3141: loss 0.156139\n",
      "batch 3142: loss 0.049600\n",
      "batch 3143: loss 0.043262\n",
      "batch 3144: loss 0.082167\n",
      "batch 3145: loss 0.354129\n",
      "batch 3146: loss 0.106781\n",
      "batch 3147: loss 0.111065\n",
      "batch 3148: loss 0.063581\n",
      "batch 3149: loss 0.180858\n",
      "batch 3150: loss 0.105335\n",
      "batch 3151: loss 0.050459\n",
      "batch 3152: loss 0.087909\n",
      "batch 3153: loss 0.019683\n",
      "batch 3154: loss 0.102126\n",
      "batch 3155: loss 0.132456\n",
      "batch 3156: loss 0.089573\n",
      "batch 3157: loss 0.039832\n",
      "batch 3158: loss 0.198200\n",
      "batch 3159: loss 0.022232\n",
      "batch 3160: loss 0.028175\n",
      "batch 3161: loss 0.053180\n",
      "batch 3162: loss 0.247564\n",
      "batch 3163: loss 0.116746\n",
      "batch 3164: loss 0.019785\n",
      "batch 3165: loss 0.178154\n",
      "batch 3166: loss 0.104488\n",
      "batch 3167: loss 0.114323\n",
      "batch 3168: loss 0.108190\n",
      "batch 3169: loss 0.086976\n",
      "batch 3170: loss 0.043817\n",
      "batch 3171: loss 0.035244\n",
      "batch 3172: loss 0.070595\n",
      "batch 3173: loss 0.066489\n",
      "batch 3174: loss 0.056686\n",
      "batch 3175: loss 0.192265\n",
      "batch 3176: loss 0.028667\n",
      "batch 3177: loss 0.059754\n",
      "batch 3178: loss 0.086769\n",
      "batch 3179: loss 0.013240\n",
      "batch 3180: loss 0.160997\n",
      "batch 3181: loss 0.050312\n",
      "batch 3182: loss 0.151500\n",
      "batch 3183: loss 0.116047\n",
      "batch 3184: loss 0.075901\n",
      "batch 3185: loss 0.070979\n",
      "batch 3186: loss 0.037251\n",
      "batch 3187: loss 0.115556\n",
      "batch 3188: loss 0.054200\n",
      "batch 3189: loss 0.047545\n",
      "batch 3190: loss 0.076389\n",
      "batch 3191: loss 0.260901\n",
      "batch 3192: loss 0.038737\n",
      "batch 3193: loss 0.148719\n",
      "batch 3194: loss 0.108850\n",
      "batch 3195: loss 0.060285\n",
      "batch 3196: loss 0.160386\n",
      "batch 3197: loss 0.061341\n",
      "batch 3198: loss 0.241008\n",
      "batch 3199: loss 0.145070\n",
      "batch 3200: loss 0.242922\n",
      "batch 3201: loss 0.134496\n",
      "batch 3202: loss 0.028432\n",
      "batch 3203: loss 0.029920\n",
      "batch 3204: loss 0.397053\n",
      "batch 3205: loss 0.149981\n",
      "batch 3206: loss 0.193505\n",
      "batch 3207: loss 0.081706\n",
      "batch 3208: loss 0.226497\n",
      "batch 3209: loss 0.046541\n",
      "batch 3210: loss 0.095869\n",
      "batch 3211: loss 0.135938\n",
      "batch 3212: loss 0.123772\n",
      "batch 3213: loss 0.079436\n",
      "batch 3214: loss 0.068711\n",
      "batch 3215: loss 0.078377\n",
      "batch 3216: loss 0.027092\n",
      "batch 3217: loss 0.216432\n",
      "batch 3218: loss 0.048123\n",
      "batch 3219: loss 0.077590\n",
      "batch 3220: loss 0.056936\n",
      "batch 3221: loss 0.112447\n",
      "batch 3222: loss 0.075578\n",
      "batch 3223: loss 0.151871\n",
      "batch 3224: loss 0.125868\n",
      "batch 3225: loss 0.057475\n",
      "batch 3226: loss 0.144193\n",
      "batch 3227: loss 0.017787\n",
      "batch 3228: loss 0.012470\n",
      "batch 3229: loss 0.031341\n",
      "batch 3230: loss 0.096218\n",
      "batch 3231: loss 0.172326\n",
      "batch 3232: loss 0.065625\n",
      "batch 3233: loss 0.250715\n",
      "batch 3234: loss 0.039231\n",
      "batch 3235: loss 0.153919\n",
      "batch 3236: loss 0.113170\n",
      "batch 3237: loss 0.133950\n",
      "batch 3238: loss 0.041624\n",
      "batch 3239: loss 0.027276\n",
      "batch 3240: loss 0.226074\n",
      "batch 3241: loss 0.101344\n",
      "batch 3242: loss 0.209746\n",
      "batch 3243: loss 0.081298\n",
      "batch 3244: loss 0.089177\n",
      "batch 3245: loss 0.082452\n",
      "batch 3246: loss 0.057355\n",
      "batch 3247: loss 0.053319\n",
      "batch 3248: loss 0.080311\n",
      "batch 3249: loss 0.114752\n",
      "batch 3250: loss 0.055025\n",
      "batch 3251: loss 0.077669\n",
      "batch 3252: loss 0.009092\n",
      "batch 3253: loss 0.131057\n",
      "batch 3254: loss 0.076433\n",
      "batch 3255: loss 0.070640\n",
      "batch 3256: loss 0.080333\n",
      "batch 3257: loss 0.099635\n",
      "batch 3258: loss 0.048895\n",
      "batch 3259: loss 0.078531\n",
      "batch 3260: loss 0.065376\n",
      "batch 3261: loss 0.032325\n",
      "batch 3262: loss 0.079042\n",
      "batch 3263: loss 0.057438\n",
      "batch 3264: loss 0.095786\n",
      "batch 3265: loss 0.216608\n",
      "batch 3266: loss 0.106643\n",
      "batch 3267: loss 0.008890\n",
      "batch 3268: loss 0.030531\n",
      "batch 3269: loss 0.108256\n",
      "batch 3270: loss 0.111286\n",
      "batch 3271: loss 0.137062\n",
      "batch 3272: loss 0.150293\n",
      "batch 3273: loss 0.274070\n",
      "batch 3274: loss 0.197958\n",
      "batch 3275: loss 0.096909\n",
      "batch 3276: loss 0.050056\n",
      "batch 3277: loss 0.008088\n",
      "batch 3278: loss 0.012420\n",
      "batch 3279: loss 0.043297\n",
      "batch 3280: loss 0.027750\n",
      "batch 3281: loss 0.200793\n",
      "batch 3282: loss 0.085719\n",
      "batch 3283: loss 0.177513\n",
      "batch 3284: loss 0.049613\n",
      "batch 3285: loss 0.070686\n",
      "batch 3286: loss 0.083601\n",
      "batch 3287: loss 0.084806\n",
      "batch 3288: loss 0.092971\n",
      "batch 3289: loss 0.089017\n",
      "batch 3290: loss 0.068546\n",
      "batch 3291: loss 0.151061\n",
      "batch 3292: loss 0.137595\n",
      "batch 3293: loss 0.057523\n",
      "batch 3294: loss 0.076998\n",
      "batch 3295: loss 0.119687\n",
      "batch 3296: loss 0.024946\n",
      "batch 3297: loss 0.122335\n",
      "batch 3298: loss 0.145721\n",
      "batch 3299: loss 0.101376\n",
      "batch 3300: loss 0.077414\n",
      "batch 3301: loss 0.086933\n",
      "batch 3302: loss 0.057025\n",
      "batch 3303: loss 0.133116\n",
      "batch 3304: loss 0.142728\n",
      "batch 3305: loss 0.069493\n",
      "batch 3306: loss 0.079567\n",
      "batch 3307: loss 0.096921\n",
      "batch 3308: loss 0.134323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3309: loss 0.030314\n",
      "batch 3310: loss 0.246824\n",
      "batch 3311: loss 0.082861\n",
      "batch 3312: loss 0.210063\n",
      "batch 3313: loss 0.094658\n",
      "batch 3314: loss 0.116507\n",
      "batch 3315: loss 0.118249\n",
      "batch 3316: loss 0.015026\n",
      "batch 3317: loss 0.071672\n",
      "batch 3318: loss 0.040574\n",
      "batch 3319: loss 0.031819\n",
      "batch 3320: loss 0.146594\n",
      "batch 3321: loss 0.052036\n",
      "batch 3322: loss 0.044089\n",
      "batch 3323: loss 0.156415\n",
      "batch 3324: loss 0.200783\n",
      "batch 3325: loss 0.066514\n",
      "batch 3326: loss 0.125016\n",
      "batch 3327: loss 0.193723\n",
      "batch 3328: loss 0.088835\n",
      "batch 3329: loss 0.080370\n",
      "batch 3330: loss 0.076001\n",
      "batch 3331: loss 0.033754\n",
      "batch 3332: loss 0.041285\n",
      "batch 3333: loss 0.080054\n",
      "batch 3334: loss 0.146508\n",
      "batch 3335: loss 0.096285\n",
      "batch 3336: loss 0.023073\n",
      "batch 3337: loss 0.015284\n",
      "batch 3338: loss 0.067409\n",
      "batch 3339: loss 0.024058\n",
      "batch 3340: loss 0.116922\n",
      "batch 3341: loss 0.085666\n",
      "batch 3342: loss 0.071994\n",
      "batch 3343: loss 0.034556\n",
      "batch 3344: loss 0.151739\n",
      "batch 3345: loss 0.280967\n",
      "batch 3346: loss 0.088228\n",
      "batch 3347: loss 0.071253\n",
      "batch 3348: loss 0.218586\n",
      "batch 3349: loss 0.280720\n",
      "batch 3350: loss 0.120558\n",
      "batch 3351: loss 0.045420\n",
      "batch 3352: loss 0.087013\n",
      "batch 3353: loss 0.033477\n",
      "batch 3354: loss 0.212496\n",
      "batch 3355: loss 0.165247\n",
      "batch 3356: loss 0.034570\n",
      "batch 3357: loss 0.038853\n",
      "batch 3358: loss 0.082235\n",
      "batch 3359: loss 0.035621\n",
      "batch 3360: loss 0.150030\n",
      "batch 3361: loss 0.108880\n",
      "batch 3362: loss 0.123890\n",
      "batch 3363: loss 0.024222\n",
      "batch 3364: loss 0.262069\n",
      "batch 3365: loss 0.102628\n",
      "batch 3366: loss 0.016012\n",
      "batch 3367: loss 0.116024\n",
      "batch 3368: loss 0.132326\n",
      "batch 3369: loss 0.142636\n",
      "batch 3370: loss 0.016141\n",
      "batch 3371: loss 0.094520\n",
      "batch 3372: loss 0.088964\n",
      "batch 3373: loss 0.103685\n",
      "batch 3374: loss 0.037970\n",
      "batch 3375: loss 0.171299\n",
      "batch 3376: loss 0.093782\n",
      "batch 3377: loss 0.077603\n",
      "batch 3378: loss 0.120325\n",
      "batch 3379: loss 0.109642\n",
      "batch 3380: loss 0.227836\n",
      "batch 3381: loss 0.081458\n",
      "batch 3382: loss 0.070233\n",
      "batch 3383: loss 0.164256\n",
      "batch 3384: loss 0.086797\n",
      "batch 3385: loss 0.087976\n",
      "batch 3386: loss 0.138344\n",
      "batch 3387: loss 0.305871\n",
      "batch 3388: loss 0.220040\n",
      "batch 3389: loss 0.185225\n",
      "batch 3390: loss 0.256154\n",
      "batch 3391: loss 0.142244\n",
      "batch 3392: loss 0.182883\n",
      "batch 3393: loss 0.081731\n",
      "batch 3394: loss 0.031703\n",
      "batch 3395: loss 0.031778\n",
      "batch 3396: loss 0.054243\n",
      "batch 3397: loss 0.230466\n",
      "batch 3398: loss 0.023310\n",
      "batch 3399: loss 0.130478\n",
      "batch 3400: loss 0.172947\n",
      "batch 3401: loss 0.056083\n",
      "batch 3402: loss 0.021854\n",
      "batch 3403: loss 0.084544\n",
      "batch 3404: loss 0.029087\n",
      "batch 3405: loss 0.083087\n",
      "batch 3406: loss 0.088963\n",
      "batch 3407: loss 0.244100\n",
      "batch 3408: loss 0.076904\n",
      "batch 3409: loss 0.078515\n",
      "batch 3410: loss 0.091080\n",
      "batch 3411: loss 0.126697\n",
      "batch 3412: loss 0.128935\n",
      "batch 3413: loss 0.036001\n",
      "batch 3414: loss 0.028229\n",
      "batch 3415: loss 0.034003\n",
      "batch 3416: loss 0.204630\n",
      "batch 3417: loss 0.050558\n",
      "batch 3418: loss 0.213884\n",
      "batch 3419: loss 0.120998\n",
      "batch 3420: loss 0.101984\n",
      "batch 3421: loss 0.179887\n",
      "batch 3422: loss 0.041152\n",
      "batch 3423: loss 0.087120\n",
      "batch 3424: loss 0.174303\n",
      "batch 3425: loss 0.062337\n",
      "batch 3426: loss 0.074682\n",
      "batch 3427: loss 0.202454\n",
      "batch 3428: loss 0.215736\n",
      "batch 3429: loss 0.098972\n",
      "batch 3430: loss 0.012652\n",
      "batch 3431: loss 0.143917\n",
      "batch 3432: loss 0.042901\n",
      "batch 3433: loss 0.134616\n",
      "batch 3434: loss 0.169580\n",
      "batch 3435: loss 0.124953\n",
      "batch 3436: loss 0.071215\n",
      "batch 3437: loss 0.095583\n",
      "batch 3438: loss 0.088619\n",
      "batch 3439: loss 0.048352\n",
      "batch 3440: loss 0.050075\n",
      "batch 3441: loss 0.052001\n",
      "batch 3442: loss 0.058873\n",
      "batch 3443: loss 0.052702\n",
      "batch 3444: loss 0.022400\n",
      "batch 3445: loss 0.069766\n",
      "batch 3446: loss 0.075502\n",
      "batch 3447: loss 0.081658\n",
      "batch 3448: loss 0.109592\n",
      "batch 3449: loss 0.457342\n",
      "batch 3450: loss 0.025004\n",
      "batch 3451: loss 0.301745\n",
      "batch 3452: loss 0.044695\n",
      "batch 3453: loss 0.234302\n",
      "batch 3454: loss 0.112163\n",
      "batch 3455: loss 0.079920\n",
      "batch 3456: loss 0.161707\n",
      "batch 3457: loss 0.047226\n",
      "batch 3458: loss 0.057625\n",
      "batch 3459: loss 0.047213\n",
      "batch 3460: loss 0.057914\n",
      "batch 3461: loss 0.067024\n",
      "batch 3462: loss 0.133815\n",
      "batch 3463: loss 0.051034\n",
      "batch 3464: loss 0.107416\n",
      "batch 3465: loss 0.060145\n",
      "batch 3466: loss 0.284243\n",
      "batch 3467: loss 0.028544\n",
      "batch 3468: loss 0.124170\n",
      "batch 3469: loss 0.022936\n",
      "batch 3470: loss 0.118505\n",
      "batch 3471: loss 0.019005\n",
      "batch 3472: loss 0.047594\n",
      "batch 3473: loss 0.156993\n",
      "batch 3474: loss 0.084594\n",
      "batch 3475: loss 0.101029\n",
      "batch 3476: loss 0.145600\n",
      "batch 3477: loss 0.055355\n",
      "batch 3478: loss 0.068418\n",
      "batch 3479: loss 0.038823\n",
      "batch 3480: loss 0.262953\n",
      "batch 3481: loss 0.091004\n",
      "batch 3482: loss 0.052165\n",
      "batch 3483: loss 0.330772\n",
      "batch 3484: loss 0.188626\n",
      "batch 3485: loss 0.041005\n",
      "batch 3486: loss 0.059881\n",
      "batch 3487: loss 0.046098\n",
      "batch 3488: loss 0.167662\n",
      "batch 3489: loss 0.251625\n",
      "batch 3490: loss 0.172221\n",
      "batch 3491: loss 0.024437\n",
      "batch 3492: loss 0.145506\n",
      "batch 3493: loss 0.076660\n",
      "batch 3494: loss 0.103862\n",
      "batch 3495: loss 0.116643\n",
      "batch 3496: loss 0.152778\n",
      "batch 3497: loss 0.015346\n",
      "batch 3498: loss 0.115328\n",
      "batch 3499: loss 0.257670\n",
      "batch 3500: loss 0.140878\n",
      "batch 3501: loss 0.049552\n",
      "batch 3502: loss 0.136929\n",
      "batch 3503: loss 0.024726\n",
      "batch 3504: loss 0.047258\n",
      "batch 3505: loss 0.295512\n",
      "batch 3506: loss 0.146007\n",
      "batch 3507: loss 0.091082\n",
      "batch 3508: loss 0.036014\n",
      "batch 3509: loss 0.072985\n",
      "batch 3510: loss 0.057474\n",
      "batch 3511: loss 0.075156\n",
      "batch 3512: loss 0.058192\n",
      "batch 3513: loss 0.077337\n",
      "batch 3514: loss 0.067729\n",
      "batch 3515: loss 0.076536\n",
      "batch 3516: loss 0.097118\n",
      "batch 3517: loss 0.056061\n",
      "batch 3518: loss 0.141646\n",
      "batch 3519: loss 0.139596\n",
      "batch 3520: loss 0.097720\n",
      "batch 3521: loss 0.202471\n",
      "batch 3522: loss 0.040879\n",
      "batch 3523: loss 0.035570\n",
      "batch 3524: loss 0.018945\n",
      "batch 3525: loss 0.065361\n",
      "batch 3526: loss 0.054993\n",
      "batch 3527: loss 0.073946\n",
      "batch 3528: loss 0.133879\n",
      "batch 3529: loss 0.073974\n",
      "batch 3530: loss 0.071071\n",
      "batch 3531: loss 0.097800\n",
      "batch 3532: loss 0.118013\n",
      "batch 3533: loss 0.078248\n",
      "batch 3534: loss 0.017250\n",
      "batch 3535: loss 0.075817\n",
      "batch 3536: loss 0.338716\n",
      "batch 3537: loss 0.141471\n",
      "batch 3538: loss 0.285756\n",
      "batch 3539: loss 0.062237\n",
      "batch 3540: loss 0.058340\n",
      "batch 3541: loss 0.218618\n",
      "batch 3542: loss 0.103508\n",
      "batch 3543: loss 0.125795\n",
      "batch 3544: loss 0.072609\n",
      "batch 3545: loss 0.038483\n",
      "batch 3546: loss 0.052197\n",
      "batch 3547: loss 0.037902\n",
      "batch 3548: loss 0.067379\n",
      "batch 3549: loss 0.047078\n",
      "batch 3550: loss 0.122561\n",
      "batch 3551: loss 0.181667\n",
      "batch 3552: loss 0.115877\n",
      "batch 3553: loss 0.088805\n",
      "batch 3554: loss 0.085990\n",
      "batch 3555: loss 0.089005\n",
      "batch 3556: loss 0.041886\n",
      "batch 3557: loss 0.048519\n",
      "batch 3558: loss 0.108196\n",
      "batch 3559: loss 0.047324\n",
      "batch 3560: loss 0.089291\n",
      "batch 3561: loss 0.339001\n",
      "batch 3562: loss 0.023632\n",
      "batch 3563: loss 0.429151\n",
      "batch 3564: loss 0.129901\n",
      "batch 3565: loss 0.035042\n",
      "batch 3566: loss 0.034269\n",
      "batch 3567: loss 0.061585\n",
      "batch 3568: loss 0.040393\n",
      "batch 3569: loss 0.027313\n",
      "batch 3570: loss 0.018614\n",
      "batch 3571: loss 0.130442\n",
      "batch 3572: loss 0.024234\n",
      "batch 3573: loss 0.207953\n",
      "batch 3574: loss 0.231546\n",
      "batch 3575: loss 0.019983\n",
      "batch 3576: loss 0.110040\n",
      "batch 3577: loss 0.054073\n",
      "batch 3578: loss 0.180820\n",
      "batch 3579: loss 0.052947\n",
      "batch 3580: loss 0.078013\n",
      "batch 3581: loss 0.139300\n",
      "batch 3582: loss 0.079798\n",
      "batch 3583: loss 0.226314\n",
      "batch 3584: loss 0.046853\n",
      "batch 3585: loss 0.106344\n",
      "batch 3586: loss 0.041384\n",
      "batch 3587: loss 0.049294\n",
      "batch 3588: loss 0.130846\n",
      "batch 3589: loss 0.246264\n",
      "batch 3590: loss 0.138371\n",
      "batch 3591: loss 0.046891\n",
      "batch 3592: loss 0.071330\n",
      "batch 3593: loss 0.162392\n",
      "batch 3594: loss 0.078385\n",
      "batch 3595: loss 0.128446\n",
      "batch 3596: loss 0.040443\n",
      "batch 3597: loss 0.099004\n",
      "batch 3598: loss 0.028234\n",
      "batch 3599: loss 0.053717\n",
      "batch 3600: loss 0.014373\n",
      "batch 3601: loss 0.177525\n",
      "batch 3602: loss 0.032108\n",
      "batch 3603: loss 0.081605\n",
      "batch 3604: loss 0.044802\n",
      "batch 3605: loss 0.079643\n",
      "batch 3606: loss 0.051182\n",
      "batch 3607: loss 0.013398\n",
      "batch 3608: loss 0.042609\n",
      "batch 3609: loss 0.034529\n",
      "batch 3610: loss 0.059670\n",
      "batch 3611: loss 0.081430\n",
      "batch 3612: loss 0.090854\n",
      "batch 3613: loss 0.124833\n",
      "batch 3614: loss 0.027220\n",
      "batch 3615: loss 0.214263\n",
      "batch 3616: loss 0.087983\n",
      "batch 3617: loss 0.063662\n",
      "batch 3618: loss 0.121735\n",
      "batch 3619: loss 0.078209\n",
      "batch 3620: loss 0.022763\n",
      "batch 3621: loss 0.180384\n",
      "batch 3622: loss 0.125861\n",
      "batch 3623: loss 0.093813\n",
      "batch 3624: loss 0.030098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3625: loss 0.077823\n",
      "batch 3626: loss 0.042253\n",
      "batch 3627: loss 0.195957\n",
      "batch 3628: loss 0.236012\n",
      "batch 3629: loss 0.210830\n",
      "batch 3630: loss 0.136804\n",
      "batch 3631: loss 0.049551\n",
      "batch 3632: loss 0.205028\n",
      "batch 3633: loss 0.073398\n",
      "batch 3634: loss 0.083240\n",
      "batch 3635: loss 0.113853\n",
      "batch 3636: loss 0.161542\n",
      "batch 3637: loss 0.019295\n",
      "batch 3638: loss 0.084335\n",
      "batch 3639: loss 0.094270\n",
      "batch 3640: loss 0.033003\n",
      "batch 3641: loss 0.097538\n",
      "batch 3642: loss 0.131005\n",
      "batch 3643: loss 0.078762\n",
      "batch 3644: loss 0.071568\n",
      "batch 3645: loss 0.234576\n",
      "batch 3646: loss 0.136510\n",
      "batch 3647: loss 0.071400\n",
      "batch 3648: loss 0.170940\n",
      "batch 3649: loss 0.037965\n",
      "batch 3650: loss 0.039460\n",
      "batch 3651: loss 0.048063\n",
      "batch 3652: loss 0.081085\n",
      "batch 3653: loss 0.067839\n",
      "batch 3654: loss 0.113121\n",
      "batch 3655: loss 0.130392\n",
      "batch 3656: loss 0.198004\n",
      "batch 3657: loss 0.197090\n",
      "batch 3658: loss 0.034712\n",
      "batch 3659: loss 0.060268\n",
      "batch 3660: loss 0.139390\n",
      "batch 3661: loss 0.048426\n",
      "batch 3662: loss 0.023378\n",
      "batch 3663: loss 0.087851\n",
      "batch 3664: loss 0.113525\n",
      "batch 3665: loss 0.060355\n",
      "batch 3666: loss 0.025516\n",
      "batch 3667: loss 0.033691\n",
      "batch 3668: loss 0.055843\n",
      "batch 3669: loss 0.164617\n",
      "batch 3670: loss 0.082457\n",
      "batch 3671: loss 0.025781\n",
      "batch 3672: loss 0.263854\n",
      "batch 3673: loss 0.193702\n",
      "batch 3674: loss 0.028302\n",
      "batch 3675: loss 0.139749\n",
      "batch 3676: loss 0.040718\n",
      "batch 3677: loss 0.065391\n",
      "batch 3678: loss 0.088493\n",
      "batch 3679: loss 0.059150\n",
      "batch 3680: loss 0.038371\n",
      "batch 3681: loss 0.041592\n",
      "batch 3682: loss 0.258316\n",
      "batch 3683: loss 0.137150\n",
      "batch 3684: loss 0.033550\n",
      "batch 3685: loss 0.091495\n",
      "batch 3686: loss 0.033441\n",
      "batch 3687: loss 0.112337\n",
      "batch 3688: loss 0.245295\n",
      "batch 3689: loss 0.141051\n",
      "batch 3690: loss 0.215621\n",
      "batch 3691: loss 0.026097\n",
      "batch 3692: loss 0.047942\n",
      "batch 3693: loss 0.221673\n",
      "batch 3694: loss 0.038648\n",
      "batch 3695: loss 0.019095\n",
      "batch 3696: loss 0.015988\n",
      "batch 3697: loss 0.045926\n",
      "batch 3698: loss 0.099921\n",
      "batch 3699: loss 0.025987\n",
      "batch 3700: loss 0.035617\n",
      "batch 3701: loss 0.069845\n",
      "batch 3702: loss 0.034110\n",
      "batch 3703: loss 0.280989\n",
      "batch 3704: loss 0.202198\n",
      "batch 3705: loss 0.078171\n",
      "batch 3706: loss 0.016643\n",
      "batch 3707: loss 0.021228\n",
      "batch 3708: loss 0.021804\n",
      "batch 3709: loss 0.089949\n",
      "batch 3710: loss 0.051484\n",
      "batch 3711: loss 0.175895\n",
      "batch 3712: loss 0.154634\n",
      "batch 3713: loss 0.118726\n",
      "batch 3714: loss 0.048729\n",
      "batch 3715: loss 0.042741\n",
      "batch 3716: loss 0.126022\n",
      "batch 3717: loss 0.180846\n",
      "batch 3718: loss 0.221819\n",
      "batch 3719: loss 0.025277\n",
      "batch 3720: loss 0.058601\n",
      "batch 3721: loss 0.043192\n",
      "batch 3722: loss 0.165231\n",
      "batch 3723: loss 0.076276\n",
      "batch 3724: loss 0.307909\n",
      "batch 3725: loss 0.050334\n",
      "batch 3726: loss 0.087727\n",
      "batch 3727: loss 0.185513\n",
      "batch 3728: loss 0.152557\n",
      "batch 3729: loss 0.182524\n",
      "batch 3730: loss 0.212500\n",
      "batch 3731: loss 0.169074\n",
      "batch 3732: loss 0.338379\n",
      "batch 3733: loss 0.027691\n",
      "batch 3734: loss 0.090400\n",
      "batch 3735: loss 0.225988\n",
      "batch 3736: loss 0.045540\n",
      "batch 3737: loss 0.046287\n",
      "batch 3738: loss 0.051631\n",
      "batch 3739: loss 0.099656\n",
      "batch 3740: loss 0.150972\n",
      "batch 3741: loss 0.021556\n",
      "batch 3742: loss 0.019133\n",
      "batch 3743: loss 0.161424\n",
      "batch 3744: loss 0.013566\n",
      "batch 3745: loss 0.017420\n",
      "batch 3746: loss 0.017735\n",
      "batch 3747: loss 0.087342\n",
      "batch 3748: loss 0.188029\n",
      "batch 3749: loss 0.047695\n",
      "batch 3750: loss 0.016863\n",
      "batch 3751: loss 0.051937\n",
      "batch 3752: loss 0.098285\n",
      "batch 3753: loss 0.062693\n",
      "batch 3754: loss 0.059414\n",
      "batch 3755: loss 0.198359\n",
      "batch 3756: loss 0.058492\n",
      "batch 3757: loss 0.243211\n",
      "batch 3758: loss 0.041771\n",
      "batch 3759: loss 0.045965\n",
      "batch 3760: loss 0.004405\n",
      "batch 3761: loss 0.045456\n",
      "batch 3762: loss 0.064687\n",
      "batch 3763: loss 0.033256\n",
      "batch 3764: loss 0.080661\n",
      "batch 3765: loss 0.220588\n",
      "batch 3766: loss 0.155617\n",
      "batch 3767: loss 0.145869\n",
      "batch 3768: loss 0.035227\n",
      "batch 3769: loss 0.049192\n",
      "batch 3770: loss 0.048769\n",
      "batch 3771: loss 0.241758\n",
      "batch 3772: loss 0.385929\n",
      "batch 3773: loss 0.024332\n",
      "batch 3774: loss 0.027924\n",
      "batch 3775: loss 0.023234\n",
      "batch 3776: loss 0.088871\n",
      "batch 3777: loss 0.104686\n",
      "batch 3778: loss 0.074094\n",
      "batch 3779: loss 0.240747\n",
      "batch 3780: loss 0.110372\n",
      "batch 3781: loss 0.102237\n",
      "batch 3782: loss 0.133942\n",
      "batch 3783: loss 0.054927\n",
      "batch 3784: loss 0.119070\n",
      "batch 3785: loss 0.187459\n",
      "batch 3786: loss 0.039990\n",
      "batch 3787: loss 0.169330\n",
      "batch 3788: loss 0.244822\n",
      "batch 3789: loss 0.113457\n",
      "batch 3790: loss 0.016189\n",
      "batch 3791: loss 0.315688\n",
      "batch 3792: loss 0.287450\n",
      "batch 3793: loss 0.285797\n",
      "batch 3794: loss 0.147890\n",
      "batch 3795: loss 0.159667\n",
      "batch 3796: loss 0.084885\n",
      "batch 3797: loss 0.023523\n",
      "batch 3798: loss 0.052806\n",
      "batch 3799: loss 0.014347\n",
      "batch 3800: loss 0.041467\n",
      "batch 3801: loss 0.044585\n",
      "batch 3802: loss 0.107578\n",
      "batch 3803: loss 0.165380\n",
      "batch 3804: loss 0.020775\n",
      "batch 3805: loss 0.097966\n",
      "batch 3806: loss 0.043809\n",
      "batch 3807: loss 0.026742\n",
      "batch 3808: loss 0.241333\n",
      "batch 3809: loss 0.037397\n",
      "batch 3810: loss 0.097597\n",
      "batch 3811: loss 0.073754\n",
      "batch 3812: loss 0.058693\n",
      "batch 3813: loss 0.332131\n",
      "batch 3814: loss 0.054585\n",
      "batch 3815: loss 0.100224\n",
      "batch 3816: loss 0.021727\n",
      "batch 3817: loss 0.042236\n",
      "batch 3818: loss 0.124359\n",
      "batch 3819: loss 0.040913\n",
      "batch 3820: loss 0.192080\n",
      "batch 3821: loss 0.071213\n",
      "batch 3822: loss 0.145734\n",
      "batch 3823: loss 0.206258\n",
      "batch 3824: loss 0.091210\n",
      "batch 3825: loss 0.035071\n",
      "batch 3826: loss 0.291422\n",
      "batch 3827: loss 0.266357\n",
      "batch 3828: loss 0.069248\n",
      "batch 3829: loss 0.029401\n",
      "batch 3830: loss 0.032157\n",
      "batch 3831: loss 0.057503\n",
      "batch 3832: loss 0.112992\n",
      "batch 3833: loss 0.086009\n",
      "batch 3834: loss 0.102443\n",
      "batch 3835: loss 0.072896\n",
      "batch 3836: loss 0.093951\n",
      "batch 3837: loss 0.040812\n",
      "batch 3838: loss 0.029290\n",
      "batch 3839: loss 0.110957\n",
      "batch 3840: loss 0.124402\n",
      "batch 3841: loss 0.030464\n",
      "batch 3842: loss 0.105878\n",
      "batch 3843: loss 0.041846\n",
      "batch 3844: loss 0.072312\n",
      "batch 3845: loss 0.036588\n",
      "batch 3846: loss 0.062551\n",
      "batch 3847: loss 0.165735\n",
      "batch 3848: loss 0.048180\n",
      "batch 3849: loss 0.040343\n",
      "batch 3850: loss 0.099663\n",
      "batch 3851: loss 0.085152\n",
      "batch 3852: loss 0.007093\n",
      "batch 3853: loss 0.155604\n",
      "batch 3854: loss 0.105510\n",
      "batch 3855: loss 0.036325\n",
      "batch 3856: loss 0.164460\n",
      "batch 3857: loss 0.047464\n",
      "batch 3858: loss 0.016730\n",
      "batch 3859: loss 0.026364\n",
      "batch 3860: loss 0.123444\n",
      "batch 3861: loss 0.020090\n",
      "batch 3862: loss 0.256442\n",
      "batch 3863: loss 0.147165\n",
      "batch 3864: loss 0.048540\n",
      "batch 3865: loss 0.177069\n",
      "batch 3866: loss 0.175990\n",
      "batch 3867: loss 0.074612\n",
      "batch 3868: loss 0.074467\n",
      "batch 3869: loss 0.057055\n",
      "batch 3870: loss 0.191320\n",
      "batch 3871: loss 0.080760\n",
      "batch 3872: loss 0.079453\n",
      "batch 3873: loss 0.193811\n",
      "batch 3874: loss 0.137262\n",
      "batch 3875: loss 0.014661\n",
      "batch 3876: loss 0.052039\n",
      "batch 3877: loss 0.258474\n",
      "batch 3878: loss 0.121274\n",
      "batch 3879: loss 0.042229\n",
      "batch 3880: loss 0.111053\n",
      "batch 3881: loss 0.133457\n",
      "batch 3882: loss 0.046946\n",
      "batch 3883: loss 0.058944\n",
      "batch 3884: loss 0.050881\n",
      "batch 3885: loss 0.089015\n",
      "batch 3886: loss 0.137220\n",
      "batch 3887: loss 0.122100\n",
      "batch 3888: loss 0.162006\n",
      "batch 3889: loss 0.033145\n",
      "batch 3890: loss 0.170933\n",
      "batch 3891: loss 0.053980\n",
      "batch 3892: loss 0.060044\n",
      "batch 3893: loss 0.119900\n",
      "batch 3894: loss 0.027946\n",
      "batch 3895: loss 0.043057\n",
      "batch 3896: loss 0.056704\n",
      "batch 3897: loss 0.070744\n",
      "batch 3898: loss 0.047597\n",
      "batch 3899: loss 0.043798\n",
      "batch 3900: loss 0.107128\n",
      "batch 3901: loss 0.184858\n",
      "batch 3902: loss 0.027382\n",
      "batch 3903: loss 0.098925\n",
      "batch 3904: loss 0.056060\n",
      "batch 3905: loss 0.024230\n",
      "batch 3906: loss 0.173713\n",
      "batch 3907: loss 0.259815\n",
      "batch 3908: loss 0.164550\n",
      "batch 3909: loss 0.142930\n",
      "batch 3910: loss 0.040984\n",
      "batch 3911: loss 0.106011\n",
      "batch 3912: loss 0.060416\n",
      "batch 3913: loss 0.065231\n",
      "batch 3914: loss 0.062250\n",
      "batch 3915: loss 0.060791\n",
      "batch 3916: loss 0.163753\n",
      "batch 3917: loss 0.036568\n",
      "batch 3918: loss 0.032667\n",
      "batch 3919: loss 0.019418\n",
      "batch 3920: loss 0.193170\n",
      "batch 3921: loss 0.072023\n",
      "batch 3922: loss 0.017115\n",
      "batch 3923: loss 0.067220\n",
      "batch 3924: loss 0.076168\n",
      "batch 3925: loss 0.047729\n",
      "batch 3926: loss 0.088449\n",
      "batch 3927: loss 0.112181\n",
      "batch 3928: loss 0.091475\n",
      "batch 3929: loss 0.029201\n",
      "batch 3930: loss 0.097654\n",
      "batch 3931: loss 0.058989\n",
      "batch 3932: loss 0.146252\n",
      "batch 3933: loss 0.078265\n",
      "batch 3934: loss 0.086061\n",
      "batch 3935: loss 0.039527\n",
      "batch 3936: loss 0.072170\n",
      "batch 3937: loss 0.077075\n",
      "batch 3938: loss 0.128180\n",
      "batch 3939: loss 0.031283\n",
      "batch 3940: loss 0.024075\n",
      "batch 3941: loss 0.072708\n",
      "batch 3942: loss 0.096310\n",
      "batch 3943: loss 0.101835\n",
      "batch 3944: loss 0.057343\n",
      "batch 3945: loss 0.101288\n",
      "batch 3946: loss 0.006739\n",
      "batch 3947: loss 0.037002\n",
      "batch 3948: loss 0.040785\n",
      "batch 3949: loss 0.046508\n",
      "batch 3950: loss 0.080913\n",
      "batch 3951: loss 0.149740\n",
      "batch 3952: loss 0.134684\n",
      "batch 3953: loss 0.091948\n",
      "batch 3954: loss 0.033417\n",
      "batch 3955: loss 0.045794\n",
      "batch 3956: loss 0.097201\n",
      "batch 3957: loss 0.011474\n",
      "batch 3958: loss 0.143297\n",
      "batch 3959: loss 0.101372\n",
      "batch 3960: loss 0.076408\n",
      "batch 3961: loss 0.162221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3962: loss 0.019947\n",
      "batch 3963: loss 0.107689\n",
      "batch 3964: loss 0.044097\n",
      "batch 3965: loss 0.057889\n",
      "batch 3966: loss 0.040046\n",
      "batch 3967: loss 0.149416\n",
      "batch 3968: loss 0.033690\n",
      "batch 3969: loss 0.049494\n",
      "batch 3970: loss 0.010871\n",
      "batch 3971: loss 0.123046\n",
      "batch 3972: loss 0.031130\n",
      "batch 3973: loss 0.154772\n",
      "batch 3974: loss 0.141090\n",
      "batch 3975: loss 0.053921\n",
      "batch 3976: loss 0.211721\n",
      "batch 3977: loss 0.015470\n",
      "batch 3978: loss 0.076875\n",
      "batch 3979: loss 0.089847\n",
      "batch 3980: loss 0.019839\n",
      "batch 3981: loss 0.209985\n",
      "batch 3982: loss 0.177108\n",
      "batch 3983: loss 0.169615\n",
      "batch 3984: loss 0.041004\n",
      "batch 3985: loss 0.031369\n",
      "batch 3986: loss 0.100805\n",
      "batch 3987: loss 0.070922\n",
      "batch 3988: loss 0.035056\n",
      "batch 3989: loss 0.226515\n",
      "batch 3990: loss 0.083433\n",
      "batch 3991: loss 0.023056\n",
      "batch 3992: loss 0.123425\n",
      "batch 3993: loss 0.072564\n",
      "batch 3994: loss 0.065476\n",
      "batch 3995: loss 0.138747\n",
      "batch 3996: loss 0.010186\n",
      "batch 3997: loss 0.069747\n",
      "batch 3998: loss 0.117253\n",
      "batch 3999: loss 0.226971\n",
      "batch 4000: loss 0.049616\n",
      "batch 4001: loss 0.062979\n",
      "batch 4002: loss 0.054169\n",
      "batch 4003: loss 0.058533\n",
      "batch 4004: loss 0.010223\n",
      "batch 4005: loss 0.226644\n",
      "batch 4006: loss 0.210019\n",
      "batch 4007: loss 0.132886\n",
      "batch 4008: loss 0.048922\n",
      "batch 4009: loss 0.014236\n",
      "batch 4010: loss 0.025861\n",
      "batch 4011: loss 0.053294\n",
      "batch 4012: loss 0.055587\n",
      "batch 4013: loss 0.204216\n",
      "batch 4014: loss 0.073060\n",
      "batch 4015: loss 0.228076\n",
      "batch 4016: loss 0.041120\n",
      "batch 4017: loss 0.063983\n",
      "batch 4018: loss 0.150541\n",
      "batch 4019: loss 0.223800\n",
      "batch 4020: loss 0.061648\n",
      "batch 4021: loss 0.099159\n",
      "batch 4022: loss 0.073284\n",
      "batch 4023: loss 0.018331\n",
      "batch 4024: loss 0.059051\n",
      "batch 4025: loss 0.019254\n",
      "batch 4026: loss 0.021203\n",
      "batch 4027: loss 0.157353\n",
      "batch 4028: loss 0.040343\n",
      "batch 4029: loss 0.157934\n",
      "batch 4030: loss 0.106877\n",
      "batch 4031: loss 0.135806\n",
      "batch 4032: loss 0.041610\n",
      "batch 4033: loss 0.079704\n",
      "batch 4034: loss 0.065204\n",
      "batch 4035: loss 0.165801\n",
      "batch 4036: loss 0.112757\n",
      "batch 4037: loss 0.129814\n",
      "batch 4038: loss 0.104466\n",
      "batch 4039: loss 0.026372\n",
      "batch 4040: loss 0.252056\n",
      "batch 4041: loss 0.035197\n",
      "batch 4042: loss 0.034905\n",
      "batch 4043: loss 0.255351\n",
      "batch 4044: loss 0.041035\n",
      "batch 4045: loss 0.033096\n",
      "batch 4046: loss 0.201696\n",
      "batch 4047: loss 0.021915\n",
      "batch 4048: loss 0.084057\n",
      "batch 4049: loss 0.019136\n",
      "batch 4050: loss 0.153800\n",
      "batch 4051: loss 0.029992\n",
      "batch 4052: loss 0.070066\n",
      "batch 4053: loss 0.028884\n",
      "batch 4054: loss 0.080405\n",
      "batch 4055: loss 0.265281\n",
      "batch 4056: loss 0.023353\n",
      "batch 4057: loss 0.040008\n",
      "batch 4058: loss 0.085786\n",
      "batch 4059: loss 0.365229\n",
      "batch 4060: loss 0.084338\n",
      "batch 4061: loss 0.077280\n",
      "batch 4062: loss 0.057559\n",
      "batch 4063: loss 0.043815\n",
      "batch 4064: loss 0.046627\n",
      "batch 4065: loss 0.016456\n",
      "batch 4066: loss 0.106315\n",
      "batch 4067: loss 0.108367\n",
      "batch 4068: loss 0.077270\n",
      "batch 4069: loss 0.026612\n",
      "batch 4070: loss 0.098077\n",
      "batch 4071: loss 0.049941\n",
      "batch 4072: loss 0.083188\n",
      "batch 4073: loss 0.097816\n",
      "batch 4074: loss 0.071237\n",
      "batch 4075: loss 0.053819\n",
      "batch 4076: loss 0.074224\n",
      "batch 4077: loss 0.078578\n",
      "batch 4078: loss 0.037798\n",
      "batch 4079: loss 0.019219\n",
      "batch 4080: loss 0.032526\n",
      "batch 4081: loss 0.128344\n",
      "batch 4082: loss 0.034245\n",
      "batch 4083: loss 0.058450\n",
      "batch 4084: loss 0.067918\n",
      "batch 4085: loss 0.093690\n",
      "batch 4086: loss 0.120638\n",
      "batch 4087: loss 0.054102\n",
      "batch 4088: loss 0.051666\n",
      "batch 4089: loss 0.130957\n",
      "batch 4090: loss 0.252735\n",
      "batch 4091: loss 0.039525\n",
      "batch 4092: loss 0.171239\n",
      "batch 4093: loss 0.039644\n",
      "batch 4094: loss 0.031694\n",
      "batch 4095: loss 0.040863\n",
      "batch 4096: loss 0.096389\n",
      "batch 4097: loss 0.261332\n",
      "batch 4098: loss 0.042917\n",
      "batch 4099: loss 0.073864\n",
      "batch 4100: loss 0.146625\n",
      "batch 4101: loss 0.044916\n",
      "batch 4102: loss 0.156081\n",
      "batch 4103: loss 0.049956\n",
      "batch 4104: loss 0.109412\n",
      "batch 4105: loss 0.233234\n",
      "batch 4106: loss 0.010852\n",
      "batch 4107: loss 0.025802\n",
      "batch 4108: loss 0.153165\n",
      "batch 4109: loss 0.096516\n",
      "batch 4110: loss 0.025069\n",
      "batch 4111: loss 0.016123\n",
      "batch 4112: loss 0.007919\n",
      "batch 4113: loss 0.088792\n",
      "batch 4114: loss 0.073203\n",
      "batch 4115: loss 0.026243\n",
      "batch 4116: loss 0.109766\n",
      "batch 4117: loss 0.057910\n",
      "batch 4118: loss 0.078983\n",
      "batch 4119: loss 0.094075\n",
      "batch 4120: loss 0.073992\n",
      "batch 4121: loss 0.054801\n",
      "batch 4122: loss 0.085616\n",
      "batch 4123: loss 0.024537\n",
      "batch 4124: loss 0.016489\n",
      "batch 4125: loss 0.058930\n",
      "batch 4126: loss 0.019173\n",
      "batch 4127: loss 0.035084\n",
      "batch 4128: loss 0.038129\n",
      "batch 4129: loss 0.136191\n",
      "batch 4130: loss 0.164288\n",
      "batch 4131: loss 0.282032\n",
      "batch 4132: loss 0.123289\n",
      "batch 4133: loss 0.010471\n",
      "batch 4134: loss 0.114508\n",
      "batch 4135: loss 0.054233\n",
      "batch 4136: loss 0.035910\n",
      "batch 4137: loss 0.044422\n",
      "batch 4138: loss 0.044774\n",
      "batch 4139: loss 0.049769\n",
      "batch 4140: loss 0.065485\n",
      "batch 4141: loss 0.027663\n",
      "batch 4142: loss 0.044554\n",
      "batch 4143: loss 0.039751\n",
      "batch 4144: loss 0.017479\n",
      "batch 4145: loss 0.159429\n",
      "batch 4146: loss 0.139642\n",
      "batch 4147: loss 0.075108\n",
      "batch 4148: loss 0.059785\n",
      "batch 4149: loss 0.025158\n",
      "batch 4150: loss 0.052156\n",
      "batch 4151: loss 0.116477\n",
      "batch 4152: loss 0.024965\n",
      "batch 4153: loss 0.019665\n",
      "batch 4154: loss 0.089112\n",
      "batch 4155: loss 0.021300\n",
      "batch 4156: loss 0.084389\n",
      "batch 4157: loss 0.161250\n",
      "batch 4158: loss 0.027786\n",
      "batch 4159: loss 0.093001\n",
      "batch 4160: loss 0.044498\n",
      "batch 4161: loss 0.059112\n",
      "batch 4162: loss 0.058834\n",
      "batch 4163: loss 0.038502\n",
      "batch 4164: loss 0.075858\n",
      "batch 4165: loss 0.145353\n",
      "batch 4166: loss 0.048482\n",
      "batch 4167: loss 0.061018\n",
      "batch 4168: loss 0.045642\n",
      "batch 4169: loss 0.053330\n",
      "batch 4170: loss 0.006388\n",
      "batch 4171: loss 0.015050\n",
      "batch 4172: loss 0.027178\n",
      "batch 4173: loss 0.021661\n",
      "batch 4174: loss 0.057583\n",
      "batch 4175: loss 0.135380\n",
      "batch 4176: loss 0.101386\n",
      "batch 4177: loss 0.175233\n",
      "batch 4178: loss 0.109582\n",
      "batch 4179: loss 0.086865\n",
      "batch 4180: loss 0.028271\n",
      "batch 4181: loss 0.096367\n",
      "batch 4182: loss 0.028757\n",
      "batch 4183: loss 0.044529\n",
      "batch 4184: loss 0.018651\n",
      "batch 4185: loss 0.057110\n",
      "batch 4186: loss 0.037393\n",
      "batch 4187: loss 0.039434\n",
      "batch 4188: loss 0.054408\n",
      "batch 4189: loss 0.071055\n",
      "batch 4190: loss 0.087818\n",
      "batch 4191: loss 0.035398\n",
      "batch 4192: loss 0.093503\n",
      "batch 4193: loss 0.022505\n",
      "batch 4194: loss 0.013755\n",
      "batch 4195: loss 0.073894\n",
      "batch 4196: loss 0.061492\n",
      "batch 4197: loss 0.030276\n",
      "batch 4198: loss 0.172514\n",
      "batch 4199: loss 0.064610\n",
      "batch 4200: loss 0.056686\n",
      "batch 4201: loss 0.029007\n",
      "batch 4202: loss 0.044002\n",
      "batch 4203: loss 0.242087\n",
      "batch 4204: loss 0.116183\n",
      "batch 4205: loss 0.036983\n",
      "batch 4206: loss 0.041347\n",
      "batch 4207: loss 0.029864\n",
      "batch 4208: loss 0.028500\n",
      "batch 4209: loss 0.102360\n",
      "batch 4210: loss 0.186866\n",
      "batch 4211: loss 0.113905\n",
      "batch 4212: loss 0.064463\n",
      "batch 4213: loss 0.048601\n",
      "batch 4214: loss 0.030027\n",
      "batch 4215: loss 0.074897\n",
      "batch 4216: loss 0.009218\n",
      "batch 4217: loss 0.024532\n",
      "batch 4218: loss 0.160349\n",
      "batch 4219: loss 0.098733\n",
      "batch 4220: loss 0.013205\n",
      "batch 4221: loss 0.066512\n",
      "batch 4222: loss 0.168641\n",
      "batch 4223: loss 0.044915\n",
      "batch 4224: loss 0.049578\n",
      "batch 4225: loss 0.048030\n",
      "batch 4226: loss 0.021099\n",
      "batch 4227: loss 0.013134\n",
      "batch 4228: loss 0.123041\n",
      "batch 4229: loss 0.073367\n",
      "batch 4230: loss 0.078244\n",
      "batch 4231: loss 0.026396\n",
      "batch 4232: loss 0.185885\n",
      "batch 4233: loss 0.098469\n",
      "batch 4234: loss 0.013872\n",
      "batch 4235: loss 0.115357\n",
      "batch 4236: loss 0.047361\n",
      "batch 4237: loss 0.004466\n",
      "batch 4238: loss 0.047972\n",
      "batch 4239: loss 0.051438\n",
      "batch 4240: loss 0.188093\n",
      "batch 4241: loss 0.217557\n",
      "batch 4242: loss 0.056893\n",
      "batch 4243: loss 0.055083\n",
      "batch 4244: loss 0.125817\n",
      "batch 4245: loss 0.216740\n",
      "batch 4246: loss 0.132150\n",
      "batch 4247: loss 0.081190\n",
      "batch 4248: loss 0.035548\n",
      "batch 4249: loss 0.032276\n",
      "batch 4250: loss 0.015200\n",
      "batch 4251: loss 0.019231\n",
      "batch 4252: loss 0.116448\n",
      "batch 4253: loss 0.065864\n",
      "batch 4254: loss 0.126936\n",
      "batch 4255: loss 0.007395\n",
      "batch 4256: loss 0.063490\n",
      "batch 4257: loss 0.021976\n",
      "batch 4258: loss 0.036004\n",
      "batch 4259: loss 0.042909\n",
      "batch 4260: loss 0.071678\n",
      "batch 4261: loss 0.050804\n",
      "batch 4262: loss 0.052354\n",
      "batch 4263: loss 0.074854\n",
      "batch 4264: loss 0.025082\n",
      "batch 4265: loss 0.042383\n",
      "batch 4266: loss 0.085880\n",
      "batch 4267: loss 0.025119\n",
      "batch 4268: loss 0.130983\n",
      "batch 4269: loss 0.078352\n",
      "batch 4270: loss 0.070154\n",
      "batch 4271: loss 0.375635\n",
      "batch 4272: loss 0.074111\n",
      "batch 4273: loss 0.089763\n",
      "batch 4274: loss 0.134432\n",
      "batch 4275: loss 0.077042\n",
      "batch 4276: loss 0.095986\n",
      "batch 4277: loss 0.066174\n",
      "batch 4278: loss 0.128791\n",
      "batch 4279: loss 0.011716\n",
      "batch 4280: loss 0.112596\n",
      "batch 4281: loss 0.009920\n",
      "batch 4282: loss 0.034821\n",
      "batch 4283: loss 0.076183\n",
      "batch 4284: loss 0.018071\n",
      "batch 4285: loss 0.063445\n",
      "batch 4286: loss 0.029010\n",
      "batch 4287: loss 0.055616\n",
      "batch 4288: loss 0.086149\n",
      "batch 4289: loss 0.211735\n",
      "batch 4290: loss 0.132346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4291: loss 0.068815\n",
      "batch 4292: loss 0.039047\n",
      "batch 4293: loss 0.023405\n",
      "batch 4294: loss 0.104691\n",
      "batch 4295: loss 0.079772\n",
      "batch 4296: loss 0.136593\n",
      "batch 4297: loss 0.032243\n",
      "batch 4298: loss 0.067666\n",
      "batch 4299: loss 0.012354\n",
      "batch 4300: loss 0.090996\n",
      "batch 4301: loss 0.134081\n",
      "batch 4302: loss 0.211925\n",
      "batch 4303: loss 0.062890\n",
      "batch 4304: loss 0.038661\n",
      "batch 4305: loss 0.110836\n",
      "batch 4306: loss 0.037497\n",
      "batch 4307: loss 0.095885\n",
      "batch 4308: loss 0.136808\n",
      "batch 4309: loss 0.075887\n",
      "batch 4310: loss 0.060023\n",
      "batch 4311: loss 0.063878\n",
      "batch 4312: loss 0.014937\n",
      "batch 4313: loss 0.125172\n",
      "batch 4314: loss 0.067940\n",
      "batch 4315: loss 0.078256\n",
      "batch 4316: loss 0.140122\n",
      "batch 4317: loss 0.087926\n",
      "batch 4318: loss 0.090511\n",
      "batch 4319: loss 0.092489\n",
      "batch 4320: loss 0.165469\n",
      "batch 4321: loss 0.018227\n",
      "batch 4322: loss 0.117055\n",
      "batch 4323: loss 0.022712\n",
      "batch 4324: loss 0.097761\n",
      "batch 4325: loss 0.045675\n",
      "batch 4326: loss 0.038400\n",
      "batch 4327: loss 0.037071\n",
      "batch 4328: loss 0.095158\n",
      "batch 4329: loss 0.114281\n",
      "batch 4330: loss 0.169583\n",
      "batch 4331: loss 0.089599\n",
      "batch 4332: loss 0.013907\n",
      "batch 4333: loss 0.097059\n",
      "batch 4334: loss 0.057301\n",
      "batch 4335: loss 0.109023\n",
      "batch 4336: loss 0.045298\n",
      "batch 4337: loss 0.268430\n",
      "batch 4338: loss 0.035059\n",
      "batch 4339: loss 0.170864\n",
      "batch 4340: loss 0.026562\n",
      "batch 4341: loss 0.179047\n",
      "batch 4342: loss 0.061720\n",
      "batch 4343: loss 0.223275\n",
      "batch 4344: loss 0.097804\n",
      "batch 4345: loss 0.060216\n",
      "batch 4346: loss 0.070521\n",
      "batch 4347: loss 0.033022\n",
      "batch 4348: loss 0.045411\n",
      "batch 4349: loss 0.032232\n",
      "batch 4350: loss 0.166007\n",
      "batch 4351: loss 0.031930\n",
      "batch 4352: loss 0.083600\n",
      "batch 4353: loss 0.068409\n",
      "batch 4354: loss 0.029021\n",
      "batch 4355: loss 0.063653\n",
      "batch 4356: loss 0.114615\n",
      "batch 4357: loss 0.102942\n",
      "batch 4358: loss 0.053357\n",
      "batch 4359: loss 0.064609\n",
      "batch 4360: loss 0.027008\n",
      "batch 4361: loss 0.008077\n",
      "batch 4362: loss 0.187446\n",
      "batch 4363: loss 0.023378\n",
      "batch 4364: loss 0.022119\n",
      "batch 4365: loss 0.072843\n",
      "batch 4366: loss 0.080546\n",
      "batch 4367: loss 0.102848\n",
      "batch 4368: loss 0.022273\n",
      "batch 4369: loss 0.152189\n",
      "batch 4370: loss 0.183474\n",
      "batch 4371: loss 0.023025\n",
      "batch 4372: loss 0.112874\n",
      "batch 4373: loss 0.245546\n",
      "batch 4374: loss 0.067226\n",
      "batch 4375: loss 0.118307\n",
      "batch 4376: loss 0.026911\n",
      "batch 4377: loss 0.087586\n",
      "batch 4378: loss 0.239024\n",
      "batch 4379: loss 0.202253\n",
      "batch 4380: loss 0.091991\n",
      "batch 4381: loss 0.027850\n",
      "batch 4382: loss 0.046285\n",
      "batch 4383: loss 0.011455\n",
      "batch 4384: loss 0.078683\n",
      "batch 4385: loss 0.056769\n",
      "batch 4386: loss 0.184626\n",
      "batch 4387: loss 0.218696\n",
      "batch 4388: loss 0.171367\n",
      "batch 4389: loss 0.063007\n",
      "batch 4390: loss 0.019447\n",
      "batch 4391: loss 0.067226\n",
      "batch 4392: loss 0.029318\n",
      "batch 4393: loss 0.097346\n",
      "batch 4394: loss 0.055158\n",
      "batch 4395: loss 0.116329\n",
      "batch 4396: loss 0.127157\n",
      "batch 4397: loss 0.097279\n",
      "batch 4398: loss 0.008318\n",
      "batch 4399: loss 0.148444\n",
      "batch 4400: loss 0.084872\n",
      "batch 4401: loss 0.086111\n",
      "batch 4402: loss 0.083488\n",
      "batch 4403: loss 0.027315\n",
      "batch 4404: loss 0.128048\n",
      "batch 4405: loss 0.141899\n",
      "batch 4406: loss 0.006446\n",
      "batch 4407: loss 0.024796\n",
      "batch 4408: loss 0.098141\n",
      "batch 4409: loss 0.034483\n",
      "batch 4410: loss 0.056483\n",
      "batch 4411: loss 0.028378\n",
      "batch 4412: loss 0.126699\n",
      "batch 4413: loss 0.042229\n",
      "batch 4414: loss 0.008937\n",
      "batch 4415: loss 0.055879\n",
      "batch 4416: loss 0.102884\n",
      "batch 4417: loss 0.059025\n",
      "batch 4418: loss 0.016723\n",
      "batch 4419: loss 0.169199\n",
      "batch 4420: loss 0.252538\n",
      "batch 4421: loss 0.106629\n",
      "batch 4422: loss 0.094880\n",
      "batch 4423: loss 0.071044\n",
      "batch 4424: loss 0.086468\n",
      "batch 4425: loss 0.077118\n",
      "batch 4426: loss 0.048512\n",
      "batch 4427: loss 0.026675\n",
      "batch 4428: loss 0.043907\n",
      "batch 4429: loss 0.083636\n",
      "batch 4430: loss 0.057259\n",
      "batch 4431: loss 0.104590\n",
      "batch 4432: loss 0.026771\n",
      "batch 4433: loss 0.023142\n",
      "batch 4434: loss 0.121172\n",
      "batch 4435: loss 0.268845\n",
      "batch 4436: loss 0.106753\n",
      "batch 4437: loss 0.027280\n",
      "batch 4438: loss 0.070234\n",
      "batch 4439: loss 0.052760\n",
      "batch 4440: loss 0.021144\n",
      "batch 4441: loss 0.026856\n",
      "batch 4442: loss 0.082250\n",
      "batch 4443: loss 0.066483\n",
      "batch 4444: loss 0.260595\n",
      "batch 4445: loss 0.179115\n",
      "batch 4446: loss 0.031899\n",
      "batch 4447: loss 0.074589\n",
      "batch 4448: loss 0.050241\n",
      "batch 4449: loss 0.046027\n",
      "batch 4450: loss 0.030330\n",
      "batch 4451: loss 0.151485\n",
      "batch 4452: loss 0.019538\n",
      "batch 4453: loss 0.032694\n",
      "batch 4454: loss 0.183696\n",
      "batch 4455: loss 0.315886\n",
      "batch 4456: loss 0.177596\n",
      "batch 4457: loss 0.041278\n",
      "batch 4458: loss 0.011418\n",
      "batch 4459: loss 0.071359\n",
      "batch 4460: loss 0.065136\n",
      "batch 4461: loss 0.145216\n",
      "batch 4462: loss 0.016985\n",
      "batch 4463: loss 0.041272\n",
      "batch 4464: loss 0.171266\n",
      "batch 4465: loss 0.062707\n",
      "batch 4466: loss 0.119410\n",
      "batch 4467: loss 0.076474\n",
      "batch 4468: loss 0.074798\n",
      "batch 4469: loss 0.089968\n",
      "batch 4470: loss 0.072467\n",
      "batch 4471: loss 0.028869\n",
      "batch 4472: loss 0.116782\n",
      "batch 4473: loss 0.077391\n",
      "batch 4474: loss 0.086007\n",
      "batch 4475: loss 0.009413\n",
      "batch 4476: loss 0.115480\n",
      "batch 4477: loss 0.062596\n",
      "batch 4478: loss 0.040194\n",
      "batch 4479: loss 0.035548\n",
      "batch 4480: loss 0.092395\n",
      "batch 4481: loss 0.170348\n",
      "batch 4482: loss 0.063333\n",
      "batch 4483: loss 0.053002\n",
      "batch 4484: loss 0.138049\n",
      "batch 4485: loss 0.135291\n",
      "batch 4486: loss 0.027417\n",
      "batch 4487: loss 0.056850\n",
      "batch 4488: loss 0.051176\n",
      "batch 4489: loss 0.103649\n",
      "batch 4490: loss 0.113062\n",
      "batch 4491: loss 0.043014\n",
      "batch 4492: loss 0.087415\n",
      "batch 4493: loss 0.053100\n",
      "batch 4494: loss 0.053471\n",
      "batch 4495: loss 0.142764\n",
      "batch 4496: loss 0.055569\n",
      "batch 4497: loss 0.018587\n",
      "batch 4498: loss 0.171111\n",
      "batch 4499: loss 0.084273\n",
      "batch 4500: loss 0.020747\n",
      "batch 4501: loss 0.013139\n",
      "batch 4502: loss 0.119149\n",
      "batch 4503: loss 0.059154\n",
      "batch 4504: loss 0.078407\n",
      "batch 4505: loss 0.126003\n",
      "batch 4506: loss 0.032318\n",
      "batch 4507: loss 0.045448\n",
      "batch 4508: loss 0.037591\n",
      "batch 4509: loss 0.012857\n",
      "batch 4510: loss 0.103798\n",
      "batch 4511: loss 0.041849\n",
      "batch 4512: loss 0.153348\n",
      "batch 4513: loss 0.058458\n",
      "batch 4514: loss 0.049953\n",
      "batch 4515: loss 0.050886\n",
      "batch 4516: loss 0.045301\n",
      "batch 4517: loss 0.036806\n",
      "batch 4518: loss 0.061742\n",
      "batch 4519: loss 0.195347\n",
      "batch 4520: loss 0.048311\n",
      "batch 4521: loss 0.052044\n",
      "batch 4522: loss 0.040790\n",
      "batch 4523: loss 0.056064\n",
      "batch 4524: loss 0.050743\n",
      "batch 4525: loss 0.166817\n",
      "batch 4526: loss 0.003433\n",
      "batch 4527: loss 0.155304\n",
      "batch 4528: loss 0.079517\n",
      "batch 4529: loss 0.023688\n",
      "batch 4530: loss 0.106487\n",
      "batch 4531: loss 0.103714\n",
      "batch 4532: loss 0.191110\n",
      "batch 4533: loss 0.149456\n",
      "batch 4534: loss 0.010268\n",
      "batch 4535: loss 0.089527\n",
      "batch 4536: loss 0.067946\n",
      "batch 4537: loss 0.136349\n",
      "batch 4538: loss 0.045086\n",
      "batch 4539: loss 0.015207\n",
      "batch 4540: loss 0.027267\n",
      "batch 4541: loss 0.093325\n",
      "batch 4542: loss 0.071572\n",
      "batch 4543: loss 0.018650\n",
      "batch 4544: loss 0.074848\n",
      "batch 4545: loss 0.043116\n",
      "batch 4546: loss 0.053715\n",
      "batch 4547: loss 0.066955\n",
      "batch 4548: loss 0.073593\n",
      "batch 4549: loss 0.081458\n",
      "batch 4550: loss 0.207032\n",
      "batch 4551: loss 0.016815\n",
      "batch 4552: loss 0.035801\n",
      "batch 4553: loss 0.201529\n",
      "batch 4554: loss 0.020287\n",
      "batch 4555: loss 0.147000\n",
      "batch 4556: loss 0.128492\n",
      "batch 4557: loss 0.120156\n",
      "batch 4558: loss 0.067763\n",
      "batch 4559: loss 0.041291\n",
      "batch 4560: loss 0.094025\n",
      "batch 4561: loss 0.202849\n",
      "batch 4562: loss 0.034324\n",
      "batch 4563: loss 0.044293\n",
      "batch 4564: loss 0.129189\n",
      "batch 4565: loss 0.118369\n",
      "batch 4566: loss 0.241451\n",
      "batch 4567: loss 0.011331\n",
      "batch 4568: loss 0.036048\n",
      "batch 4569: loss 0.020384\n",
      "batch 4570: loss 0.019841\n",
      "batch 4571: loss 0.062068\n",
      "batch 4572: loss 0.056024\n",
      "batch 4573: loss 0.057072\n",
      "batch 4574: loss 0.030139\n",
      "batch 4575: loss 0.038944\n",
      "batch 4576: loss 0.091079\n",
      "batch 4577: loss 0.034532\n",
      "batch 4578: loss 0.195599\n",
      "batch 4579: loss 0.032960\n",
      "batch 4580: loss 0.046043\n",
      "batch 4581: loss 0.035045\n",
      "batch 4582: loss 0.073321\n",
      "batch 4583: loss 0.033304\n",
      "batch 4584: loss 0.124680\n",
      "batch 4585: loss 0.154801\n",
      "batch 4586: loss 0.047300\n",
      "batch 4587: loss 0.101198\n",
      "batch 4588: loss 0.020957\n",
      "batch 4589: loss 0.061739\n",
      "batch 4590: loss 0.044572\n",
      "batch 4591: loss 0.054755\n",
      "batch 4592: loss 0.030468\n",
      "batch 4593: loss 0.029763\n",
      "batch 4594: loss 0.068691\n",
      "batch 4595: loss 0.061678\n",
      "batch 4596: loss 0.154411\n",
      "batch 4597: loss 0.046919\n",
      "batch 4598: loss 0.029532\n",
      "batch 4599: loss 0.119919\n",
      "batch 4600: loss 0.021466\n",
      "batch 4601: loss 0.028174\n",
      "batch 4602: loss 0.022796\n",
      "batch 4603: loss 0.132154\n",
      "batch 4604: loss 0.069599\n",
      "batch 4605: loss 0.016597\n",
      "batch 4606: loss 0.110508\n",
      "batch 4607: loss 0.045888\n",
      "batch 4608: loss 0.012243\n",
      "batch 4609: loss 0.017707\n",
      "batch 4610: loss 0.036591\n",
      "batch 4611: loss 0.025387\n",
      "batch 4612: loss 0.038444\n",
      "batch 4613: loss 0.052148\n",
      "batch 4614: loss 0.033073\n",
      "batch 4615: loss 0.091372\n",
      "batch 4616: loss 0.030312\n",
      "batch 4617: loss 0.051692\n",
      "batch 4618: loss 0.104158\n",
      "batch 4619: loss 0.026539\n",
      "batch 4620: loss 0.116490\n",
      "batch 4621: loss 0.117680\n",
      "batch 4622: loss 0.001781\n",
      "batch 4623: loss 0.079823\n",
      "batch 4624: loss 0.141902\n",
      "batch 4625: loss 0.014716\n",
      "batch 4626: loss 0.100353\n",
      "batch 4627: loss 0.201265\n",
      "batch 4628: loss 0.051994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4629: loss 0.085330\n",
      "batch 4630: loss 0.148907\n",
      "batch 4631: loss 0.166689\n",
      "batch 4632: loss 0.079469\n",
      "batch 4633: loss 0.068460\n",
      "batch 4634: loss 0.196799\n",
      "batch 4635: loss 0.074109\n",
      "batch 4636: loss 0.043014\n",
      "batch 4637: loss 0.049414\n",
      "batch 4638: loss 0.061597\n",
      "batch 4639: loss 0.054588\n",
      "batch 4640: loss 0.153458\n",
      "batch 4641: loss 0.025826\n",
      "batch 4642: loss 0.243437\n",
      "batch 4643: loss 0.152457\n",
      "batch 4644: loss 0.021538\n",
      "batch 4645: loss 0.086030\n",
      "batch 4646: loss 0.026500\n",
      "batch 4647: loss 0.054665\n",
      "batch 4648: loss 0.068525\n",
      "batch 4649: loss 0.148917\n",
      "batch 4650: loss 0.189607\n",
      "batch 4651: loss 0.125152\n",
      "batch 4652: loss 0.041981\n",
      "batch 4653: loss 0.168390\n",
      "batch 4654: loss 0.089323\n",
      "batch 4655: loss 0.043213\n",
      "batch 4656: loss 0.142522\n",
      "batch 4657: loss 0.065116\n",
      "batch 4658: loss 0.125994\n",
      "batch 4659: loss 0.046073\n",
      "batch 4660: loss 0.026101\n",
      "batch 4661: loss 0.092264\n",
      "batch 4662: loss 0.050095\n",
      "batch 4663: loss 0.055037\n",
      "batch 4664: loss 0.072047\n",
      "batch 4665: loss 0.054269\n",
      "batch 4666: loss 0.046013\n",
      "batch 4667: loss 0.122878\n",
      "batch 4668: loss 0.015863\n",
      "batch 4669: loss 0.190988\n",
      "batch 4670: loss 0.015049\n",
      "batch 4671: loss 0.023233\n",
      "batch 4672: loss 0.086006\n",
      "batch 4673: loss 0.060024\n",
      "batch 4674: loss 0.109103\n",
      "batch 4675: loss 0.048579\n",
      "batch 4676: loss 0.067431\n",
      "batch 4677: loss 0.069599\n",
      "batch 4678: loss 0.014452\n",
      "batch 4679: loss 0.063226\n",
      "batch 4680: loss 0.125250\n",
      "batch 4681: loss 0.020856\n",
      "batch 4682: loss 0.038144\n",
      "batch 4683: loss 0.152076\n",
      "batch 4684: loss 0.030204\n",
      "batch 4685: loss 0.020641\n",
      "batch 4686: loss 0.024516\n",
      "batch 4687: loss 0.064799\n",
      "batch 4688: loss 0.121231\n",
      "batch 4689: loss 0.025161\n",
      "batch 4690: loss 0.076502\n",
      "batch 4691: loss 0.019931\n",
      "batch 4692: loss 0.022762\n",
      "batch 4693: loss 0.043063\n",
      "batch 4694: loss 0.016241\n",
      "batch 4695: loss 0.122422\n",
      "batch 4696: loss 0.098154\n",
      "batch 4697: loss 0.037171\n",
      "batch 4698: loss 0.094677\n",
      "batch 4699: loss 0.039422\n",
      "batch 4700: loss 0.089507\n",
      "batch 4701: loss 0.070686\n",
      "batch 4702: loss 0.008267\n",
      "batch 4703: loss 0.030897\n",
      "batch 4704: loss 0.115071\n",
      "batch 4705: loss 0.240196\n",
      "batch 4706: loss 0.019948\n",
      "batch 4707: loss 0.015751\n",
      "batch 4708: loss 0.059631\n",
      "batch 4709: loss 0.123881\n",
      "batch 4710: loss 0.090370\n",
      "batch 4711: loss 0.010363\n",
      "batch 4712: loss 0.041997\n",
      "batch 4713: loss 0.072657\n",
      "batch 4714: loss 0.058240\n",
      "batch 4715: loss 0.171044\n",
      "batch 4716: loss 0.065055\n",
      "batch 4717: loss 0.048205\n",
      "batch 4718: loss 0.064338\n",
      "batch 4719: loss 0.097355\n",
      "batch 4720: loss 0.116375\n",
      "batch 4721: loss 0.179713\n",
      "batch 4722: loss 0.007818\n",
      "batch 4723: loss 0.075653\n",
      "batch 4724: loss 0.019897\n",
      "batch 4725: loss 0.182176\n",
      "batch 4726: loss 0.164873\n",
      "batch 4727: loss 0.061702\n",
      "batch 4728: loss 0.119313\n",
      "batch 4729: loss 0.050056\n",
      "batch 4730: loss 0.148177\n",
      "batch 4731: loss 0.050765\n",
      "batch 4732: loss 0.072142\n",
      "batch 4733: loss 0.037889\n",
      "batch 4734: loss 0.075731\n",
      "batch 4735: loss 0.323727\n",
      "batch 4736: loss 0.049562\n",
      "batch 4737: loss 0.039083\n",
      "batch 4738: loss 0.107819\n",
      "batch 4739: loss 0.067612\n",
      "batch 4740: loss 0.013418\n",
      "batch 4741: loss 0.034105\n",
      "batch 4742: loss 0.264557\n",
      "batch 4743: loss 0.030287\n",
      "batch 4744: loss 0.139507\n",
      "batch 4745: loss 0.010970\n",
      "batch 4746: loss 0.044321\n",
      "batch 4747: loss 0.023030\n",
      "batch 4748: loss 0.025101\n",
      "batch 4749: loss 0.089401\n",
      "batch 4750: loss 0.211868\n",
      "batch 4751: loss 0.060062\n",
      "batch 4752: loss 0.166643\n",
      "batch 4753: loss 0.089997\n",
      "batch 4754: loss 0.074370\n",
      "batch 4755: loss 0.091266\n",
      "batch 4756: loss 0.065883\n",
      "batch 4757: loss 0.044606\n",
      "batch 4758: loss 0.103201\n",
      "batch 4759: loss 0.127290\n",
      "batch 4760: loss 0.081476\n",
      "batch 4761: loss 0.036001\n",
      "batch 4762: loss 0.040678\n",
      "batch 4763: loss 0.030915\n",
      "batch 4764: loss 0.029172\n",
      "batch 4765: loss 0.155900\n",
      "batch 4766: loss 0.014513\n",
      "batch 4767: loss 0.077311\n",
      "batch 4768: loss 0.045812\n",
      "batch 4769: loss 0.142312\n",
      "batch 4770: loss 0.025830\n",
      "batch 4771: loss 0.101529\n",
      "batch 4772: loss 0.122437\n",
      "batch 4773: loss 0.015859\n",
      "batch 4774: loss 0.039694\n",
      "batch 4775: loss 0.098144\n",
      "batch 4776: loss 0.010375\n",
      "batch 4777: loss 0.137229\n",
      "batch 4778: loss 0.152501\n",
      "batch 4779: loss 0.017740\n",
      "batch 4780: loss 0.112748\n",
      "batch 4781: loss 0.102307\n",
      "batch 4782: loss 0.248703\n",
      "batch 4783: loss 0.045408\n",
      "batch 4784: loss 0.020223\n",
      "batch 4785: loss 0.023978\n",
      "batch 4786: loss 0.139866\n",
      "batch 4787: loss 0.024729\n",
      "batch 4788: loss 0.021502\n",
      "batch 4789: loss 0.114501\n",
      "batch 4790: loss 0.078487\n",
      "batch 4791: loss 0.114445\n",
      "batch 4792: loss 0.061350\n",
      "batch 4793: loss 0.153873\n",
      "batch 4794: loss 0.160033\n",
      "batch 4795: loss 0.055597\n",
      "batch 4796: loss 0.006664\n",
      "batch 4797: loss 0.007562\n",
      "batch 4798: loss 0.010792\n",
      "batch 4799: loss 0.117150\n",
      "batch 4800: loss 0.036965\n",
      "batch 4801: loss 0.068856\n",
      "batch 4802: loss 0.062070\n",
      "batch 4803: loss 0.114687\n",
      "batch 4804: loss 0.042999\n",
      "batch 4805: loss 0.021419\n",
      "batch 4806: loss 0.165086\n",
      "batch 4807: loss 0.119936\n",
      "batch 4808: loss 0.058447\n",
      "batch 4809: loss 0.083605\n",
      "batch 4810: loss 0.238612\n",
      "batch 4811: loss 0.121274\n",
      "batch 4812: loss 0.068585\n",
      "batch 4813: loss 0.022618\n",
      "batch 4814: loss 0.047054\n",
      "batch 4815: loss 0.062997\n",
      "batch 4816: loss 0.022874\n",
      "batch 4817: loss 0.193966\n",
      "batch 4818: loss 0.037075\n",
      "batch 4819: loss 0.018467\n",
      "batch 4820: loss 0.056464\n",
      "batch 4821: loss 0.025456\n",
      "batch 4822: loss 0.055153\n",
      "batch 4823: loss 0.084942\n",
      "batch 4824: loss 0.058573\n",
      "batch 4825: loss 0.027440\n",
      "batch 4826: loss 0.083037\n",
      "batch 4827: loss 0.152202\n",
      "batch 4828: loss 0.071144\n",
      "batch 4829: loss 0.080926\n",
      "batch 4830: loss 0.060285\n",
      "batch 4831: loss 0.066264\n",
      "batch 4832: loss 0.099886\n",
      "batch 4833: loss 0.255141\n",
      "batch 4834: loss 0.023561\n",
      "batch 4835: loss 0.020056\n",
      "batch 4836: loss 0.011652\n",
      "batch 4837: loss 0.175980\n",
      "batch 4838: loss 0.045157\n",
      "batch 4839: loss 0.014496\n",
      "batch 4840: loss 0.027522\n",
      "batch 4841: loss 0.071746\n",
      "batch 4842: loss 0.008394\n",
      "batch 4843: loss 0.115783\n",
      "batch 4844: loss 0.172689\n",
      "batch 4845: loss 0.034140\n",
      "batch 4846: loss 0.066987\n",
      "batch 4847: loss 0.050929\n",
      "batch 4848: loss 0.010373\n",
      "batch 4849: loss 0.102054\n",
      "batch 4850: loss 0.173174\n",
      "batch 4851: loss 0.099390\n",
      "batch 4852: loss 0.045052\n",
      "batch 4853: loss 0.077278\n",
      "batch 4854: loss 0.051773\n",
      "batch 4855: loss 0.035964\n",
      "batch 4856: loss 0.010303\n",
      "batch 4857: loss 0.106764\n",
      "batch 4858: loss 0.005143\n",
      "batch 4859: loss 0.092422\n",
      "batch 4860: loss 0.129912\n",
      "batch 4861: loss 0.032073\n",
      "batch 4862: loss 0.157863\n",
      "batch 4863: loss 0.067463\n",
      "batch 4864: loss 0.180658\n",
      "batch 4865: loss 0.077037\n",
      "batch 4866: loss 0.038492\n",
      "batch 4867: loss 0.051680\n",
      "batch 4868: loss 0.045901\n",
      "batch 4869: loss 0.033496\n",
      "batch 4870: loss 0.058979\n",
      "batch 4871: loss 0.114383\n",
      "batch 4872: loss 0.027573\n",
      "batch 4873: loss 0.044623\n",
      "batch 4874: loss 0.039851\n",
      "batch 4875: loss 0.033498\n",
      "batch 4876: loss 0.018439\n",
      "batch 4877: loss 0.097960\n",
      "batch 4878: loss 0.047072\n",
      "batch 4879: loss 0.034621\n",
      "batch 4880: loss 0.025552\n",
      "batch 4881: loss 0.065861\n",
      "batch 4882: loss 0.279011\n",
      "batch 4883: loss 0.014466\n",
      "batch 4884: loss 0.108775\n",
      "batch 4885: loss 0.012838\n",
      "batch 4886: loss 0.059926\n",
      "batch 4887: loss 0.042326\n",
      "batch 4888: loss 0.079337\n",
      "batch 4889: loss 0.058050\n",
      "batch 4890: loss 0.134554\n",
      "batch 4891: loss 0.044721\n",
      "batch 4892: loss 0.270812\n",
      "batch 4893: loss 0.260515\n",
      "batch 4894: loss 0.129806\n",
      "batch 4895: loss 0.085606\n",
      "batch 4896: loss 0.115460\n",
      "batch 4897: loss 0.039214\n",
      "batch 4898: loss 0.189231\n",
      "batch 4899: loss 0.070817\n",
      "batch 4900: loss 0.047552\n",
      "batch 4901: loss 0.012890\n",
      "batch 4902: loss 0.089146\n",
      "batch 4903: loss 0.058202\n",
      "batch 4904: loss 0.047047\n",
      "batch 4905: loss 0.102431\n",
      "batch 4906: loss 0.045979\n",
      "batch 4907: loss 0.064969\n",
      "batch 4908: loss 0.086356\n",
      "batch 4909: loss 0.050053\n",
      "batch 4910: loss 0.114482\n",
      "batch 4911: loss 0.058674\n",
      "batch 4912: loss 0.353856\n",
      "batch 4913: loss 0.078715\n",
      "batch 4914: loss 0.066550\n",
      "batch 4915: loss 0.107273\n",
      "batch 4916: loss 0.043746\n",
      "batch 4917: loss 0.019421\n",
      "batch 4918: loss 0.026961\n",
      "batch 4919: loss 0.153693\n",
      "batch 4920: loss 0.019965\n",
      "batch 4921: loss 0.068488\n",
      "batch 4922: loss 0.049530\n",
      "batch 4923: loss 0.049691\n",
      "batch 4924: loss 0.065501\n",
      "batch 4925: loss 0.068601\n",
      "batch 4926: loss 0.101701\n",
      "batch 4927: loss 0.021514\n",
      "batch 4928: loss 0.033997\n",
      "batch 4929: loss 0.177932\n",
      "batch 4930: loss 0.100114\n",
      "batch 4931: loss 0.072060\n",
      "batch 4932: loss 0.014915\n",
      "batch 4933: loss 0.044067\n",
      "batch 4934: loss 0.088639\n",
      "batch 4935: loss 0.019827\n",
      "batch 4936: loss 0.016641\n",
      "batch 4937: loss 0.014009\n",
      "batch 4938: loss 0.106299\n",
      "batch 4939: loss 0.099911\n",
      "batch 4940: loss 0.016236\n",
      "batch 4941: loss 0.142781\n",
      "batch 4942: loss 0.026085\n",
      "batch 4943: loss 0.037182\n",
      "batch 4944: loss 0.050026\n",
      "batch 4945: loss 0.085364\n",
      "batch 4946: loss 0.070149\n",
      "batch 4947: loss 0.033565\n",
      "batch 4948: loss 0.066942\n",
      "batch 4949: loss 0.062136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4950: loss 0.065326\n",
      "batch 4951: loss 0.091914\n",
      "batch 4952: loss 0.010241\n",
      "batch 4953: loss 0.070059\n",
      "batch 4954: loss 0.210465\n",
      "batch 4955: loss 0.020213\n",
      "batch 4956: loss 0.036556\n",
      "batch 4957: loss 0.033881\n",
      "batch 4958: loss 0.065653\n",
      "batch 4959: loss 0.150886\n",
      "batch 4960: loss 0.063256\n",
      "batch 4961: loss 0.016677\n",
      "batch 4962: loss 0.025026\n",
      "batch 4963: loss 0.012877\n",
      "batch 4964: loss 0.166644\n",
      "batch 4965: loss 0.061532\n",
      "batch 4966: loss 0.005842\n",
      "batch 4967: loss 0.166808\n",
      "batch 4968: loss 0.080744\n",
      "batch 4969: loss 0.010656\n",
      "batch 4970: loss 0.131685\n",
      "batch 4971: loss 0.013739\n",
      "batch 4972: loss 0.091181\n",
      "batch 4973: loss 0.042109\n",
      "batch 4974: loss 0.192586\n",
      "batch 4975: loss 0.017975\n",
      "batch 4976: loss 0.069225\n",
      "batch 4977: loss 0.099964\n",
      "batch 4978: loss 0.132273\n",
      "batch 4979: loss 0.071527\n",
      "batch 4980: loss 0.326163\n",
      "batch 4981: loss 0.021273\n",
      "batch 4982: loss 0.066881\n",
      "batch 4983: loss 0.022290\n",
      "batch 4984: loss 0.079827\n",
      "batch 4985: loss 0.042479\n",
      "batch 4986: loss 0.015993\n",
      "batch 4987: loss 0.161107\n",
      "batch 4988: loss 0.039043\n",
      "batch 4989: loss 0.084857\n",
      "batch 4990: loss 0.018057\n",
      "batch 4991: loss 0.032372\n",
      "batch 4992: loss 0.150797\n",
      "batch 4993: loss 0.019012\n",
      "batch 4994: loss 0.045017\n",
      "batch 4995: loss 0.221572\n",
      "batch 4996: loss 0.046097\n",
      "batch 4997: loss 0.043070\n",
      "batch 4998: loss 0.043266\n",
      "batch 4999: loss 0.045288\n",
      "batch 5000: loss 0.061865\n",
      "batch 5001: loss 0.088143\n",
      "batch 5002: loss 0.015595\n",
      "batch 5003: loss 0.050401\n",
      "batch 5004: loss 0.123072\n",
      "batch 5005: loss 0.086020\n",
      "batch 5006: loss 0.040283\n",
      "batch 5007: loss 0.054815\n",
      "batch 5008: loss 0.043246\n",
      "batch 5009: loss 0.165823\n",
      "batch 5010: loss 0.038146\n",
      "batch 5011: loss 0.089473\n",
      "batch 5012: loss 0.022626\n",
      "batch 5013: loss 0.081907\n",
      "batch 5014: loss 0.133827\n",
      "batch 5015: loss 0.030535\n",
      "batch 5016: loss 0.020672\n",
      "batch 5017: loss 0.030233\n",
      "batch 5018: loss 0.101798\n",
      "batch 5019: loss 0.143326\n",
      "batch 5020: loss 0.029131\n",
      "batch 5021: loss 0.076291\n",
      "batch 5022: loss 0.016389\n",
      "batch 5023: loss 0.036150\n",
      "batch 5024: loss 0.059767\n",
      "batch 5025: loss 0.149948\n",
      "batch 5026: loss 0.049135\n",
      "batch 5027: loss 0.138401\n",
      "batch 5028: loss 0.090439\n",
      "batch 5029: loss 0.103650\n",
      "batch 5030: loss 0.028487\n",
      "batch 5031: loss 0.112498\n",
      "batch 5032: loss 0.160489\n",
      "batch 5033: loss 0.127898\n",
      "batch 5034: loss 0.008255\n",
      "batch 5035: loss 0.021748\n",
      "batch 5036: loss 0.026373\n",
      "batch 5037: loss 0.048149\n",
      "batch 5038: loss 0.221794\n",
      "batch 5039: loss 0.110359\n",
      "batch 5040: loss 0.048897\n",
      "batch 5041: loss 0.016127\n",
      "batch 5042: loss 0.071606\n",
      "batch 5043: loss 0.030800\n",
      "batch 5044: loss 0.083759\n",
      "batch 5045: loss 0.079385\n",
      "batch 5046: loss 0.022467\n",
      "batch 5047: loss 0.254253\n",
      "batch 5048: loss 0.040752\n",
      "batch 5049: loss 0.028680\n",
      "batch 5050: loss 0.077988\n",
      "batch 5051: loss 0.078714\n",
      "batch 5052: loss 0.051321\n",
      "batch 5053: loss 0.026623\n",
      "batch 5054: loss 0.039486\n",
      "batch 5055: loss 0.014442\n",
      "batch 5056: loss 0.030023\n",
      "batch 5057: loss 0.034586\n",
      "batch 5058: loss 0.111162\n",
      "batch 5059: loss 0.070869\n",
      "batch 5060: loss 0.049143\n",
      "batch 5061: loss 0.081322\n",
      "batch 5062: loss 0.058956\n",
      "batch 5063: loss 0.096848\n",
      "batch 5064: loss 0.030508\n",
      "batch 5065: loss 0.028884\n",
      "batch 5066: loss 0.101641\n",
      "batch 5067: loss 0.023436\n",
      "batch 5068: loss 0.008159\n",
      "batch 5069: loss 0.080655\n",
      "batch 5070: loss 0.012752\n",
      "batch 5071: loss 0.028851\n",
      "batch 5072: loss 0.144086\n",
      "batch 5073: loss 0.075330\n",
      "batch 5074: loss 0.014651\n",
      "batch 5075: loss 0.031931\n",
      "batch 5076: loss 0.082480\n",
      "batch 5077: loss 0.239352\n",
      "batch 5078: loss 0.035694\n",
      "batch 5079: loss 0.093004\n",
      "batch 5080: loss 0.055636\n",
      "batch 5081: loss 0.035836\n",
      "batch 5082: loss 0.045941\n",
      "batch 5083: loss 0.033418\n",
      "batch 5084: loss 0.016216\n",
      "batch 5085: loss 0.068330\n",
      "batch 5086: loss 0.063738\n",
      "batch 5087: loss 0.031812\n",
      "batch 5088: loss 0.140909\n",
      "batch 5089: loss 0.059224\n",
      "batch 5090: loss 0.065632\n",
      "batch 5091: loss 0.121058\n",
      "batch 5092: loss 0.065975\n",
      "batch 5093: loss 0.006233\n",
      "batch 5094: loss 0.024843\n",
      "batch 5095: loss 0.053675\n",
      "batch 5096: loss 0.062936\n",
      "batch 5097: loss 0.137504\n",
      "batch 5098: loss 0.076040\n",
      "batch 5099: loss 0.062569\n",
      "batch 5100: loss 0.036180\n",
      "batch 5101: loss 0.027722\n",
      "batch 5102: loss 0.014752\n",
      "batch 5103: loss 0.046743\n",
      "batch 5104: loss 0.036252\n",
      "batch 5105: loss 0.111666\n",
      "batch 5106: loss 0.036866\n",
      "batch 5107: loss 0.159357\n",
      "batch 5108: loss 0.085213\n",
      "batch 5109: loss 0.059343\n",
      "batch 5110: loss 0.018828\n",
      "batch 5111: loss 0.264130\n",
      "batch 5112: loss 0.022911\n",
      "batch 5113: loss 0.085462\n",
      "batch 5114: loss 0.044804\n",
      "batch 5115: loss 0.123500\n",
      "batch 5116: loss 0.258589\n",
      "batch 5117: loss 0.079994\n",
      "batch 5118: loss 0.027392\n",
      "batch 5119: loss 0.016999\n",
      "batch 5120: loss 0.075115\n",
      "batch 5121: loss 0.169389\n",
      "batch 5122: loss 0.114863\n",
      "batch 5123: loss 0.382319\n",
      "batch 5124: loss 0.024873\n",
      "batch 5125: loss 0.035778\n",
      "batch 5126: loss 0.125292\n",
      "batch 5127: loss 0.407493\n",
      "batch 5128: loss 0.073823\n",
      "batch 5129: loss 0.073330\n",
      "batch 5130: loss 0.014804\n",
      "batch 5131: loss 0.064277\n",
      "batch 5132: loss 0.083060\n",
      "batch 5133: loss 0.029291\n",
      "batch 5134: loss 0.079953\n",
      "batch 5135: loss 0.090058\n",
      "batch 5136: loss 0.138596\n",
      "batch 5137: loss 0.101333\n",
      "batch 5138: loss 0.147034\n",
      "batch 5139: loss 0.027945\n",
      "batch 5140: loss 0.137399\n",
      "batch 5141: loss 0.085919\n",
      "batch 5142: loss 0.095108\n",
      "batch 5143: loss 0.046813\n",
      "batch 5144: loss 0.039405\n",
      "batch 5145: loss 0.027906\n",
      "batch 5146: loss 0.013430\n",
      "batch 5147: loss 0.038909\n",
      "batch 5148: loss 0.101937\n",
      "batch 5149: loss 0.092992\n",
      "batch 5150: loss 0.180833\n",
      "batch 5151: loss 0.155260\n",
      "batch 5152: loss 0.026071\n",
      "batch 5153: loss 0.132328\n",
      "batch 5154: loss 0.089140\n",
      "batch 5155: loss 0.157345\n",
      "batch 5156: loss 0.037180\n",
      "batch 5157: loss 0.095561\n",
      "batch 5158: loss 0.048409\n",
      "batch 5159: loss 0.081380\n",
      "batch 5160: loss 0.038387\n",
      "batch 5161: loss 0.113500\n",
      "batch 5162: loss 0.009526\n",
      "batch 5163: loss 0.005466\n",
      "batch 5164: loss 0.042987\n",
      "batch 5165: loss 0.143550\n",
      "batch 5166: loss 0.149399\n",
      "batch 5167: loss 0.018929\n",
      "batch 5168: loss 0.068821\n",
      "batch 5169: loss 0.198917\n",
      "batch 5170: loss 0.082960\n",
      "batch 5171: loss 0.096166\n",
      "batch 5172: loss 0.106591\n",
      "batch 5173: loss 0.144999\n",
      "batch 5174: loss 0.175633\n",
      "batch 5175: loss 0.048621\n",
      "batch 5176: loss 0.082780\n",
      "batch 5177: loss 0.016207\n",
      "batch 5178: loss 0.068908\n",
      "batch 5179: loss 0.026963\n",
      "batch 5180: loss 0.205704\n",
      "batch 5181: loss 0.049383\n",
      "batch 5182: loss 0.065992\n",
      "batch 5183: loss 0.043236\n",
      "batch 5184: loss 0.003629\n",
      "batch 5185: loss 0.029044\n",
      "batch 5186: loss 0.209674\n",
      "batch 5187: loss 0.062614\n",
      "batch 5188: loss 0.032783\n",
      "batch 5189: loss 0.090861\n",
      "batch 5190: loss 0.017754\n",
      "batch 5191: loss 0.013054\n",
      "batch 5192: loss 0.115609\n",
      "batch 5193: loss 0.084509\n",
      "batch 5194: loss 0.116496\n",
      "batch 5195: loss 0.065922\n",
      "batch 5196: loss 0.099140\n",
      "batch 5197: loss 0.032154\n",
      "batch 5198: loss 0.058542\n",
      "batch 5199: loss 0.028703\n",
      "batch 5200: loss 0.129781\n",
      "batch 5201: loss 0.035059\n",
      "batch 5202: loss 0.028571\n",
      "batch 5203: loss 0.077761\n",
      "batch 5204: loss 0.014856\n",
      "batch 5205: loss 0.082898\n",
      "batch 5206: loss 0.175619\n",
      "batch 5207: loss 0.024798\n",
      "batch 5208: loss 0.020587\n",
      "batch 5209: loss 0.099531\n",
      "batch 5210: loss 0.064206\n",
      "batch 5211: loss 0.097289\n",
      "batch 5212: loss 0.086449\n",
      "batch 5213: loss 0.054540\n",
      "batch 5214: loss 0.053043\n",
      "batch 5215: loss 0.009541\n",
      "batch 5216: loss 0.087399\n",
      "batch 5217: loss 0.022124\n",
      "batch 5218: loss 0.145844\n",
      "batch 5219: loss 0.121786\n",
      "batch 5220: loss 0.072016\n",
      "batch 5221: loss 0.076126\n",
      "batch 5222: loss 0.069798\n",
      "batch 5223: loss 0.216358\n",
      "batch 5224: loss 0.024525\n",
      "batch 5225: loss 0.017070\n",
      "batch 5226: loss 0.126389\n",
      "batch 5227: loss 0.050285\n",
      "batch 5228: loss 0.088431\n",
      "batch 5229: loss 0.020864\n",
      "batch 5230: loss 0.036621\n",
      "batch 5231: loss 0.015110\n",
      "batch 5232: loss 0.032752\n",
      "batch 5233: loss 0.059246\n",
      "batch 5234: loss 0.172728\n",
      "batch 5235: loss 0.180676\n",
      "batch 5236: loss 0.013402\n",
      "batch 5237: loss 0.095755\n",
      "batch 5238: loss 0.061845\n",
      "batch 5239: loss 0.083834\n",
      "batch 5240: loss 0.020018\n",
      "batch 5241: loss 0.037391\n",
      "batch 5242: loss 0.088821\n",
      "batch 5243: loss 0.111164\n",
      "batch 5244: loss 0.011345\n",
      "batch 5245: loss 0.047057\n",
      "batch 5246: loss 0.026162\n",
      "batch 5247: loss 0.109495\n",
      "batch 5248: loss 0.039392\n",
      "batch 5249: loss 0.009013\n",
      "batch 5250: loss 0.031640\n",
      "batch 5251: loss 0.032777\n",
      "batch 5252: loss 0.018987\n",
      "batch 5253: loss 0.075201\n",
      "batch 5254: loss 0.143834\n",
      "batch 5255: loss 0.012637\n",
      "batch 5256: loss 0.030485\n",
      "batch 5257: loss 0.056437\n",
      "batch 5258: loss 0.212425\n",
      "batch 5259: loss 0.095673\n",
      "batch 5260: loss 0.060827\n",
      "batch 5261: loss 0.043812\n",
      "batch 5262: loss 0.038106\n",
      "batch 5263: loss 0.153790\n",
      "batch 5264: loss 0.121549\n",
      "batch 5265: loss 0.116330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5266: loss 0.032471\n",
      "batch 5267: loss 0.037286\n",
      "batch 5268: loss 0.028106\n",
      "batch 5269: loss 0.019467\n",
      "batch 5270: loss 0.206604\n",
      "batch 5271: loss 0.036082\n",
      "batch 5272: loss 0.088939\n",
      "batch 5273: loss 0.031300\n",
      "batch 5274: loss 0.014082\n",
      "batch 5275: loss 0.024156\n",
      "batch 5276: loss 0.022793\n",
      "batch 5277: loss 0.039969\n",
      "batch 5278: loss 0.013255\n",
      "batch 5279: loss 0.089904\n",
      "batch 5280: loss 0.006617\n",
      "batch 5281: loss 0.034938\n",
      "batch 5282: loss 0.037879\n",
      "batch 5283: loss 0.039417\n",
      "batch 5284: loss 0.057888\n",
      "batch 5285: loss 0.050026\n",
      "batch 5286: loss 0.040277\n",
      "batch 5287: loss 0.010113\n",
      "batch 5288: loss 0.061660\n",
      "batch 5289: loss 0.030639\n",
      "batch 5290: loss 0.172887\n",
      "batch 5291: loss 0.008731\n",
      "batch 5292: loss 0.013973\n",
      "batch 5293: loss 0.063820\n",
      "batch 5294: loss 0.173242\n",
      "batch 5295: loss 0.098553\n",
      "batch 5296: loss 0.250057\n",
      "batch 5297: loss 0.111490\n",
      "batch 5298: loss 0.085538\n",
      "batch 5299: loss 0.243834\n",
      "batch 5300: loss 0.042019\n",
      "batch 5301: loss 0.015550\n",
      "batch 5302: loss 0.049167\n",
      "batch 5303: loss 0.017217\n",
      "batch 5304: loss 0.349548\n",
      "batch 5305: loss 0.127901\n",
      "batch 5306: loss 0.242526\n",
      "batch 5307: loss 0.121726\n",
      "batch 5308: loss 0.100960\n",
      "batch 5309: loss 0.145685\n",
      "batch 5310: loss 0.028485\n",
      "batch 5311: loss 0.027247\n",
      "batch 5312: loss 0.045442\n",
      "batch 5313: loss 0.039140\n",
      "batch 5314: loss 0.026415\n",
      "batch 5315: loss 0.086668\n",
      "batch 5316: loss 0.038384\n",
      "batch 5317: loss 0.032945\n",
      "batch 5318: loss 0.040155\n",
      "batch 5319: loss 0.171926\n",
      "batch 5320: loss 0.079193\n",
      "batch 5321: loss 0.077945\n",
      "batch 5322: loss 0.103986\n",
      "batch 5323: loss 0.044780\n",
      "batch 5324: loss 0.118118\n",
      "batch 5325: loss 0.024844\n",
      "batch 5326: loss 0.089653\n",
      "batch 5327: loss 0.071379\n",
      "batch 5328: loss 0.022052\n",
      "batch 5329: loss 0.027129\n",
      "batch 5330: loss 0.038629\n",
      "batch 5331: loss 0.057407\n",
      "batch 5332: loss 0.104617\n",
      "batch 5333: loss 0.066358\n",
      "batch 5334: loss 0.034730\n",
      "batch 5335: loss 0.144293\n",
      "batch 5336: loss 0.097923\n",
      "batch 5337: loss 0.021228\n",
      "batch 5338: loss 0.027479\n",
      "batch 5339: loss 0.047790\n",
      "batch 5340: loss 0.015669\n",
      "batch 5341: loss 0.206439\n",
      "batch 5342: loss 0.041393\n",
      "batch 5343: loss 0.018773\n",
      "batch 5344: loss 0.131669\n",
      "batch 5345: loss 0.039111\n",
      "batch 5346: loss 0.172846\n",
      "batch 5347: loss 0.024130\n",
      "batch 5348: loss 0.152360\n",
      "batch 5349: loss 0.074957\n",
      "batch 5350: loss 0.082760\n",
      "batch 5351: loss 0.005747\n",
      "batch 5352: loss 0.020706\n",
      "batch 5353: loss 0.095765\n",
      "batch 5354: loss 0.013996\n",
      "batch 5355: loss 0.007905\n",
      "batch 5356: loss 0.025507\n",
      "batch 5357: loss 0.066535\n",
      "batch 5358: loss 0.110109\n",
      "batch 5359: loss 0.047844\n",
      "batch 5360: loss 0.054136\n",
      "batch 5361: loss 0.009333\n",
      "batch 5362: loss 0.077057\n",
      "batch 5363: loss 0.032600\n",
      "batch 5364: loss 0.081871\n",
      "batch 5365: loss 0.084668\n",
      "batch 5366: loss 0.074012\n",
      "batch 5367: loss 0.008911\n",
      "batch 5368: loss 0.046310\n",
      "batch 5369: loss 0.094526\n",
      "batch 5370: loss 0.120959\n",
      "batch 5371: loss 0.026841\n",
      "batch 5372: loss 0.015853\n",
      "batch 5373: loss 0.234665\n",
      "batch 5374: loss 0.115799\n",
      "batch 5375: loss 0.245620\n",
      "batch 5376: loss 0.197538\n",
      "batch 5377: loss 0.038332\n",
      "batch 5378: loss 0.113875\n",
      "batch 5379: loss 0.036173\n",
      "batch 5380: loss 0.029303\n",
      "batch 5381: loss 0.011840\n",
      "batch 5382: loss 0.099336\n",
      "batch 5383: loss 0.136403\n",
      "batch 5384: loss 0.122377\n",
      "batch 5385: loss 0.103351\n",
      "batch 5386: loss 0.025639\n",
      "batch 5387: loss 0.080247\n",
      "batch 5388: loss 0.020661\n",
      "batch 5389: loss 0.048292\n",
      "batch 5390: loss 0.012147\n",
      "batch 5391: loss 0.046776\n",
      "batch 5392: loss 0.147952\n",
      "batch 5393: loss 0.222127\n",
      "batch 5394: loss 0.088837\n",
      "batch 5395: loss 0.068093\n",
      "batch 5396: loss 0.030570\n",
      "batch 5397: loss 0.076388\n",
      "batch 5398: loss 0.100173\n",
      "batch 5399: loss 0.028448\n",
      "batch 5400: loss 0.111344\n",
      "batch 5401: loss 0.133442\n",
      "batch 5402: loss 0.013971\n",
      "batch 5403: loss 0.090982\n",
      "batch 5404: loss 0.101907\n",
      "batch 5405: loss 0.323450\n",
      "batch 5406: loss 0.054187\n",
      "batch 5407: loss 0.171903\n",
      "batch 5408: loss 0.174236\n",
      "batch 5409: loss 0.090399\n",
      "batch 5410: loss 0.011575\n",
      "batch 5411: loss 0.017089\n",
      "batch 5412: loss 0.046318\n",
      "batch 5413: loss 0.037815\n",
      "batch 5414: loss 0.110041\n",
      "batch 5415: loss 0.054359\n",
      "batch 5416: loss 0.103735\n",
      "batch 5417: loss 0.091489\n",
      "batch 5418: loss 0.018586\n",
      "batch 5419: loss 0.032227\n",
      "batch 5420: loss 0.062194\n",
      "batch 5421: loss 0.094070\n",
      "batch 5422: loss 0.020857\n",
      "batch 5423: loss 0.247075\n",
      "batch 5424: loss 0.054258\n",
      "batch 5425: loss 0.078859\n",
      "batch 5426: loss 0.016471\n",
      "batch 5427: loss 0.119962\n",
      "batch 5428: loss 0.058518\n",
      "batch 5429: loss 0.092672\n",
      "batch 5430: loss 0.042736\n",
      "batch 5431: loss 0.030708\n",
      "batch 5432: loss 0.143441\n",
      "batch 5433: loss 0.055996\n",
      "batch 5434: loss 0.050613\n",
      "batch 5435: loss 0.060007\n",
      "batch 5436: loss 0.103701\n",
      "batch 5437: loss 0.011135\n",
      "batch 5438: loss 0.121413\n",
      "batch 5439: loss 0.019709\n",
      "batch 5440: loss 0.021760\n",
      "batch 5441: loss 0.041076\n",
      "batch 5442: loss 0.057724\n",
      "batch 5443: loss 0.139777\n",
      "batch 5444: loss 0.072299\n",
      "batch 5445: loss 0.043709\n",
      "batch 5446: loss 0.034218\n",
      "batch 5447: loss 0.017079\n",
      "batch 5448: loss 0.065793\n",
      "batch 5449: loss 0.026870\n",
      "batch 5450: loss 0.062847\n",
      "batch 5451: loss 0.058929\n",
      "batch 5452: loss 0.050494\n",
      "batch 5453: loss 0.070465\n",
      "batch 5454: loss 0.156525\n",
      "batch 5455: loss 0.045924\n",
      "batch 5456: loss 0.083227\n",
      "batch 5457: loss 0.039345\n",
      "batch 5458: loss 0.021106\n",
      "batch 5459: loss 0.043671\n",
      "batch 5460: loss 0.048778\n",
      "batch 5461: loss 0.151073\n",
      "batch 5462: loss 0.114989\n",
      "batch 5463: loss 0.054516\n",
      "batch 5464: loss 0.126996\n",
      "batch 5465: loss 0.008760\n",
      "batch 5466: loss 0.021787\n",
      "batch 5467: loss 0.023411\n",
      "batch 5468: loss 0.139658\n",
      "batch 5469: loss 0.154173\n",
      "batch 5470: loss 0.162808\n",
      "batch 5471: loss 0.044423\n",
      "batch 5472: loss 0.011379\n",
      "batch 5473: loss 0.413731\n",
      "batch 5474: loss 0.065620\n",
      "batch 5475: loss 0.103908\n",
      "batch 5476: loss 0.020432\n",
      "batch 5477: loss 0.106073\n",
      "batch 5478: loss 0.014551\n",
      "batch 5479: loss 0.072651\n",
      "batch 5480: loss 0.023497\n",
      "batch 5481: loss 0.186150\n",
      "batch 5482: loss 0.132993\n",
      "batch 5483: loss 0.037464\n",
      "batch 5484: loss 0.092965\n",
      "batch 5485: loss 0.052143\n",
      "batch 5486: loss 0.064693\n",
      "batch 5487: loss 0.038731\n",
      "batch 5488: loss 0.051761\n",
      "batch 5489: loss 0.077036\n",
      "batch 5490: loss 0.081120\n",
      "batch 5491: loss 0.115275\n",
      "batch 5492: loss 0.034383\n",
      "batch 5493: loss 0.093902\n",
      "batch 5494: loss 0.055500\n",
      "batch 5495: loss 0.086046\n",
      "batch 5496: loss 0.145503\n",
      "batch 5497: loss 0.081951\n",
      "batch 5498: loss 0.006270\n",
      "batch 5499: loss 0.030507\n",
      "batch 5500: loss 0.051089\n",
      "batch 5501: loss 0.087042\n",
      "batch 5502: loss 0.069687\n",
      "batch 5503: loss 0.011768\n",
      "batch 5504: loss 0.008654\n",
      "batch 5505: loss 0.060773\n",
      "batch 5506: loss 0.104788\n",
      "batch 5507: loss 0.103585\n",
      "batch 5508: loss 0.014250\n",
      "batch 5509: loss 0.096980\n",
      "batch 5510: loss 0.014663\n",
      "batch 5511: loss 0.037603\n",
      "batch 5512: loss 0.051149\n",
      "batch 5513: loss 0.008150\n",
      "batch 5514: loss 0.039290\n",
      "batch 5515: loss 0.058773\n",
      "batch 5516: loss 0.055534\n",
      "batch 5517: loss 0.092530\n",
      "batch 5518: loss 0.009993\n",
      "batch 5519: loss 0.042239\n",
      "batch 5520: loss 0.086375\n",
      "batch 5521: loss 0.278805\n",
      "batch 5522: loss 0.152629\n",
      "batch 5523: loss 0.118260\n",
      "batch 5524: loss 0.064347\n",
      "batch 5525: loss 0.276199\n",
      "batch 5526: loss 0.028178\n",
      "batch 5527: loss 0.199386\n",
      "batch 5528: loss 0.031104\n",
      "batch 5529: loss 0.080256\n",
      "batch 5530: loss 0.077792\n",
      "batch 5531: loss 0.035317\n",
      "batch 5532: loss 0.023964\n",
      "batch 5533: loss 0.092691\n",
      "batch 5534: loss 0.144829\n",
      "batch 5535: loss 0.067272\n",
      "batch 5536: loss 0.029957\n",
      "batch 5537: loss 0.028442\n",
      "batch 5538: loss 0.212865\n",
      "batch 5539: loss 0.170137\n",
      "batch 5540: loss 0.003271\n",
      "batch 5541: loss 0.014275\n",
      "batch 5542: loss 0.034435\n",
      "batch 5543: loss 0.046521\n",
      "batch 5544: loss 0.158718\n",
      "batch 5545: loss 0.160811\n",
      "batch 5546: loss 0.014663\n",
      "batch 5547: loss 0.028859\n",
      "batch 5548: loss 0.033380\n",
      "batch 5549: loss 0.020150\n",
      "batch 5550: loss 0.049294\n",
      "batch 5551: loss 0.024348\n",
      "batch 5552: loss 0.016523\n",
      "batch 5553: loss 0.156575\n",
      "batch 5554: loss 0.051294\n",
      "batch 5555: loss 0.046170\n",
      "batch 5556: loss 0.016889\n",
      "batch 5557: loss 0.075163\n",
      "batch 5558: loss 0.131056\n",
      "batch 5559: loss 0.010390\n",
      "batch 5560: loss 0.170843\n",
      "batch 5561: loss 0.008036\n",
      "batch 5562: loss 0.048903\n",
      "batch 5563: loss 0.069845\n",
      "batch 5564: loss 0.011609\n",
      "batch 5565: loss 0.129171\n",
      "batch 5566: loss 0.052831\n",
      "batch 5567: loss 0.089405\n",
      "batch 5568: loss 0.100593\n",
      "batch 5569: loss 0.084822\n",
      "batch 5570: loss 0.022933\n",
      "batch 5571: loss 0.032935\n",
      "batch 5572: loss 0.022935\n",
      "batch 5573: loss 0.021468\n",
      "batch 5574: loss 0.058351\n",
      "batch 5575: loss 0.017591\n",
      "batch 5576: loss 0.014012\n",
      "batch 5577: loss 0.037128\n",
      "batch 5578: loss 0.035777\n",
      "batch 5579: loss 0.090696\n",
      "batch 5580: loss 0.081269\n",
      "batch 5581: loss 0.195446\n",
      "batch 5582: loss 0.170734\n",
      "batch 5583: loss 0.073423\n",
      "batch 5584: loss 0.224303\n",
      "batch 5585: loss 0.066911\n",
      "batch 5586: loss 0.016457\n",
      "batch 5587: loss 0.028099\n",
      "batch 5588: loss 0.009371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5589: loss 0.055489\n",
      "batch 5590: loss 0.004743\n",
      "batch 5591: loss 0.025435\n",
      "batch 5592: loss 0.247563\n",
      "batch 5593: loss 0.050641\n",
      "batch 5594: loss 0.030752\n",
      "batch 5595: loss 0.074567\n",
      "batch 5596: loss 0.059085\n",
      "batch 5597: loss 0.025437\n",
      "batch 5598: loss 0.038885\n",
      "batch 5599: loss 0.046174\n",
      "batch 5600: loss 0.020081\n",
      "batch 5601: loss 0.234942\n",
      "batch 5602: loss 0.096789\n",
      "batch 5603: loss 0.078944\n",
      "batch 5604: loss 0.021988\n",
      "batch 5605: loss 0.043715\n",
      "batch 5606: loss 0.058761\n",
      "batch 5607: loss 0.051432\n",
      "batch 5608: loss 0.065724\n",
      "batch 5609: loss 0.045942\n",
      "batch 5610: loss 0.088972\n",
      "batch 5611: loss 0.151358\n",
      "batch 5612: loss 0.047748\n",
      "batch 5613: loss 0.040296\n",
      "batch 5614: loss 0.194476\n",
      "batch 5615: loss 0.051982\n",
      "batch 5616: loss 0.008694\n",
      "batch 5617: loss 0.019762\n",
      "batch 5618: loss 0.040182\n",
      "batch 5619: loss 0.108695\n",
      "batch 5620: loss 0.029578\n",
      "batch 5621: loss 0.037690\n",
      "batch 5622: loss 0.040565\n",
      "batch 5623: loss 0.050018\n",
      "batch 5624: loss 0.013998\n",
      "batch 5625: loss 0.095331\n",
      "batch 5626: loss 0.113982\n",
      "batch 5627: loss 0.063716\n",
      "batch 5628: loss 0.121458\n",
      "batch 5629: loss 0.063014\n",
      "batch 5630: loss 0.198824\n",
      "batch 5631: loss 0.008772\n",
      "batch 5632: loss 0.027026\n",
      "batch 5633: loss 0.081714\n",
      "batch 5634: loss 0.034375\n",
      "batch 5635: loss 0.152289\n",
      "batch 5636: loss 0.038380\n",
      "batch 5637: loss 0.020741\n",
      "batch 5638: loss 0.022837\n",
      "batch 5639: loss 0.036596\n",
      "batch 5640: loss 0.039274\n",
      "batch 5641: loss 0.066565\n",
      "batch 5642: loss 0.139112\n",
      "batch 5643: loss 0.040504\n",
      "batch 5644: loss 0.109565\n",
      "batch 5645: loss 0.032151\n",
      "batch 5646: loss 0.064835\n",
      "batch 5647: loss 0.020372\n",
      "batch 5648: loss 0.037039\n",
      "batch 5649: loss 0.038435\n",
      "batch 5650: loss 0.021295\n",
      "batch 5651: loss 0.081105\n",
      "batch 5652: loss 0.093851\n",
      "batch 5653: loss 0.035621\n",
      "batch 5654: loss 0.095659\n",
      "batch 5655: loss 0.103745\n",
      "batch 5656: loss 0.025572\n",
      "batch 5657: loss 0.014003\n",
      "batch 5658: loss 0.012100\n",
      "batch 5659: loss 0.194041\n",
      "batch 5660: loss 0.016667\n",
      "batch 5661: loss 0.191956\n",
      "batch 5662: loss 0.109015\n",
      "batch 5663: loss 0.030588\n",
      "batch 5664: loss 0.073034\n",
      "batch 5665: loss 0.158920\n",
      "batch 5666: loss 0.024461\n",
      "batch 5667: loss 0.085294\n",
      "batch 5668: loss 0.028593\n",
      "batch 5669: loss 0.057840\n",
      "batch 5670: loss 0.021284\n",
      "batch 5671: loss 0.010994\n",
      "batch 5672: loss 0.031901\n",
      "batch 5673: loss 0.160880\n",
      "batch 5674: loss 0.181609\n",
      "batch 5675: loss 0.104488\n",
      "batch 5676: loss 0.085832\n",
      "batch 5677: loss 0.045937\n",
      "batch 5678: loss 0.078755\n",
      "batch 5679: loss 0.036476\n",
      "batch 5680: loss 0.021247\n",
      "batch 5681: loss 0.032637\n",
      "batch 5682: loss 0.027051\n",
      "batch 5683: loss 0.017245\n",
      "batch 5684: loss 0.035329\n",
      "batch 5685: loss 0.101385\n",
      "batch 5686: loss 0.008225\n",
      "batch 5687: loss 0.042171\n",
      "batch 5688: loss 0.010572\n",
      "batch 5689: loss 0.074532\n",
      "batch 5690: loss 0.033832\n",
      "batch 5691: loss 0.020528\n",
      "batch 5692: loss 0.015218\n",
      "batch 5693: loss 0.167188\n",
      "batch 5694: loss 0.080107\n",
      "batch 5695: loss 0.021944\n",
      "batch 5696: loss 0.137609\n",
      "batch 5697: loss 0.054580\n",
      "batch 5698: loss 0.007447\n",
      "batch 5699: loss 0.088452\n",
      "batch 5700: loss 0.012209\n",
      "batch 5701: loss 0.143920\n",
      "batch 5702: loss 0.034541\n",
      "batch 5703: loss 0.021337\n",
      "batch 5704: loss 0.061074\n",
      "batch 5705: loss 0.018855\n",
      "batch 5706: loss 0.039249\n",
      "batch 5707: loss 0.020285\n",
      "batch 5708: loss 0.127632\n",
      "batch 5709: loss 0.061053\n",
      "batch 5710: loss 0.057646\n",
      "batch 5711: loss 0.056666\n",
      "batch 5712: loss 0.058158\n",
      "batch 5713: loss 0.007074\n",
      "batch 5714: loss 0.019033\n",
      "batch 5715: loss 0.028170\n",
      "batch 5716: loss 0.169916\n",
      "batch 5717: loss 0.046612\n",
      "batch 5718: loss 0.016922\n",
      "batch 5719: loss 0.099278\n",
      "batch 5720: loss 0.132366\n",
      "batch 5721: loss 0.047424\n",
      "batch 5722: loss 0.053395\n",
      "batch 5723: loss 0.048395\n",
      "batch 5724: loss 0.083803\n",
      "batch 5725: loss 0.080633\n",
      "batch 5726: loss 0.229232\n",
      "batch 5727: loss 0.087526\n",
      "batch 5728: loss 0.013334\n",
      "batch 5729: loss 0.046585\n",
      "batch 5730: loss 0.027844\n",
      "batch 5731: loss 0.038570\n",
      "batch 5732: loss 0.020949\n",
      "batch 5733: loss 0.068048\n",
      "batch 5734: loss 0.056221\n",
      "batch 5735: loss 0.022821\n",
      "batch 5736: loss 0.041582\n",
      "batch 5737: loss 0.132343\n",
      "batch 5738: loss 0.038977\n",
      "batch 5739: loss 0.087242\n",
      "batch 5740: loss 0.069142\n",
      "batch 5741: loss 0.035537\n",
      "batch 5742: loss 0.025587\n",
      "batch 5743: loss 0.031589\n",
      "batch 5744: loss 0.126642\n",
      "batch 5745: loss 0.052747\n",
      "batch 5746: loss 0.023515\n",
      "batch 5747: loss 0.021644\n",
      "batch 5748: loss 0.040324\n",
      "batch 5749: loss 0.070975\n",
      "batch 5750: loss 0.054035\n",
      "batch 5751: loss 0.085311\n",
      "batch 5752: loss 0.051204\n",
      "batch 5753: loss 0.023802\n",
      "batch 5754: loss 0.040137\n",
      "batch 5755: loss 0.066132\n",
      "batch 5756: loss 0.032887\n",
      "batch 5757: loss 0.308879\n",
      "batch 5758: loss 0.044740\n",
      "batch 5759: loss 0.091059\n",
      "batch 5760: loss 0.246102\n",
      "batch 5761: loss 0.022166\n",
      "batch 5762: loss 0.037812\n",
      "batch 5763: loss 0.041752\n",
      "batch 5764: loss 0.017379\n",
      "batch 5765: loss 0.113763\n",
      "batch 5766: loss 0.035497\n",
      "batch 5767: loss 0.265805\n",
      "batch 5768: loss 0.045017\n",
      "batch 5769: loss 0.036384\n",
      "batch 5770: loss 0.011188\n",
      "batch 5771: loss 0.018913\n",
      "batch 5772: loss 0.238144\n",
      "batch 5773: loss 0.050193\n",
      "batch 5774: loss 0.017530\n",
      "batch 5775: loss 0.047263\n",
      "batch 5776: loss 0.045937\n",
      "batch 5777: loss 0.051535\n",
      "batch 5778: loss 0.023806\n",
      "batch 5779: loss 0.053266\n",
      "batch 5780: loss 0.064052\n",
      "batch 5781: loss 0.107259\n",
      "batch 5782: loss 0.016065\n",
      "batch 5783: loss 0.059055\n",
      "batch 5784: loss 0.128520\n",
      "batch 5785: loss 0.095815\n",
      "batch 5786: loss 0.047398\n",
      "batch 5787: loss 0.068215\n",
      "batch 5788: loss 0.061837\n",
      "batch 5789: loss 0.218660\n",
      "batch 5790: loss 0.025182\n",
      "batch 5791: loss 0.108505\n",
      "batch 5792: loss 0.036920\n",
      "batch 5793: loss 0.067704\n",
      "batch 5794: loss 0.124882\n",
      "batch 5795: loss 0.099724\n",
      "batch 5796: loss 0.103782\n",
      "batch 5797: loss 0.012829\n",
      "batch 5798: loss 0.095383\n",
      "batch 5799: loss 0.111560\n",
      "batch 5800: loss 0.047892\n",
      "batch 5801: loss 0.018862\n",
      "batch 5802: loss 0.097019\n",
      "batch 5803: loss 0.063561\n",
      "batch 5804: loss 0.026554\n",
      "batch 5805: loss 0.118296\n",
      "batch 5806: loss 0.026857\n",
      "batch 5807: loss 0.015086\n",
      "batch 5808: loss 0.043137\n",
      "batch 5809: loss 0.019729\n",
      "batch 5810: loss 0.164507\n",
      "batch 5811: loss 0.032520\n",
      "batch 5812: loss 0.207301\n",
      "batch 5813: loss 0.063464\n",
      "batch 5814: loss 0.016655\n",
      "batch 5815: loss 0.009427\n",
      "batch 5816: loss 0.015890\n",
      "batch 5817: loss 0.022536\n",
      "batch 5818: loss 0.051811\n",
      "batch 5819: loss 0.059001\n",
      "batch 5820: loss 0.020407\n",
      "batch 5821: loss 0.051295\n",
      "batch 5822: loss 0.019676\n",
      "batch 5823: loss 0.152101\n",
      "batch 5824: loss 0.075374\n",
      "batch 5825: loss 0.239588\n",
      "batch 5826: loss 0.052623\n",
      "batch 5827: loss 0.009218\n",
      "batch 5828: loss 0.033675\n",
      "batch 5829: loss 0.069327\n",
      "batch 5830: loss 0.016747\n",
      "batch 5831: loss 0.287037\n",
      "batch 5832: loss 0.009396\n",
      "batch 5833: loss 0.076019\n",
      "batch 5834: loss 0.008876\n",
      "batch 5835: loss 0.048282\n",
      "batch 5836: loss 0.033241\n",
      "batch 5837: loss 0.033758\n",
      "batch 5838: loss 0.126921\n",
      "batch 5839: loss 0.096751\n",
      "batch 5840: loss 0.038758\n",
      "batch 5841: loss 0.262727\n",
      "batch 5842: loss 0.048869\n",
      "batch 5843: loss 0.011796\n",
      "batch 5844: loss 0.020686\n",
      "batch 5845: loss 0.066658\n",
      "batch 5846: loss 0.055169\n",
      "batch 5847: loss 0.025270\n",
      "batch 5848: loss 0.140845\n",
      "batch 5849: loss 0.069721\n",
      "batch 5850: loss 0.041570\n",
      "batch 5851: loss 0.058874\n",
      "batch 5852: loss 0.085635\n",
      "batch 5853: loss 0.070371\n",
      "batch 5854: loss 0.181022\n",
      "batch 5855: loss 0.019075\n",
      "batch 5856: loss 0.048807\n",
      "batch 5857: loss 0.061301\n",
      "batch 5858: loss 0.007697\n",
      "batch 5859: loss 0.012044\n",
      "batch 5860: loss 0.041838\n",
      "batch 5861: loss 0.117237\n",
      "batch 5862: loss 0.037133\n",
      "batch 5863: loss 0.017514\n",
      "batch 5864: loss 0.019074\n",
      "batch 5865: loss 0.070343\n",
      "batch 5866: loss 0.053737\n",
      "batch 5867: loss 0.192729\n",
      "batch 5868: loss 0.116363\n",
      "batch 5869: loss 0.110164\n",
      "batch 5870: loss 0.169702\n",
      "batch 5871: loss 0.066434\n",
      "batch 5872: loss 0.024571\n",
      "batch 5873: loss 0.011737\n",
      "batch 5874: loss 0.042391\n",
      "batch 5875: loss 0.117893\n",
      "batch 5876: loss 0.096679\n",
      "batch 5877: loss 0.047744\n",
      "batch 5878: loss 0.056324\n",
      "batch 5879: loss 0.045227\n",
      "batch 5880: loss 0.044208\n",
      "batch 5881: loss 0.056387\n",
      "batch 5882: loss 0.021778\n",
      "batch 5883: loss 0.059857\n",
      "batch 5884: loss 0.034373\n",
      "batch 5885: loss 0.053830\n",
      "batch 5886: loss 0.053749\n",
      "batch 5887: loss 0.025820\n",
      "batch 5888: loss 0.043846\n",
      "batch 5889: loss 0.027482\n",
      "batch 5890: loss 0.114336\n",
      "batch 5891: loss 0.032438\n",
      "batch 5892: loss 0.078150\n",
      "batch 5893: loss 0.008428\n",
      "batch 5894: loss 0.018470\n",
      "batch 5895: loss 0.010131\n",
      "batch 5896: loss 0.103303\n",
      "batch 5897: loss 0.015242\n",
      "batch 5898: loss 0.189939\n",
      "batch 5899: loss 0.024182\n",
      "batch 5900: loss 0.253199\n",
      "batch 5901: loss 0.120737\n",
      "batch 5902: loss 0.336806\n",
      "batch 5903: loss 0.218185\n",
      "batch 5904: loss 0.050231\n",
      "batch 5905: loss 0.072974\n",
      "batch 5906: loss 0.053299\n",
      "batch 5907: loss 0.158971\n",
      "batch 5908: loss 0.129024\n",
      "batch 5909: loss 0.059777\n",
      "batch 5910: loss 0.092853\n",
      "batch 5911: loss 0.033288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5912: loss 0.131022\n",
      "batch 5913: loss 0.047539\n",
      "batch 5914: loss 0.070756\n",
      "batch 5915: loss 0.079483\n",
      "batch 5916: loss 0.028989\n",
      "batch 5917: loss 0.090162\n",
      "batch 5918: loss 0.007731\n",
      "batch 5919: loss 0.028504\n",
      "batch 5920: loss 0.034852\n",
      "batch 5921: loss 0.054969\n",
      "batch 5922: loss 0.193714\n",
      "batch 5923: loss 0.084070\n",
      "batch 5924: loss 0.076362\n",
      "batch 5925: loss 0.007256\n",
      "batch 5926: loss 0.026977\n",
      "batch 5927: loss 0.068007\n",
      "batch 5928: loss 0.022077\n",
      "batch 5929: loss 0.043461\n",
      "batch 5930: loss 0.045775\n",
      "batch 5931: loss 0.016251\n",
      "batch 5932: loss 0.124308\n",
      "batch 5933: loss 0.008541\n",
      "batch 5934: loss 0.154007\n",
      "batch 5935: loss 0.087722\n",
      "batch 5936: loss 0.144453\n",
      "batch 5937: loss 0.022793\n",
      "batch 5938: loss 0.098625\n",
      "batch 5939: loss 0.034785\n",
      "batch 5940: loss 0.041915\n",
      "batch 5941: loss 0.008431\n",
      "batch 5942: loss 0.040161\n",
      "batch 5943: loss 0.078341\n",
      "batch 5944: loss 0.088413\n",
      "batch 5945: loss 0.099366\n",
      "batch 5946: loss 0.023092\n",
      "batch 5947: loss 0.057573\n",
      "batch 5948: loss 0.190539\n",
      "batch 5949: loss 0.154315\n",
      "batch 5950: loss 0.010634\n",
      "batch 5951: loss 0.113626\n",
      "batch 5952: loss 0.044921\n",
      "batch 5953: loss 0.041013\n",
      "batch 5954: loss 0.077689\n",
      "batch 5955: loss 0.014691\n",
      "batch 5956: loss 0.131471\n",
      "batch 5957: loss 0.006284\n",
      "batch 5958: loss 0.028960\n",
      "batch 5959: loss 0.024157\n",
      "batch 5960: loss 0.039846\n",
      "batch 5961: loss 0.062192\n",
      "batch 5962: loss 0.075398\n",
      "batch 5963: loss 0.048862\n",
      "batch 5964: loss 0.030116\n",
      "batch 5965: loss 0.019158\n",
      "batch 5966: loss 0.138062\n",
      "batch 5967: loss 0.022590\n",
      "batch 5968: loss 0.051969\n",
      "batch 5969: loss 0.046946\n",
      "batch 5970: loss 0.055737\n",
      "batch 5971: loss 0.096623\n",
      "batch 5972: loss 0.060861\n",
      "batch 5973: loss 0.049361\n",
      "batch 5974: loss 0.142605\n",
      "batch 5975: loss 0.053752\n",
      "batch 5976: loss 0.031831\n",
      "batch 5977: loss 0.079489\n",
      "batch 5978: loss 0.064552\n",
      "batch 5979: loss 0.033479\n",
      "batch 5980: loss 0.039121\n",
      "batch 5981: loss 0.141858\n",
      "batch 5982: loss 0.032855\n",
      "batch 5983: loss 0.040446\n",
      "batch 5984: loss 0.005001\n",
      "batch 5985: loss 0.158919\n",
      "batch 5986: loss 0.195043\n",
      "batch 5987: loss 0.020188\n",
      "batch 5988: loss 0.015907\n",
      "batch 5989: loss 0.068636\n",
      "batch 5990: loss 0.150680\n",
      "batch 5991: loss 0.024799\n",
      "batch 5992: loss 0.048268\n",
      "batch 5993: loss 0.033484\n",
      "batch 5994: loss 0.049845\n",
      "batch 5995: loss 0.021939\n",
      "batch 5996: loss 0.041778\n",
      "batch 5997: loss 0.035558\n",
      "batch 5998: loss 0.017060\n",
      "batch 5999: loss 0.011226\n",
      "test accuracy: 0.969600\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(units=100)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.ac1 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "        \n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.ac2 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.bn1(x)\n",
    "        x = self.ac1(x)\n",
    "        \n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        x = self.bn2(x)\n",
    "        x = self.ac2(x)\n",
    "\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)   (60000,)\n",
      "(10000, 784)   (10000,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,910\n",
      "Trainable params: 79,710\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "165/165 - 1s - loss: 0.4516 - accuracy: 0.8703 - val_loss: 0.2729 - val_accuracy: 0.9227\n",
      "Epoch 2/100\n",
      "165/165 - 0s - loss: 0.2446 - accuracy: 0.9308 - val_loss: 0.2365 - val_accuracy: 0.9339\n",
      "Epoch 3/100\n",
      "165/165 - 0s - loss: 0.1990 - accuracy: 0.9455 - val_loss: 0.2004 - val_accuracy: 0.9441\n",
      "Epoch 4/100\n",
      "165/165 - 0s - loss: 0.1687 - accuracy: 0.9538 - val_loss: 0.1836 - val_accuracy: 0.9476\n",
      "Epoch 5/100\n",
      "165/165 - 0s - loss: 0.1461 - accuracy: 0.9597 - val_loss: 0.1731 - val_accuracy: 0.9521\n",
      "Epoch 6/100\n",
      "165/165 - 0s - loss: 0.1269 - accuracy: 0.9659 - val_loss: 0.1692 - val_accuracy: 0.9500\n",
      "Epoch 7/100\n",
      "165/165 - 0s - loss: 0.1151 - accuracy: 0.9680 - val_loss: 0.1542 - val_accuracy: 0.9566\n",
      "Epoch 8/100\n",
      "165/165 - 0s - loss: 0.1035 - accuracy: 0.9724 - val_loss: 0.1458 - val_accuracy: 0.9593\n",
      "Epoch 9/100\n",
      "165/165 - 0s - loss: 0.0921 - accuracy: 0.9752 - val_loss: 0.1434 - val_accuracy: 0.9591\n",
      "Epoch 10/100\n",
      "165/165 - 0s - loss: 0.0835 - accuracy: 0.9765 - val_loss: 0.1435 - val_accuracy: 0.9579\n",
      "Epoch 11/100\n",
      "165/165 - 1s - loss: 0.0765 - accuracy: 0.9792 - val_loss: 0.1389 - val_accuracy: 0.9604\n",
      "Epoch 12/100\n",
      "165/165 - 0s - loss: 0.0666 - accuracy: 0.9825 - val_loss: 0.1314 - val_accuracy: 0.9630\n",
      "Epoch 13/100\n",
      "165/165 - 0s - loss: 0.0630 - accuracy: 0.9835 - val_loss: 0.1390 - val_accuracy: 0.9604\n",
      "Epoch 14/100\n",
      "165/165 - 0s - loss: 0.0564 - accuracy: 0.9856 - val_loss: 0.1450 - val_accuracy: 0.9602\n",
      "Epoch 15/100\n",
      "165/165 - 0s - loss: 0.0505 - accuracy: 0.9880 - val_loss: 0.1254 - val_accuracy: 0.9637\n",
      "Epoch 16/100\n",
      "165/165 - 0s - loss: 0.0457 - accuracy: 0.9889 - val_loss: 0.1271 - val_accuracy: 0.9639\n",
      "Epoch 17/100\n",
      "165/165 - 0s - loss: 0.0433 - accuracy: 0.9893 - val_loss: 0.1211 - val_accuracy: 0.9662\n",
      "Epoch 18/100\n",
      "165/165 - 0s - loss: 0.0439 - accuracy: 0.9887 - val_loss: 0.1350 - val_accuracy: 0.9608\n",
      "Epoch 19/100\n",
      "165/165 - 0s - loss: 0.0504 - accuracy: 0.9854 - val_loss: 0.1336 - val_accuracy: 0.9615\n",
      "Epoch 20/100\n",
      "165/165 - 0s - loss: 0.0356 - accuracy: 0.9916 - val_loss: 0.1288 - val_accuracy: 0.9648\n",
      "Epoch 21/100\n",
      "165/165 - 0s - loss: 0.0306 - accuracy: 0.9931 - val_loss: 0.1215 - val_accuracy: 0.9657\n",
      "Epoch 22/100\n",
      "165/165 - 0s - loss: 0.0261 - accuracy: 0.9952 - val_loss: 0.1177 - val_accuracy: 0.9666\n",
      "Epoch 23/100\n",
      "165/165 - 0s - loss: 0.0258 - accuracy: 0.9947 - val_loss: 0.1301 - val_accuracy: 0.9641\n",
      "Epoch 24/100\n",
      "165/165 - 0s - loss: 0.0264 - accuracy: 0.9942 - val_loss: 0.1273 - val_accuracy: 0.9647\n",
      "Epoch 25/100\n",
      "165/165 - 0s - loss: 0.0206 - accuracy: 0.9967 - val_loss: 0.1224 - val_accuracy: 0.9678\n",
      "Epoch 26/100\n",
      "165/165 - 0s - loss: 0.0253 - accuracy: 0.9937 - val_loss: 0.1258 - val_accuracy: 0.9662\n",
      "Epoch 27/100\n",
      "165/165 - 0s - loss: 0.0231 - accuracy: 0.9952 - val_loss: 0.1334 - val_accuracy: 0.9633\n",
      "Epoch 28/100\n",
      "165/165 - 0s - loss: 0.0266 - accuracy: 0.9927 - val_loss: 0.1257 - val_accuracy: 0.9662\n",
      "Epoch 29/100\n",
      "165/165 - 0s - loss: 0.0264 - accuracy: 0.9936 - val_loss: 0.1211 - val_accuracy: 0.9680\n",
      "Epoch 30/100\n",
      "165/165 - 0s - loss: 0.0159 - accuracy: 0.9972 - val_loss: 0.1243 - val_accuracy: 0.9671\n",
      "Epoch 31/100\n",
      "165/165 - 0s - loss: 0.0133 - accuracy: 0.9981 - val_loss: 0.1257 - val_accuracy: 0.9665\n",
      "Epoch 32/100\n",
      "165/165 - 0s - loss: 0.0119 - accuracy: 0.9986 - val_loss: 0.1358 - val_accuracy: 0.9644\n",
      "Epoch 33/100\n",
      "165/165 - 0s - loss: 0.0299 - accuracy: 0.9916 - val_loss: 0.1300 - val_accuracy: 0.9666\n",
      "Epoch 34/100\n",
      "165/165 - 0s - loss: 0.0134 - accuracy: 0.9979 - val_loss: 0.1298 - val_accuracy: 0.9664\n",
      "Epoch 35/100\n",
      "165/165 - 0s - loss: 0.0109 - accuracy: 0.9987 - val_loss: 0.1279 - val_accuracy: 0.9662\n",
      "Epoch 36/100\n",
      "165/165 - 0s - loss: 0.0199 - accuracy: 0.9953 - val_loss: 0.1370 - val_accuracy: 0.9640\n",
      "Epoch 37/100\n",
      "165/165 - 0s - loss: 0.0145 - accuracy: 0.9972 - val_loss: 0.1301 - val_accuracy: 0.9681\n",
      "Epoch 38/100\n",
      "165/165 - 0s - loss: 0.0103 - accuracy: 0.9991 - val_loss: 0.1261 - val_accuracy: 0.9682\n",
      "Epoch 39/100\n",
      "165/165 - 0s - loss: 0.0083 - accuracy: 0.9994 - val_loss: 0.1274 - val_accuracy: 0.9676\n",
      "Epoch 40/100\n",
      "165/165 - 0s - loss: 0.0161 - accuracy: 0.9965 - val_loss: 0.1324 - val_accuracy: 0.9673\n",
      "Epoch 41/100\n",
      "165/165 - 0s - loss: 0.0085 - accuracy: 0.9992 - val_loss: 0.1284 - val_accuracy: 0.9680\n",
      "Epoch 42/100\n",
      "165/165 - 0s - loss: 0.0072 - accuracy: 0.9994 - val_loss: 0.1387 - val_accuracy: 0.9673\n",
      "Epoch 43/100\n",
      "165/165 - 0s - loss: 0.0147 - accuracy: 0.9968 - val_loss: 0.1350 - val_accuracy: 0.9681\n",
      "Epoch 44/100\n",
      "165/165 - 0s - loss: 0.0077 - accuracy: 0.9990 - val_loss: 0.1331 - val_accuracy: 0.9683\n",
      "Epoch 45/100\n",
      "165/165 - 0s - loss: 0.0063 - accuracy: 0.9995 - val_loss: 0.1342 - val_accuracy: 0.9674\n",
      "Epoch 46/100\n",
      "165/165 - 0s - loss: 0.0079 - accuracy: 0.9991 - val_loss: 0.1360 - val_accuracy: 0.9671\n",
      "Epoch 47/100\n",
      "165/165 - 0s - loss: 0.0059 - accuracy: 0.9995 - val_loss: 0.1433 - val_accuracy: 0.9654\n",
      "Epoch 48/100\n",
      "165/165 - 1s - loss: 0.0172 - accuracy: 0.9953 - val_loss: 0.1583 - val_accuracy: 0.9625\n",
      "Epoch 49/100\n",
      "165/165 - 1s - loss: 0.0118 - accuracy: 0.9974 - val_loss: 0.1327 - val_accuracy: 0.9688\n",
      "Epoch 50/100\n",
      "165/165 - 0s - loss: 0.0061 - accuracy: 0.9994 - val_loss: 0.1353 - val_accuracy: 0.9684\n",
      "Epoch 51/100\n",
      "165/165 - 1s - loss: 0.0045 - accuracy: 0.9997 - val_loss: 0.1359 - val_accuracy: 0.9683\n",
      "Epoch 52/100\n",
      "165/165 - 0s - loss: 0.0041 - accuracy: 0.9998 - val_loss: 0.1386 - val_accuracy: 0.9692\n",
      "Epoch 53/100\n",
      "165/165 - 0s - loss: 0.0042 - accuracy: 0.9997 - val_loss: 0.1392 - val_accuracy: 0.9673\n",
      "Epoch 54/100\n",
      "165/165 - 0s - loss: 0.0163 - accuracy: 0.9956 - val_loss: 0.1721 - val_accuracy: 0.9606\n",
      "Epoch 55/100\n",
      "165/165 - 0s - loss: 0.0130 - accuracy: 0.9971 - val_loss: 0.1510 - val_accuracy: 0.9664\n",
      "Epoch 56/100\n",
      "165/165 - 0s - loss: 0.0054 - accuracy: 0.9992 - val_loss: 0.1372 - val_accuracy: 0.9672\n",
      "Epoch 57/100\n",
      "165/165 - 0s - loss: 0.0042 - accuracy: 0.9996 - val_loss: 0.1327 - val_accuracy: 0.9701\n",
      "Epoch 58/100\n",
      "165/165 - 0s - loss: 0.0032 - accuracy: 0.9999 - val_loss: 0.1385 - val_accuracy: 0.9683\n",
      "Epoch 59/100\n",
      "165/165 - 0s - loss: 0.0168 - accuracy: 0.9955 - val_loss: 0.1493 - val_accuracy: 0.9656\n",
      "Epoch 60/100\n",
      "165/165 - 0s - loss: 0.0153 - accuracy: 0.9964 - val_loss: 0.1431 - val_accuracy: 0.9676\n",
      "Epoch 61/100\n",
      "165/165 - 0s - loss: 0.0047 - accuracy: 0.9996 - val_loss: 0.1416 - val_accuracy: 0.9679\n",
      "Epoch 62/100\n",
      "165/165 - 0s - loss: 0.0034 - accuracy: 0.9998 - val_loss: 0.1354 - val_accuracy: 0.9693\n",
      "Epoch 63/100\n",
      "165/165 - 0s - loss: 0.0032 - accuracy: 0.9999 - val_loss: 0.1367 - val_accuracy: 0.9686\n",
      "Epoch 64/100\n",
      "165/165 - 0s - loss: 0.0067 - accuracy: 0.9989 - val_loss: 0.1454 - val_accuracy: 0.9674\n",
      "Epoch 65/100\n",
      "165/165 - 0s - loss: 0.0031 - accuracy: 0.9999 - val_loss: 0.1364 - val_accuracy: 0.9697\n",
      "Epoch 66/100\n",
      "165/165 - 0s - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.1339 - val_accuracy: 0.9702\n",
      "Epoch 67/100\n",
      "165/165 - 0s - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.1389 - val_accuracy: 0.9697\n",
      "Epoch 68/100\n",
      "165/165 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1437 - val_accuracy: 0.9684\n",
      "Epoch 69/100\n",
      "165/165 - 0s - loss: 0.0099 - accuracy: 0.9976 - val_loss: 0.1784 - val_accuracy: 0.9621\n",
      "Epoch 70/100\n",
      "165/165 - 0s - loss: 0.0152 - accuracy: 0.9955 - val_loss: 0.1518 - val_accuracy: 0.9677\n",
      "Epoch 71/100\n",
      "165/165 - 0s - loss: 0.0134 - accuracy: 0.9965 - val_loss: 0.1534 - val_accuracy: 0.9660\n",
      "Epoch 72/100\n",
      "165/165 - 0s - loss: 0.0129 - accuracy: 0.9964 - val_loss: 0.1419 - val_accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "165/165 - 0s - loss: 0.0037 - accuracy: 0.9996 - val_loss: 0.1354 - val_accuracy: 0.9704\n",
      "Epoch 74/100\n",
      "165/165 - 0s - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.1325 - val_accuracy: 0.9707\n",
      "Epoch 75/100\n",
      "165/165 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1354 - val_accuracy: 0.9712\n",
      "Epoch 76/100\n",
      "165/165 - 0s - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.1384 - val_accuracy: 0.9698\n",
      "Epoch 77/100\n",
      "165/165 - 0s - loss: 0.0063 - accuracy: 0.9986 - val_loss: 0.1382 - val_accuracy: 0.9702\n",
      "Epoch 78/100\n",
      "165/165 - 0s - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.1395 - val_accuracy: 0.9703\n",
      "Epoch 79/100\n",
      "165/165 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1408 - val_accuracy: 0.9707\n",
      "Epoch 80/100\n",
      "165/165 - 0s - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.1411 - val_accuracy: 0.9701\n",
      "Epoch 81/100\n",
      "165/165 - 0s - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.1430 - val_accuracy: 0.9692\n",
      "Epoch 82/100\n",
      "165/165 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.9697\n",
      "Epoch 83/100\n",
      "165/165 - 0s - loss: 0.0162 - accuracy: 0.9955 - val_loss: 0.1606 - val_accuracy: 0.9659\n",
      "Epoch 84/100\n",
      "165/165 - 0s - loss: 0.0163 - accuracy: 0.9949 - val_loss: 0.1532 - val_accuracy: 0.9673\n",
      "Epoch 85/100\n",
      "165/165 - 0s - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.1409 - val_accuracy: 0.9693\n",
      "Epoch 86/100\n",
      "165/165 - 0s - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.1404 - val_accuracy: 0.9697\n",
      "Epoch 87/100\n",
      "165/165 - 0s - loss: 0.0121 - accuracy: 0.9965 - val_loss: 0.1669 - val_accuracy: 0.9654\n",
      "Epoch 88/100\n",
      "165/165 - 0s - loss: 0.0124 - accuracy: 0.9974 - val_loss: 0.1532 - val_accuracy: 0.9676\n",
      "Epoch 89/100\n",
      "165/165 - 0s - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.1494 - val_accuracy: 0.9681\n",
      "Epoch 90/100\n",
      "165/165 - 0s - loss: 0.0038 - accuracy: 0.9995 - val_loss: 0.1401 - val_accuracy: 0.9697\n",
      "Epoch 91/100\n",
      "165/165 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.9684\n",
      "Epoch 92/100\n",
      "165/165 - 0s - loss: 0.0132 - accuracy: 0.9962 - val_loss: 0.1464 - val_accuracy: 0.9697\n",
      "Epoch 93/100\n",
      "165/165 - 0s - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.1472 - val_accuracy: 0.9680\n",
      "Epoch 94/100\n",
      "165/165 - 0s - loss: 0.0145 - accuracy: 0.9962 - val_loss: 0.1439 - val_accuracy: 0.9691\n",
      "Epoch 95/100\n",
      "165/165 - 0s - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.1379 - val_accuracy: 0.9690\n",
      "Epoch 96/100\n",
      "165/165 - 0s - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.1403 - val_accuracy: 0.9689\n",
      "Epoch 97/100\n",
      "165/165 - 0s - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.1439 - val_accuracy: 0.9695\n",
      "Epoch 98/100\n",
      "165/165 - 0s - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.1400 - val_accuracy: 0.9706\n",
      "Epoch 99/100\n",
      "165/165 - 0s - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.1417 - val_accuracy: 0.9702\n",
      "Epoch 100/100\n",
      "165/165 - 0s - loss: 0.0050 - accuracy: 0.9990 - val_loss: 0.1460 - val_accuracy: 0.9695\n",
      "313/313 [==============================] - 0s 629us/step - loss: 0.1248 - accuracy: 0.9724\n",
      "[0.12484800070524216, 0.9724000096321106]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])\n",
    "print(x_train.shape, ' ', y_train.shape)\n",
    "print(x_test.shape, ' ', y_test.shape)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(100, input_shape=(784,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "#keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#keras.optimizers.Adam(learning_rate=0.01)\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# provide labels as one_hot representation => tf.keras.losses.CategoricalCrossentropy\n",
    "# provide labels as integers => tf.keras.losses.SparseCategoricalCrossentropy \n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=256, epochs=100, validation_split=0.3, verbose=2)\n",
    "\n",
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
